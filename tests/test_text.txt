
Homework Problems 128 ^ 

Solutions to Practice Problems 143 

3 _____ 

Machine-Level Representation'of Programs 163 

3.1 A Historical Perspective 166 


Contents^ 


ix 


3.2 


3.3 

3.4 


3.5 


3.6 


3.7 


3.8 


Program Encodings 169 

3.2.1 Machine-Level Code 170 

3.2.2 Code Examples 172 

3.2.3 Notes on Formatting 175 i 

Data Formats 177- t' ‘ 

Accessing Infomiation 17p 

3.4.1 Operand,Specifiers 180 , 

3.4.2 Data Movement Instpctions , 182 

3.4.3 Data Moyement Example 186 

3.4.4 Pushing and Popping Stack Data 189 

Arithmetic and Logical Operations 191 

3.5.1 Load Effective Address 191 

3.5.2 Unary and Binary Oper^tionSf 194 

3.5.3 Shift Operations 194 , ^ 

3.5.4 Discussion, 196 

3.5.5 Special Arithmetic Operatiops 197 
Control 200' 

3.6.1 Condition Codes 201 

3.6.2 Accessing the Condition Codes 202 ' ‘ 

3.6.3 Jump Instructions 205 

3.6.4 Jump Instruction Encodings 207 

3.6.5 Implementing Conditional Branches with 
Conditional Control 209 

3.6.6 Implementing-Conditional Branches.with 
Conditional Moves 214 j j 

3.6.7 Loops 220 

3.6.8 Switch Statements 232 
Procedures 238 

3.7.1 The Run-Turie Stack 239 

3.7.2 Control Transfer 241 

3.7.3 Data Transfer 245 

3.7.4 Local Storage on the Stack 248 

3.7.5 Local Storage in Registers 251 

3.7.6 Recursive Procedures 253 

VJ. 

'Array Allocatipn-and Access 255 

3.8.1 Basic Principles 255 , I 

3.8.2 Pointer Arithmetic 257 

3.8.3 Nested Arrays 258 < 

3.8.4 Fixed-Size Arrays 260, 

3.8.5 Variable-Size Arrays 262 


^ 1 


! 


I -v 

e 


X > I I 
'jf.i 


01 




i 


)‘li 0 ' 


I 


t 



1 ' i 


>'i 


3.9 Heterogeneous Data Structures 265 

3.9.1 Structures 265 

3.9.2 Unions 269 

3.9.3 Data Alignment 273 > 

3.10 Combining Control and Data in Machine-Level Programs 276 
3 10.1 Understanding Pointers 277 

3.10.2 Life in the Real World: Using the gdb Debugger 279 

3.10.3 Out-of-Bounds Memory References and Buffer Oyerflow 279 

3.10.4 Thwarting Buffer Overflow Attacks 284 

3.10.5 Supporting Variable-Size Stack‘'Frames’ 290 

3.11 Hoating-Point Code 293 

3.11.1 Floating-Point Movement and Conversion Operations zyo 

3.11.2 Floating-Point Code in Procedures 301 

3.11.3 Floating-Point Arithmetic Operations 302 

3.11.4 Defining and Using Floating-Point Constants 304 

3.11.5 Using Bitwise Operations in Floating-Point Code 305 

3.11.6 Floating-Point Comparison Operations 306 

3.11.7 Observations about Floating-Point Code 309 

3.12 Summary 309 
Bibliographic Notes 310 
Homework Problems 311 
Solutions to Practice Problems 325 

4 __-— 

Processor Architecture 351 

4.1 The Y86-64 Instruction Set Architecture 355 

4.1.1 Programmer-Visible State 355 

4.1.2 Y86-64 Instructions 356 

4.1.3 Instruction Encoding 358 

4.1.4 Y86-64 Exceptions 363 

4.1.5 Y86-64 Programs 364 

4.1.6 Some Y86-64 Instruction Details 370 

4.2 Logic Design and the Hardware Control Language HCL 372 

4.2.1 Logic Gates 373 

4.2.2 Combinational Circuits and HCL Boolean Expressions 374 

4.2.3 Word-Level Combinational Circuits and HCL 
Integer Expressions 376 

4.2.4 Set Membership 380 

4.2.5 Memory and Clocking 381 

4.3 Sequential Y86-64 Implementations 384 
4.3.1 Organizing Processing into Stages 384 



) < 1 .. 


4.3.2 SEQ Hardware Structure 396qO - S 

4.3.3 SEQ Timing 400 ^ nu 

4.3.4 SEQ Stage Implementations 404 •yi'i t * 

4.4 General Principles of Pipelining 412 ? J „ 

4.4.1 Cornputational Pipelines 412' yn >t/ 

4.4.2 A Detailed Look at Pipeline Qperationi- 414’ 

4.4.3 Limit^ations of Pipelining 416 

4.4.4 t.jEipelihing'.a''System with Feedback 419 i 

4.5 PipelinedY86-64Implementations' '421'if.'ti' 

4.5.1 SEQ+; Rearranging the ComputatioA Stagesr.421 

4.5.2 Inserting.Pipeline Registers 422 i -Ijir 

4.5.3 Rearranging and Relabeling Signals 426 

4.5.4 Next PC Prediction 427 • ' 

4.5.5 Pipeline Hazards 429 

4.5.6 Exception Handling 444 '' i 

4.5.7 PIPE Stage Implementations 447 

4.5.8 Pipeline Control Logic 455 

4.5.9 - Performance Analysis -464 

4.5.10 Unfinished Business 468 ^ 

4.6 Summary 470 

4.6.1 Y86-64 Simulators 472 ‘ 

Bibliographic Notes 473 
Homework Problems 473 
Solutions to Practice Problems 480 

a' 

Optimizing Program'Feiforiri^nc.e' 495 

5.1 Capabilities and Limitations'of Optimizing Compilers 498 

5.2 Expressing Program Performance. -502 

53 Program Example 504 

5.4 Eliminating LoopTnefficiencies 508 ^ ji 

5.5 Reducing Procedure Calls 512?^ < jH m 

5.6 Eliminating Unneeded Memory Referencesjj'514 -i* 

5.7 Understanding Modern Processors <517. 

5.7.1 Overall Operation 518 

5.7.2 Functional Unit Performance 523 

5.7.3 An Abstract Model of Processor Operation 525 

5.8 Loop Unrolling 53]3fa .fi 

5.9 Enhanqing Parallelism 536 

5.9.1 Multiple .Accumulators 536 

5.9.2 Reassociation Transformation 541 




5.10 Summary of Results for Optimizing Combining Code 547 

5.11 Some Limiting Factors 548 

5.11.1 Register Spilling 548 

5.11.2 Branch Prediction and-Mispredictipn Penalties ''549 

5.12 Understanding Memory Performance 553 i> ‘' -t 

5.12.1 Load Performance 554’it i 

5.12.2 Store Performance 555 

5.13 Life in the Real World: Performance Improvement Techniques - 561 

5.14 Identifying and Eliminating Performance Bottlenecks 562 

5.14.1 Program Profiling 562 

5.14.2 Using a Profiler to Guide Optimization 565 ' 

5.15 Summary 568 
Bibliographic Notes 569 
Homework Problems 570 
Solutions to Practice Problems 573 

6 ___ 

The Memory Hierarchy 579 

6.1 Storage Technologies 581 

6.1.1 Random Access Memory 581 

6.1.2 Disk Storage 589 

6.1.3 SoUd State Disks 600 

6.1.4 Storage Technology Trends 602 

6.2 Locality 604 

6.2.1 Locality of References to Program Data 606 

6.2.2 Locality of Instruction Fetches. ^ 607 

6.2.3 Summary of Locality 608 

6.3 The Memory Hierarchy 609 

6.3.1 Caching in the Memory Hierarchy 610 

6.3.2 Summary of Memory Hierarchy Concepts 614 

6.4 Cache Memories 614 

6.4.1 Generic Cache Memory Organization -615 

6.4.2 Direct-Mapped Caches 617 

6.4.3 Set Associative Caches 624 

6.4.4 Fully Associative Caches 626 

6.4.5 Issues with Writes 630 

6.4.6 Anatomy of a Real Cache Hierarchy 631 

6.4.7 Performance Impact of Cache Parameters 631 

6.5 Writing Cache-Friendly Code 633 

6.6 Putting It Together; The Impact of Caches on Program Performance 639 











6.6.1 "The Memory Mountain 639 

6.6.2 Rearranging Loops to Increase Spatial Locality 643 

6.6.3 Exploiting Locality in Your Programs '647 
6.7 Summary 648 

Bibliographic Notes 648 
Homework* Problems 649 
Solutions to Practice Problems 660 

> 

Part II Running Programs on a System 

7 _ 

Linking 669 

7.1 Compiler Drivers 671 

7.2 Static Linking 672 

7.3 Object Files 673 

7.4 Relocatable Object Files 674 

7.5 Syipbols and Symbol-Tables 675 

7.6 Symbol Resolution 679 

7.6.1 How Linkers Resolve Duplicate Symbol Names 680 

7.6.2 Linking with Static Libraries 684 

7.6.3 How Linkers Use Static Libraries to Resolve References 688 

7.7 Relocation 689 

7.7.1 Relocation Entries 690 

7.7.2 Relocating Symbol References 691 

7.8 6 Executable Object Files 695 

7.9 Loading Executable Object Files 697 

7.10 Dynamic Linking with Shared Libraries 698 

7.11 Loading and Linking Shared Libraries from Applications 701 

7.12 Position-Independent Code (PIC) 704 

7.13 Library Interpositioning 707 

7.13.1 Compile-Time Interpositioning 708 

7.13.2 Link-Time Interpositioning 708 

7.13.3 Run-Time Interpositioning 710 

7.14 Tools for Manipulating Object Files 713 

7.15 Summary 713 
Bibliographic Notes 714 
Homework Problems 714 
Solutions to Practice Problems 717 


8 ____ 

Exceptional Control Flow-? 721 

8.1 Exceptions 723 , 

8.1.1 Exception Handling 724 

8.1.2 Classes of Exceptions 726 

8.1.3 Exceptions in Linux/x86-64 Systems 729 

8.2 Processes 732 

8.2.1 Logical ControfFlow 732 

8.2.2 Concurrent Flows 733 j 

8.2.3 Private Address Space 734 

8.2.4 User 4nd Kernel Modes 734 

8.2.5 Context Switches 736 _ ^ ' 

8.3 System Call Error Handling 737 | 

8.4 Process Control 738 

8.4.1 Obtaining Process IDs 739 I 

8.4.2 Creating and Terminating Processes 739 

8.4.3 Reaping Child Processes 743 

8.4.4 Putting Processes to Sleep 749 -• 

8.4.5 Loading and Running Programs 750 ^ 

8.4.6 Using fork and execve to Run Programs 753 

8.5 Signals 756 

8.5.1 Signal Terminology 758 

8.5.2 Sending Signals 759 

8.5.3 Receiving Signals 762 

8.5.4 Blocking and Unblocking Signals 764 

8.5.5 Writing Signal Handlers 766 ^ 

8.5.6 Synchronizing Flows to Avoid Nasty. Concurrency Bugs 776 

8.5.7 Explicitly Waiting fof Signals 778 

8.6 Nonlocal Jumps 781 

8.7 Tools for Manipulating Processes. 786 

8.8 Summary 787 j 

Bibliographic Notes 787 ' 

Homework Problems 788 

Solutions to Practice Problems 795 

9 _ 

Virtual Memory 801 

9.1 Physical md Virtual Addressing 893 

9.2 Address Spaces 804 








Contents 


XV 


93 VM as a Tool for Caching 805 » 

9.3.1 DRAM Cache Organization 806 

9.3.2 Page Tables SOfr - 

9.3.3 Page Hits_ 808 

9.3.4 Page Faults 808- > 

9.3.5 Allocating Pages 810 

9.3.6 Locality to the Rescue Again 810 ■ f ' 

9.4 VM as a Tool fdr Memory Management 811 

9.5 VM as a Tool for Memory Protection ,812 

9.6 Address Translation 813 i 

9.6.1 Integrating Caches and y]M 817 

9.6.2 Speeding Up Address liranslation with a TLB 817 

9.6.3 Multi-Level Page Tables 819 

9.6.4 Putting It Together: End-to-End Address Translation 821 

9.7 Case Study: The Intel Core i7/Linux Memory System 825 

9.7.1 Core i7 Address Translation 826 

9.7.2 Linux Virtual Memory System 828 

9.8 Memory Mapping 833 

9.8.1 Scared Objects Revisited 833 

9.8.2 The fork Function Revisited 836 

9.8.3 The execve Function Revisited 836 

9.8.4 User-Level Memory Mapping with the ramap Function 837 

9.9 Dynamic Memory Allocation 83? 

9.9.1 The malloc and free Functions §40 

9.9.2 Why Dynamic Memory Allocation? 843 

9.9.3 Allocator Requirements and Goals 844 

9.9.4 Fragmentation 846 

9.9.5 Implementation Issues 846 * 

9.9.6 Implicit Free Lists 847 ^ 

9.9.7 Placing Allocated Blocks 849 ' • 

9.9.8 Splitting'Free Blocks •849 * ' ^ 

9.9.9 Getting Additional Heap Memoiy -850 '' 

9.9.10 Coalescing Free Blocks 850 * 

9.9.11 Coalescing with Boundary Tags 851 

9.9.12 Putting It Together: Implementing a Simple? Allocator 854 

9.9.13 Explicit Free Lists 862 

9.9.14 Segregated Free Lists 863 > 

9.10 Garbage Collection 865' 

9.10.1 Garbage Collector Basics 866 ‘ 

9.10.2 Mark&Sweep Garbage Collectors 867 ‘ ' 

9.10.3 Conservative Mark&Sweep for C Programs 869 ” 




9.11 Common Memory'Related Bugs in'C Programs' 870. 

9.11.1 Dereferencing Bad Pointers 870 

9.11.2 Reading Uninitialized Memory 871 

9.11.3 Allowing Stack Buffer Overflows 871 

9.11.4 Assuming That Pointers and the Objects They Point to 
Are the Same Size 872 

9.11.5 Making Off-by-One Errors 872 

9.11.6 Referencing a Pointer Instead of the Object It Points Xo 873 

9.11.7 Misunderstanding Pointer Arithmetic 873 

9.11.8 Referencing Nonexistent Variables 874 

9.11.9 Referencing Data in Free Heap Blocks 874 

9.11.10 Introducing Memory Leaks 875 

9.12 Summary 875 
Bibliographic Notes 876 
Homework Problems 876 
Solutions to Practice Prdblems 880 

Part III Interaction and Communication 
between Programs 

10 _ 

System-Level I/O 889 

10.1 Unix I/O 890 

10.2 Files 891 

10.3 Opening and Closing Files 893 

10.4 Reading and Writing Files 895 

10.5 Robust Reading and Writing with the Rio Package 897.. 

10.5.1 Rio Unbuffered Input and Output Functions 897 

10.5.2 Rip Buffered Input Functions 898 

10.6 Reading File Metadata 903 

10.7 Reading Directory Contents 905 

10.8 Sharing Files 906 v 

10.9 I/O Redirection 909 

10.10 Standard I/O 911 

10.11 Putting It Together: Which I/O Functions Should I Use? 911 

10.12 Summary 913 
Bibliographic Notes 914 
Homework Problems 914 
Solutions to Practice Problems 915 










Network Programming 917 

11.1 The Client-Server Programming Model 918 

11.2 Networks 919 

11.3 The Global IP Internet 924 

11.3.1 IP Addresses 925 

11.3.2 Internet Domain Names 927 

11.3.3 Internet Connections 929 ' ^ 

11.4 The Sockets Interface 932 

11.4.1 Socket Address Structures 933 

11.4.2 The socket Function 934 
11.4.3' The connect Function 934 

11.4.4 The bind Function 935 

11.4.5 The listen Function 935 

11.4.6 The accept Function 936 ’ 

11.4.7 Host and Service Conversion 937 

11.4.8 Helper Functions for the Sockets Interface 942 

11.4.9 Example Echo Client and Server ' 944 

11.5 Web Servers 948 

11.5.1 Web Basics 948 

11.5.2 Web Content 949 

11.5.3 HTTP Transactions 950 

11.5.4 Serving Dynamic Content 953 

11.6 Putting It Together: The Tiny Web Server 956 

11.7 Summary 964 
Bibliographic Notes 965 
Homework Problems 965 
Solutions to Practice Problems 966 

12 __ 

Concurrent Programming 971 

12.1 Concurrent Progr ammin g with Processes 973 

12.1.1 A Concurrent Server Based on Processes 974 

12.1.2 Pros and Cons of Processes 975 

12.2 Concurrent Programming with I/O Multiplexing 977 

12.2.1 A Concurrent Event-Driven Server Based on I/O 
Multiplexing 980 

12.2.2 Pros and Cons of I/O Multiplexing 985 
123 Concurrent Programming with Threads 985 

12.3.1 Thread Execution Model 986 





xviii Contents 


12.3.2 Posix Threads 987 

12.3.3 Creating Threads 988 

12.3.4 Terminating Threads 988 

12.3.5 Reaping Terminated Threads 989 

12.3.6 Detaching Threads 989 

12.3.7 Initializing Threads 990 

12.3.8 A Concurrent Server Based on Threads 991 

12.4 Shared Variables in Threaded Programs 992 

12.4.1 Threads Memory Model 993 

12.4.2 Mapping Variables to Memory 994 

12.4.3 Shared Variables 995 

* < 

12.5 Synchronizing Threads with Semaphores 995 

12.5.1 Progress Graphs 999 

12.5.2 Semaphores 1001 

12.5.3 Using Semaphores for Mutual Exclusion 1002 

12.5.4 Using Semaphores to Schedule Shared Resources. 1004 

12.5.5 Putting It Together: A Concurrent Server Based on 
Prethreading 1008 

12.6 Using Threads for Parallelism 1013 

12.7 Other Concurrency Issues 1020 

12.7.1 Thread Safety 1020 

12.7.2 Reentrancy 1023 

12.7.3 Using Existing Library Functions in Threaded Programs 1024 

12.7.4 Races 1025 

12.7.5 Deadlocks 1027 

12.8 Summary 1030 
Bibliographic Notes 1030 
Homework Problems 1031 
Solutions to Practice Proble ms 1036 

A_ 

Error Handling 1041- 

A.1 Error Handling in Unix Systems 1042 
A.2 Error-Handling Wrappers 1043 

References 1047 


Index 1053 










Preface 


This book (known as CS: APP) is for computer scientists, computer engineers, and 
others who want to be able to write better programs by learning what is going on 
“under the hdc(d”'of'a computer systeln. 

Our aim’is to explain the enduring concepts underlying all computer systenis, 
and to show you the cohcrete ways that these ideas affect the'correctness,'perfor¬ 
mance,'hnd utility of yourap^)lication ftrogra’ms/Many systems 6bbks are'Witten 
from a builder’s perspective, describing how to implement the hardware or th’e sys¬ 
tems softwar6, including the operating system, compiler, and network'ihtefface. 
This book is written from’a prog/dm’Mer^perspective,’desciihing how application 
programmers can use thdir knowledge of a system to write better programs.'Of 
course, learning what a system iS supposed to do provides-.a good first step in learn¬ 
ing how to build one,‘so this book also serves as a'valuable introduction to those 
who go on to implemeht systems h’ardward and software. Most systeihs books also 
tend to focus on just one aspect of the system, for example, the'hardware archi¬ 
tecture; the operating system, the cohipiler, or the network. This book'spans all 
of-these aspects,' with the uhifyirig theme of h progranfmer’s perspective. 

If you study andlearh‘.the-conceptSMn-this'bookryou';will"be'*on"yotirway tcT 
becoming the bdre power pi-d^raiUmef'-who knows how things work'and how to 
fix them wKen they bteak. You wiU'be able to writd prografns that'make'better 
use of the‘capiabi4ties provided by the*operating system'and systeihs software, 
that operate fcotrectly aci'oss'a wide' range of operating conditibhs and ruri-time 
parameters,' that run faster, and that avoid the flaws that make programs vulner¬ 
able t'6 cybefattack-. You will be prepared to delve deeper into’advariced topics 
suc'h as cbmpiIers,-‘computer architecture, ‘operating systems, embedded systems, 
networking, and cybersecurity. 

Assumptions about the Reader's Background ’ 

This book focuses on systems that execute x86-64 machine code. x86-64 is the latest 
in an evolutionary path followed by Intel and its competitors .that started with the 
8086 microprocessor in 1978. Due to the naming conventions used by Intel for 
its microprocessor line, this class of microprocessors is referred to colloquially as 
“x86.” As semiconductor technology has evolved to allow more transistors to be 
integrated onto a single, chip, these processors have progressed greatly in their 
computing, power, and theit memory capacity. As .part ofithis progression, (hey 
have gone from operating on* 16-bit wpi^s, to, 32-bit .words with the; introduction 
of IA32 processors, and most recently to 64-bit words with x86-64. 

We consider how these machines execute C programs on Linux. Linux is.one 
of a number* of operating systems'having them heritage in the Unix operating 
system developed originally by BeU Laboratories. Other members-of this class 





XX Preface 


,;|Ne^w” tp^^f"” ^fvicCpn|^h|,Ci3r6grai^nij,rtgJan^Uag"is\ ' ,,^,, “" *' „ 

4 K< s % * » « » ■%Z'‘^' f?? « ^ >-■''■ * 'fe ^'' a . v^ a • • "* 

«To help readers’whose background in.6programming4s weak (or nonexistent), we havef also included 
Sthese special fe'aturjes that are espedallyj^poftant in C We assume you are fajnilihr | 

-f Cs», S 

n, «w *yjh ri&v ^ -. ~ 


*Ww«#' « n!fl« !• 


of operating systems include Solaris, FreeBSD, and MacOS X. In recent years, 
these operating systems have maintained a high level of compatibility through the 
efforts of the Posix and Standard Unix Specification standardization efforts. Thus, 
the material in this book applies almost directly to these “Unix-like” operating 
systems. 

The.text eontains numerous programming examples that have been compiled 
and run on Linux systems. We assume thajyou have access to such a machine, and 
are able to log in and do simple things such as listing files and changing directo¬ 
ries. If your computer runs Microsoft Windows, we recommend that you install 
one of the many different virtual machine environments (such as VirtualBox or 
VMWare) that allow programs written for one operating system (the guest OS) 
to run under another (the host OS). 

We also assume that you have some familiarity with C or C+-t-. If your only 
prior experience is with Java, the transition will require more effort on your part, 
but we will help you. Java and C share similar syntax and control statements. 
However, there are aspects of C (particularly pointers, exphcit dynamic memory 
allocation, and formatted I/O) that do not exist in Java. Fortunately, C is a small 
language, and it is clearly and beautifully described in the classic “K&R” text 
by Brian Kernighan and Dennis Ritchie [61]. Regardless of your programming 
background, consider K&R an essential part of your personal systems library. If 
your prior experience ^ with an interpreted language, such as Python, Ruby, or 
Perl, you will definitely want to devote some time to learning C before you attempt 
to use this book. 

Several of the early chapters in the book explore the interactions between C 
programs and their machine-language counterparts. The machine-language exam¬ 
ples were all generated by the GNU ccc compiler running on x86-64 processors. 
We do not assume any prior experience with hardware, machine language, or 
assembly-language programming. 

How to Read the Book 

Learning how computer systems work from a programmer’s perspective is great 
fun, mainly because you can do it actively. Whenever you learn somethihg new, 
you can try it out right away and see the result firsthand. In fact, we believe that 
the only way to learn systems is to do systems, either working concrete problems 
or writing and running programs on real systems. 

This theme pervades the entire book. When a new concept is introduced, it 
is followed in the text by one or more practice problems that you should work 






Icode/intro/hello.c 


^ #include <stldio.h> 

2 

3 int mainO 

4 { 

5 printf("hello, world\n"); 

6 return 0; 

7 > 

- 1 -- code/intro/hello.c 

Figure 1 A typical cbde eic^mple. 


immediately to test your understanding. Solutions to the practice problems are 
at the end of each chapter. As you read, try to solve each problem on your own 
dnd then check the solution to make sure you are on the right track. Each chapter 
is followed by a set of homework problems of varying difficulty. Your instructor 
has the solutions to the homework problems in an instructor’s manual. For each 
homework problem, we show a rating 6f the amount of effort we feel it will require: 

I 

♦ Should require just a few minutes. Little or no programming required. 

Might, rfjquhe up, to 20 minutes. Often involves wqrjling, and .testing some 
code. (Many of these,arq derived from problems, we ha,ve given on exams.) 

♦♦♦ Requires a significanticffort, perhaps 1-2 hours. Generally involves writ¬ 
ing and testing a significant amount of code.. 

♦♦♦♦ A lab assignment, requiring up to 10 hours of effort. 

Each code example in the text was formatted directly, without,any manual 
intervention, from a C piogram compilecf with gcc and' tested on a Linux' system. 
Oftoursfe, your system may have a differeht version of gcc, or a different compiler 
altogether, so your compiler might generate different machme code; but the 
overall behavior should be the same. All 6f thb,source coddul^available from the 
CS:APP Web page (“CSiXPP” being our sliorthanJfor the' book’s title) at csapp 
.cs.cmu.edu. In the text, the filenariids of the source program^ are documented 
in horizontal bars that sufround the formatted code. For example*, the program in 
Figure 1 can be found in the file hello. c in directory code/intro/. We encourage 
yoiflo try running the example programs on your system as you encounter them. 

To avoid having a,book that is overwhelming, both in'bidk and in content, we 
have; created *a*number of.i^b asides containing material that.supplements the 
main presentatioh oflhebool5.firhese asides are referencedxwithin the book with 
a notation’of the form chaktop, where chap is a short encoding of the chapter sub- 
»ject, and TOE;is.a shortcode fonthe topiethat is cOvered.-For example, Web Aside 
data:bool contains sujiplementary material on-Booleanalgebra for the presenta¬ 
tion on data representations in Chapter 2, while Web Aside akch:vlog contains 





material describing processor designs using the Verilog hardware description lan¬ 
guage, supplementing the presentation of processor design in Chapter 4. All of 
these Web asides are available from the CS:APP Web page. 

Book Overview 

The CS:APP book consists of 12 chapters designed to capture the core ideas in 
computer systems. Here is an overview. 

Chapter 1: A Tour of Computer Systems. This chapter introduces the major ideas 
and themes in computer systems by tracing the life cycle of a simple “hello, 
world” program. 

Chapter 2: Representing and Manipulating Information. We cover computer arith¬ 
metic, emphasizing the properties of unsigned and two’s-complement num¬ 
ber representations that affect programmers. We consider how numbers 
are represented and therefore what range of values can be encoded for 
a given word size. We consider the effect of casting between signed and 
unsigned numbers. We cover the mathematical properties of arithmetic op¬ 
erations. Novice programmers are often surprised to learn that the (two’s- 
complement) sum or product of two positive numbers can be negative. On 
the other hand, two’s-complement arithmetic satisfies many of the algebraic 
properties of integer arithmetic, and hence a compiler can safely transform 
multiplication by a constant into a sequence of shifts and adds. We use the 
bit-level operations of C to demonstrate the principles and applications of 
Boolean algebra. We cover the IEEE floating-point format in terms of how 
it represents values and the mathematical properties of floating-point oper¬ 
ations. 

Having a solid understanding of computer arithmetic is critical to writ¬ 
ing reliable programs. For example, programmers and compilers cannot re¬ 
place the expression (x<y) with (x-y < 0), due to the possibility of overflow. 
They cannot even replace it with the expression (-y < -x), due to the asym¬ 
metric range of negative and positive numbers in the two’s-complenient 
representation. Arithmetic overflow is a common source of programming 
errors and security vulnerabilities, yet few other books cover the properties 
of computer arithmetic from a programmer’s perspective. 

Chapter 3: Machine-Level Representation of Programs. We teach you how to read 
the x86-64 mac hi ne code generated by a C compiler. We cover the ba¬ 
sic instruction patterns generated for different control constructs, such as 
conditionals, loops, and switch statements. We cover the implementation 
of procedures, including stack allocation, register usage conventions, and 
parameter passing. We cover the way different data structures such as struc¬ 
tures, unions, and arrays are allocated and accessed. We cover the instruc¬ 
tions that implement both integer and floating-point arithmetic. We also 
use the machine-level view of programs as a way to understand common 
code security vulnerabilities, such as buffer overflow, and steps that the pro- 






Preface xxiii 


I A .'ll— ®%«'vu_j. :-•■__:j£S* * ^ ’ ’*. * 


>V$^de 


'»j. 


'Ssfn 


% I 


* 




WMWS. ■*» «ws>*. „ •'► 

iW 








L You^vill encounter ^^idds^o£this *fo"rm^throu|hotft^t]:iCtex1;f^sides gifc^ reto'^lcfthSf^iv^* 

[ you soiSe^adclitionalinSight^ihto the cifrrent«tdpic.;)^sides se^eth%uni 8 er’ 0 f purj^oses. Some, are lit^tp ,,, 
^history.fessoiis/;^orexamjle3vhere did C, nnu^f and‘tfettntertfe%:prfie’£r®mtt)"ther asides are meant 
t 6 clarify ideas that student 5 ^tten^fi|dfcpn&sihg;^Q;^elample, wfiatHs'thd difference ^ cache 

liife, set, and 81ock?«'Other asides give rear-v?orldfex^rffples, shcfe dsliqw |i fl'pating-pdmt e|ror crashed , 
a'French'^qcketpr the* gedmetric“and operational p^apel^soha c^rtMefd^Y|lisl*dri\re. Fipalfy, somp ,* 
'asides’are^ust-lun'stuff. FoEp^amplej.whht 5 s»%’“hQinkyj|t|,,^ 

t. »■ ££*.•* 't,±' ilSik <■«!«>«> *« ‘V >lfi. •» ^ 


MUiS ’■f 


Stm? >5.-. 


grammer, the compiler, and the operating system can take to reduce these 
threats. Learning the concepts in this chapter helps you become a better 
programmer, because you will understand how programs are represented 
on a mac hin e. One certain benefit is that you■ will develop a thorough and 
concrete understanding of pointers. 

Chapter 4: Processor Architecture. This chapter covers basic combinational and 
sequential logc elements, and then shows how these elements can be com¬ 
bined in a datapath that executes a simplified subset of the x86-64 instruction 
set called “Y86-64.” We begin with the design of a single-cycle datapath. 
This design is conceptually very simple, but it would not be very fast. We 
then introduce pipelining, where the different steps required to process an 
instruction are implemented as separate stages. At any given .time, each 
stage can work on a different instruction. Ojur five-stage processor pipeline is 
much more realistic. The control logic for the processor designs is described 
using a simple hardware description language called HCL. Hardware de¬ 
signs written in HCL can be compiled and linked intO‘simulators provided 
with the textbook, and they can be used to generate Verilog descriptions 
suitable for synthesis into working hardware. 

Chapter 5: Qptimizing program Performance. This chapter introduces a number 
of techniques for improving code performance, with the idea being that pro¬ 
grammers lean} to write their C code in such,a way that a compiler can then 
generate efficient machine code. We start with transformations that reduce 
the work to be dope t}y a program andjjencp ^hould be standar.d practice 
when writing any pfogram for any machine. We then progress to trans¬ 
formations that enhance the degree of instruction-level parallelism in the 
generated machine code, thereby improving their performance on modern 
“superscalar” processors. To motivate these transformations, we introduce 
a simple operational model of how modern out-of-order processors work, 
and show how to measure the potential performance of a program in terms 
of the critical paths through a graphical representation of a program. You 
will be surprised how much you can speed up a program by simple transfor¬ 
mations of the C code. 



xxiv Preface 


Chapter 6: The Memory Hierarchy. The memory system is one of the most visible 
parts of a computer system to application programmers. To this point, you 
have relied on a conceptual model of the memory system as a linear array 
with uniform access times. In practice, a memory system is a hierarchy of 
storage devices with different capacities, costs, and access times. We cover 
the different types of RAM and ROM memories and the geometry and 
organization of magnetic-disk and>solid state drives. We describe how these 
storage devices are arranged in a hierarchy. We show how this hierarchy is 
made possible by locality of reference. We make these ideas concrete by 
introducing a unique view of a memory system as a “memory mountain” 
with ridges of temporal locality and slopes of spatial locality. Finally, we 
show you how to improve the performance of application programs by 
improving their temporal and spatial locality. 

Chapter 1: Linking. This chapter covers both static and dynamic linking, including 
the ideas of relocatable and executable object files, symbol resolution, re¬ 
location, static libraries, shared object libraries, position-independent code, 
and library interpositioning. Linking is not covered in most systems texts, 
but we cover it for two reasons. First, some of the most confusing errors that 
programmers can encpunter are related to glitches during linking, especially 
for large software packages. Second, the object files produced by linkers are 
tied to concepts such as loading, virtual memory, and memory mapping. 

i 

Chapter 8: Exceptional Control Flow. In this part of the presentation, we step 
beyond the single-program model by introducing the general concept of 
exceptional control flow (i.e., changes in control flow that are outside the 
normal branches and procedure calls). We cover examples of exceptional 
control flow that exist at all levels of the system, from low-level hardware ex¬ 
ceptions and interrupts, to context switches between concurrent processes, 
to abrupt changes in control flow caused by the receipt of Linux signals, to 
the nonlocal jumps in C that break the stack discipline. 

This is the part of the book where we introduce the fundamental idea 
of a process, an abstraction of an executing program. You will learn how 
processes work and how they can be created and manipulated from appli¬ 
cation programs. We show how application programmers can make use of 
multiple processes via Linux system calls. When you finish this chapter, you 
will be able to write a simple Linux shell with job control. It is also your first 
introduction to the nondeterministic behavior that arises with concurrent 
program execution. 

Chapter 9: Virtual Memory. Our presentation of the virtual memory system seeks 
to give some understanding of how it works and its characteristics. We want 
you to know how it is that the different simultaneous processes can each use 
an identical range of addresses, sharing some pages but having individual 
copies of others. We also cover issues involved in managing and manip¬ 
ulating virtual memory. In particular, we cover the operation of storage 
allocators such as the standard-library malloc and free operations. Cov- 










ering this material serves several purposes. It reinforces the concept that 
the virtual membry space is just an array of bytes that the program can 
subdivide into different storage units. It helps you understand the effects 
of programs containing memory referencing errors such as storage leaks 
an^l invalid pointer references. Finally, many application programmers write 
’their own storage allocators optimized toward the needs and characteris¬ 
tics of the apjilication. This chapter, more than any other, demcinstrates the 
benefit of covering both the'hardw'are and the software aspects'of computer 
systems in a unified way. Traditional computer architecture and operating 
systems texts present only part of the virtual memory story. 


Chapter 10: System-Level I/O. We cover the basic concepts of Unix I/O such as 
files and descriptors. We describe how files are shared, how I/O redirection 
works, and how to access file metadata. We also develop a robust buffered 
I/O package that deals correctjy with a curigus behavior known as,sh,ort 
counts, where the library function reads onjy parfe of the ipput data. We 
cover the C standard I/O library and its relationship to Linux I/O, focusing 
on limitations of standard I/O that make it unsuitable fbr'network program- 
minjg. In -general, the topics covered in this’chapter are “building blocks for 
the next two chapters on network and concurrent programming. 

I 

Chapter 11; Network Programming. Networks are interesting I/O devices to pro¬ 
gram, tying together many of the ideas that we study earlier in the text, such 
as processes, signals, byte ordering, memory mapping, and dynamic storage 
allocati'dn. Netyvofk programs also provide a -cbmpeUing context for con¬ 
current^, which is the topit df the next chapfef. This chapter^ is* a thin slice 
through network programminjg that' gets you to the pfeiht where you can 
wiritn'k simf)le Web server. We cover the client-server model that underlies 
all network applications. We pre^eftt a programmer’s View of the Internet 
and show how to write Internet clients ah’d servers using the sbckets inter¬ 
face. Finally, we introduce HTTP and develop a simple iterative Web server. 


Chapter 12: Concurrent Programming. This chapter introduces concurrent pro¬ 
gramming using Internet'se'rver design as the funffing motivational'example. 
We compare and contrast the three basic mechanisms for writing concur¬ 
rent programs-^processeSj^FO multiplexing, and threads—apd shpw how 
to use them to build concurrent Internet servers. We cover basic principles 
of synchrgnization.using P and y semaphore operations, thread safety and 
^ reentrancy, race conditions, anb deadlocjcs. Writing concurrent code is es¬ 
sential for mo^t server applfcations. We alsc^^^escribp the use of thread-level 
programming to express parallelism in,an application^pro^am, enabling 
faster execution on multi-core processors. Getting all of the cores working 
on a single computational problem requires a, careful coordination of the 
concurrent threads, both for correctness and to achieve high p)erformance. 




New to This Edition 


The first edition of this book was published with a copyright of 2003, while the 
second had a copyright of 2011. Considering the rapid evolution of computer 
technology, the book content has held up surprisingly well. Intel x86 machines 
running C programs under Linux (and related operating systems) has proved to 
be a combination that continues to encompass many systems today. However, 
changes in hardware technology, compilers, program library interfaces, and the 
experience of many instructors teaching the material have prompted a substantial 
revision. 

The biggest overall change from the second edition is that we have switched 
our presentation from one based on a mix of IA32 and x86-64 to one based 
exclusively on x86-64. This shift in focus affected the contents of many of the 
chapters. Here is a summary of the significant changes. 

Chapter 1: A Tour of Computer Systems We have moved the discussion of Am¬ 
dahl’s Law from Chapter 5 into this chapter. 

Chapter 2: Representing anfi Manipulating Information. A consistent bit of feed¬ 
back from readers and reviewers is that some of the material in this chapter 
can be a bit overwhelming. So we have tried to make the material more ac¬ 
cessible by clarifying the points at which we delve into a more mathematical 
style of presentation. This enables readers to first skim over mathematical 
details to get a high-level overview and then return for a more thorough 
reading. 

Chapter 3: Machine-Level Representation of Programs. We have converted from 
the earlier presentation based on a mix of IA32 and x86-64 to one based 
entirely on x86-64. We have also updated for the style of code generated by 
more recent versions of gcc. The result is a substantial rewriting, including 
changing the order in which some of the concepts are presented. We also 
have included, for the first time, a presentation of the machine-level support 
for programs operating on floating-point data. We have created a Web aside 
describing IA32 machine code for legacy reasons. 

Chapter 4: Processor Architecture. We have revised the earlier processor design, 
based on a 32-bit architecture, to one that supports 64-bit words and oper¬ 
ations. 

Chapter 5: Optimizing Program Performance. We have updated the material to 
reflect the performance capabilities of recent generations of x86-64 proces¬ 
sors. With the introduction of more functional units and more sophisticated 
control logic, the model of program performance we developed based on a 
data-flow representation of programs has become a more reliable predictor 
of performance than it was before. 

Chapter 6: The'Memory Hierarchy. We have updated the material to reflect more 
recent technology. 












Chapter 7: Linking. We have rewritten this chapter, for x86-64, expanded the 
discussion of using the GOT and PLT to create position-independent code, 
and added a new section on a powerful linking technique known as library 
interpositioning. 

Chapter SpExgeptiori'al Control Flow.^ We have added a more rigorous treatment 
of signal* handlers, including async-signal-safe functions, specific guidelines 
for writin^^si^iial handlers, and using sigsuspend to wait for handlers. 

Chapter 9: Virtual Memory. This chapter has changed only slightly. 

(^^apterJO.-^Sy'ftern-Level I/O. We haye acfded q new section on files and the file 
hierarchy, but ptherwise, this chapter has changed only slightly. 

Chapter 11: Network Programming. We have introduced techniques for protocol- 
independent and thread-safe network progra mmin g using the modern 
getaddrinf 0 and getnameinfo functions, which replace the obsolete and 
non-reentrant gethostbyname and gethostbyaddr functions. 

Chapter 12: Concurrent Programming. We have increa^d our coverage of using 
thread-level parallelism to make programs run faster on multi-core ma¬ 
chines. 

In addition, we have added and revised a number of practice and homework 
problems throughout the text. 

Origins of the Book 

This book stems from an introductory course that we developed at Carnegie Mel¬ 
lon University in the fall of1998, called 15-213: Introduction to Computer Systems 
(ICS) [14]. The ICS cburse has'been taught every semester since then. Over 400 
studentsdake the course each semester. TTie students range from sophomores to 
graduate gtudents in a wide variety of majors. It is a required core course for all 
undergraduates in the C§ and ECE departments at Carnegie Mellon, and it has 
become a prerequisite for most upper-level systems courses in CS and ECE. 

The idea with ICS was to introduce students to computers in a different way. 
Few of our students Would have the dppprtimity to build a computer system. On 
the other hand, most students, incli/ding all computer scientists and computer 
engineers, would be required to use and program computers on a daily basis. So we 
decided to teach about systems from the p6int of view of the programmer, using 
the following filter: we would cover a topic only if it affected the performance, 
correctness, or. utility of user-level C programs. 

For example, tojiics such as hardwafe adder and bus designs were out. Top¬ 
ics such as machine language were in; but instead of focusing on how to write 
assembly language by hand, we would look at how a C compiler translates C con¬ 
structs into,machine code, including pointers, loops, procedure calls, and switch 
statements. Further, we wohld take a broader and more holistic view of the system 
as both hardware and systems software, covering such topics as linking, loading. 




processes, signals, performance optimization, virtual memory, I/O, and network 
and concurrent programming. 

This approach allowed us to teach the ICS course in a way that is practical, 
concrete, hands-on, and exciting for the students. The response from our students 
and faculty colleagues was immediate and overwhelmingly positive, and we real¬ 
ized that others outside of CMU might benefit from using our approach. Hence 
this book, which we developed from the ICS lecture notes, and which we have 
now revised to reflect changes in technology and in how computer systems are 
implemented. 

Via the multiple editions and multiple translations of this book, ICS and many 
variants have become part of the computer science and computer engineering 
curricula at hundreds of colleges and universities worldwide. 

For Instructors: Courses Based on the Book 

Instructors can use the CS:APP book to teach a number of different types of 
systems courses. Five categories of these courses are illustrated in Figure 2. The 
particular course depends on curriculum requirements, personal taste, and 
the backgrounds and abilities of the students. From left to right in the figure, 
the courses are characterized by an increasing emphasis on the programmer’s 
perspective of a system. Here is a brief description. 

ORG. A computer organization course with traditional topics covered in an un- 
traditional style. Traditional topics such as logic design, processor architec¬ 
ture, assembly language, and memory systems are covered. However, there 
is more emphasis on the impact for the programmer. For example, data rep¬ 
resentations are related back to the data types and operations of C programs, 
and the presentation on assembly code is based on machine code generated 
by a C compiler rather than handwritten assembly code. 

ORG+. The ORG course with additional emphasis on the impact of hardware 
on the performance of application programs. Compared to ORG, students 
learn more about code optimization and about improving the memory per¬ 
formance of their C programs. 

ICS. The baseline ICS course, designed to produce enlightened programmers who 
understand the impact of the hardware, operating system, and compilation 
system on the performance and correctness of their application programs. 
A significant difference from ORG-i- is that low-level processor architecture 
is not covered. Instead, programmers work with a higher-level model of a 
modern out-of-order processor. The ICS course fits nicely into a 10-week 
quarter, and can also be stretched to a 15-week semester if covered at a 
more leisurely pace. 

ICS-I-. The baseline ICS course with additional coverage of systems programming 
topics such as system-level I/O, network programming, and concurrent pro¬ 
gramming. This is the semester-long Carnegie Mellon course, which covers 
every chapter in CS:APP except low-level processor architecture. 




Chapter 

Topic , 

t 

ORG 

^ ‘ ’ Course 
QRG+ ICS 

ICS-I- 

SP 

■1 

'Tour of systems 

• 

1 , 

• 

• 


2 

Datk repres'ehtation 

• 

• 

• 

• 

©(d) 

3 

Machine language. 

• ' 

• 

• 

• 

• 

4 

Processor architecture 

• 

• 




5 

Code optimization 


• 

« 

• 



Meraojy hierarchy 

©(a) 

• 

• 

• 

©(a) 

7 

Linking jj, , 



©(e) 

©(c) 

• 

'8. 

Exceptional pontrol, flow- 



• 

• 

# 

9 

Virtual memory 

0(b) 

• 


* 

♦ 

10 

System-level I/O 





• 

11 

N etwork programming 




• 

• 

12 

^ Concurrent programming 

, 



• 

• 


Figure 2 Fiv^ systems courses based on the CS:APP book. ICsVis the 15-213 course 
from Carnegie Mellon'. Notes: The © symbol denotes partial coverage of a chapter, as 
follows: (a) hardware only; (b) no dynamic storage cillocatfon; (c) no dynamic'Iinking; 
(d) no floating point 


SP. A sys'teiks pro'gtamifaing course. This coiirse is'siniilar to ICS-i-, 6ut if drops 
floating point taiid performance‘optimization,'and it places more empha¬ 
sis on systems 'programming, including process pontrol, dynamic linking, 
system-level I/O, getwork prpgramming, and concurrent programming. In- 
structQrs mjght.vyant to supplement frem other sources for advanced topics 
^such as daenjogs,]tenninal control, an4UnixTPC. 

The main message of figure 2 is that tjip CS:APP‘booJc gives a lot pf options 
to students and instructors. If you want your students to be exposed to lower- 
level processor architecture, then that option is available via the ORG' and ORG-i- 
courses. On the other hand, if you want to switch from your current computer 
organization course /to an ICS or ICS-i- course, but are wary of making such a 
drastic change all at once, then you jcan move toward ICS incrementally. You 
can start with ORG,>Which teaches the traditional topics in a nontraditional way. 
Once you are comfortable with that material, then you can move to ORG-i-, 
and eventually to'tCS! If students have no experience in C (e.g., they have only 
proppimmed in Java),,you could spend several weeks on C and then cover the 
material of ORG or ICS. 

Finally, we note that the ORG+ and SP courses would make a nice two-term 
sequence (either quarters or semesters). Or you might consider offering ICS-t- as 
one term of ICS and one term of SP. 










For Instructors; Classroom-Tested Laboratory Exercises 

The ICS+ course at Carnegie Mellon receives very high evaluations from students. 

Median scores of 5.0/5.0 and means of 4.6/5.0 are typical for the student course 

evaluations. Students cite the fun, exciting, and relevant laboratory exercises as 

the primary reason. The labs are available from the CS:APP Web page. Here are 

examples of the labs that are provided with the book. 

Data Lab. This lab requires students to implement simple logical and arithmetic 
functions, but using a highly restricted subset of C. For example, they must 
compute the absolute value of a number using only bit-level operations. This 
lab helps students understand the bit-level representations of C data types 
and the bit-level behavior of the operations on data. 

Binary Bomb Lab. A binary bomb is a program provided to students as an object- 
code file. When run, it prompts the user to type in six different strings. If 
any of these are incorrect, the bomb “explodes,” printing an error message 
and logging the event on a grading^ server. Students must “defuse” their 
own unique bombs by disassembling and reverse engineering the programs 
to determine what the six strings shoqld be. The lab teaches students to 
understand assembly language and also forces them to learn ,how to use a 
debugger. 

Buffer Overflow Lab. Students are required to modify the run-time behavior of 
a binary executable by exploiting a buffer overflow vulnerability. This lab 
teaches the students about the stack discipline and abput the ganger of 
writing code that is vulnerable to buffer overflow attacks. 

Architecture Lab. Several of the homework problems of Chapter 4 can be com¬ 
bined into a lab assignment, where students modify the HCL description of 
a processor to add new instructions, change the branch prediction policy, or 
add or remove bypassing paths and register ports. Thfe resulting processors 
can be simulated and run through automated tests that will detect most of 
the possible bugs. This lab lets students experience the exciting parts of pro¬ 
cessor design without requiring a complete background in logic design and 
hardware description languages. 

Performance Lab. Students must optimize the performance of an application ker¬ 
nel function such as convolution or matrix transposition. This lab provides 
a very clear demonstration of the properties of cache memories and gives 
students experience with low-level program optimization. 

Cache Lab. In this alternative to the performance lab, students write a general- 
purpose cache simulator, and then optimize a small matrix transpose kernel 
to minimize the number of misses on a simulated cache. We use the Valgrind 
tool to generate real address traces for the matrix transpose kernel. 

Shell Lab. Students implement their own Unix shell program with job control, 
including the Ctrl-l-C and Ctrl+Z keystrokes and the fg, bg, and jobs com- 













mands. This is the student’s! first introduction to concurrency, and it gives 
them a clear idea of Unix process control, signals, and signal-handling. 

Malloc Lab. Students implement their own versions'of ma'lloc, free, and (op¬ 
tionally) reallbc. This lab gb>^es students a clear understanding of data 
layout and organization, and requires them to evaluate different trade-offs 
betweeh space and time^ efficiency. 

Proxy Lahii^tudents implement a concurrent;Web ^roxy tha^sits between their 
i, browsers and the rest of the World Wide Web. This lab exposes the students 
to such topics as Web clients and servers, and ties together many of the con¬ 
cepts from the course, such as byte ordering, file I/O, process control, signals, 
signal handling, memory mapping, sockets, and concurrency. Students like 
being able to see their programs in action with real Web browsers and Web 
servers. ,e. •' 

The CS:APP instructor’s manual has a detailed discussion of the labs, as well 
as directions for downloading the support software. 

i 

Acknowledgments for the Third Edition 

It is a pleasure fo acknowledge and thank those who have helped us produce this 
third edition of,the CS'.APP text. 

We would like to tha nk our Carnegie Mellon colleagues who have taught the 
ICS course over the years and who ha^'e provided so much in'sightful feedback 
and encouragement: Guy Blelloch, Roger Dannenberg, David Eckhardt, Franz 
Franchetti, Greg Ganger, Seth Goldsteui, Khaled Harras,^Greg Kesden, Bruee 
Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, Markus Pueschel, and 
Anthony Rowe. David Wihters was very helpful in installing and configuring the 
reference Linux box. .i 

Jason Fritts (St. Louis University) and'Cindyi Norris (Appalachian State) 
provided us with detailed and thoughtful reviews of the second edition. Yili Gong 
(Wuhan University) wrote the Chinese'trahslation, maintained-the errata page for 
the Chinese.version, and contributed many bug reports. Godmar Back (Virginia 
Tech) helped us improve the text significantly by introducing us to the.notions of 
async-signal safety and protocol-independent'network programming. 

Many, thanks to our eagle-eyed readers who reported bugs in the second edi¬ 
tion: Rami Ammari, Paul Anagndstopoulos, Lucas Barenfanger, Godmar Back, 
Ji Bifi, Shhrbel Bousemaan, RicTiarddLallahan, Seth Chaiken, Cheng Chen, Libo 
Chenl Tao Du, Pascal Garcia,, Yili Xiong, Ronald Greenberg, Dortikhan Giiloz, 
Dong Han,,Dominik Helm, Ronald Jones, Mustafa Kazdagli, Gordon Kindlmann, 
Sankdr Krishnan, Kanak Kshetri, Junhn Lu, Qiangqiang Luo, Sebastian Luy, 
Lei Ma, Ashwin Nanjappa, Grdgoire Paradis, Jonas Pfenninger, Karl Pichotta, 
David Ramsey, Kaustabh Roy, David.Selvaraj, Sankar Shanmugam, Dbminique 
Smulkowska, Dag S0rb0, Michael Spear, Yu Tanaka, Steven Tricanowicz, Scott 
Wright, Waiki Wright, Han Xu, Zhengshan Yan, Firo Yang, Shuang«Yang, John 
Ye, Taketo Yoshida, Yan Zhu, and Michael Zink. 





Thanks also to our readers who have contributed to the labs, including God- 
mar Back (Virginia Tech), Taymo'n Beal (Worcester Polytechnic Institute), Aran 
Clauson (Western Washington University), Cary Gray (Wheaton College), Paul 
Haiduk (West Texas A&M University), Ten Harney (Macquarie University), Ed¬ 
die Kohler (Harvard), Hugh Lauer (Worcester Polytechnic Institute), Robert 
Marmorstein (Longwood University), and James Riely (DePaul University). 

Once again, Paul Anagnostopoulos of Windfall Software did a masterful job 
of typesetting the book and leading the production process. Many thanks to Paul 
and his stellar team: Richard Camp (copyediting), Jennifer McClain (proofread¬ 
ing), Laurel Muller (art production), and Ted Laux (indexing). Paul even spotted 
a bug in our description of the origins of the acronym BSS that had persisted 
undetected since the first edition! 

Finally, we would like to thank our friends at Prentice Hall. Marcia Horton 
and our editor. Matt Goldstein, have been unflagging in their support and encour¬ 
agement, and we are deeply grateful to them. 

Acknowledgments from the Second Edition 

We are deeply grateful to the many people who have helped us produce this second 
edition of the CS. APP text. 

First and foremost, we would like to recognize our colleagues who have taught 
the ICS course at Carnegie Mellon for their insightful feedback and encourage¬ 
ment; Guy Blelloch, Roger Dannenberg, David Eckhardt, Greg Ganger, Seth 
Goldstein, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank 
Pfenning, and Markus Pueschel. 

Thanks also to our sharp-eyed readers who contributed reports to the errata 
page for the first edition: Daniel Amelang, Rui Baptista, Quarup Barreirinhas, 
Michael Bombyk, Jorg Brauer, Jordan Brough, Yixin Cao, James Caroll, Rui Car¬ 
valho, Hyoung-Kee Choi, A1 Davis, Grant Davis, Christian Dufour, Mao Fan, 
Tim Freeman, Inge Frick, Max Gebhardt, Jeff Goldblat, Thomas Gross, Anita 
Gupta, John Hampton, Hiep Hong, Greg Israelsen, Ronald Jones, Haudy Kazemi, 
Brian Kell, Constantine Kousoulis, Sacha Krakowiak, Arun Krishnaswamy, Mar¬ 
tin Kulas, Michael Li, Zeyang Li, Ricky Liu, Mario Lo Conte, Dirk Maas, Devon 
Macey, Carl Marcinik, Will Marrero, Simone Martins, Tao Men, Mark Morris¬ 
sey, Venkata Naidu, Bhas Nalabothula, Thortias Niemann, Eric Peskin, David Po, 
Anne Rogers, John Ross, Michael Scott, Seiki, Ray Shih, Darren Shultz, Erik 
Silkensen, Suryanto, Emil Tarazi, Nawanan Theera-Ampornpunt, Joe Trdinich, 
Michael Trigoboff, James Troup, Martin Vopatek, Alan West, Betsy Wolff, Tim 
Wong, James Woodruff, Scott Wright, Jackie Xiao, Guanpeng Xu, Qing Xu, Caren 
Yang, Yin Yongsheng, Wang Yuanxuan, Steven Zhang, and Day Zhong. Special 
thanks to Inge Frick, who identified a subtle deep copy bug in our lock-and-copy 
example, and to Ricky Liu for his amazing proofreading skills. 

Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally 
supportive throughout the writing of the text. Steve Schlosser graciously provided 
some disk drive characterizations. Casey Helfrich and Michael Ryan installed 







Preface 


xxxiii 


and maintained our-new Core il box.-Michael Kozuch, Babu Pillai, and Jason 
Campbell provided valuable insight .on memory system performance, multi-core 
systems, and the power wall. Phil Gibbons and Shimin Chen shared their consid-; 
erable expertise on solid state disb designs. 

We have been able to call on-the talents'of many, including Wen-Mei Hwu; 
Markus Pueschel, and Jiri Simsa, to provide both detailed comments and high- 
level advice. James Hoe helped us create a Verilog version of the Y86 processor 
and did all of the'work needed to synthesize working hardware. 

Many thanks to our colleagues wha-provided re views, o£> the draft manu¬ 
script: James Archibald (Brigham Young University), Richard Carver (George 
Mason University), Mirela Damian (Villanova University), Peter Dinda (North¬ 
western University), John Fiore (Tfemple University), Jason Fritts (St; Louis Uni¬ 
versity), John Greiner (Rice University), Brian Harvey (University of California, 
Berkeley), Don Heller (Penn State University), Wei .Chung Hsu (University of 
Minnesota), Michelle Hugue (University of Maryland),-Jeremy Johnson (Drexel 
University), Geoff Kuenning (Harvey Mudd College), Ricky Liu, Sam,Mad¬ 
den (MIT), Fred Martin (University of Massachusetts, Lowell), Abraham Matta 
(Boston University), Markus Pueschel (Carnegie Mellon University), Norman 
Ramsey (Tufts University), Glenn Reinmann (UCLA); Michela Taufer (Univer¬ 
sity of Delaware), and Craig Zilles (UIUC). 

Paul Anagnostopoulos of Windfall Software did an outstanding job of type¬ 
setting the book and leading the production team. Many thanks to Paul and his 
superb team: Rick Camp (copyeditor), Joe Snowden (compositor), Mary Ellen N. 
Oliver (proofreader)-. Laurel Muller (artist), and Ted Laux (indexer). 

Finally, we would like to thank our friends at Prentice Halt; Marcia Horton has 
always been there for us. Our editor. Matt Goldstein, provided stellar leadership 
from’’beginning to end. We are profoundly grateful for their help, encouragement, 
and insights. 

Acknowledgments from the First Edition 

We are deeply indebted to many friends and colleagues for their thoughtful crit¬ 
icisms and encouragement. A special thanks to our 15-213 students, whose infec¬ 
tious energy and enthusiasm spurred us on. Nick Carter and Vinny Furia gener¬ 
ously provided their malloc package. 

Guy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mo wry taught the course 
over multiple semesters, gave us encouragement, and helped improve the course 
material. Herb Derby provided early spiritual guidance and encouragement. Al¬ 
lan Fisher, Garth Gibson, Thomas Gross, Satya, Peter Steenkiste, and Hui Zhang 
encouraged us to develop the course from the start. A suggestion from Garth 
early on got the whole ball rolling, and this was picked up and refined with the 
help of a group led by Allan Fisher. Mark Stehlik and Peter Lee have been very 
supportive about building this material into the undergraduate curriculum. Greg 
Kesden provided helpful feedback on the impact of ICS on .the OS course. Greg 
Ganger and Jiri Schindler-graciously provided some disk drive characterizations 




and answered our questions on modern disks. Tom Strieker showed us the mem¬ 
ory mountain. James Hoe provided useful ideas and feedback on how to present 
processor architecture. 

A special group of students—Khalil Amiri, Angela Demke Brown, Chris 
Colohan, Jason Crawford, Peter Dinda, Julio Lopez, Bruce Lowekamp, Jeff 
Pierce, Sanjay Rao, Balaji Sarpeshkar, Blake Scholl, Sanjit Seshia, Greg Stef- 
fan, Tiankai TU, Kip Walker, and Tmglian Xie—were instrumental in helping 
us develop the content of the course. In particular, Chris Colohan established a 
fun (and funny) tone that persists to this day, and invented the legendary “binary 
bomb” that has proven to be a great tool for teaching machine code and debugging 
concepts. 

Chris Bauer, Alan Cox, Peter Dinda, Sandhya Dwarkadas, John Greiner, 
Don Heller, Bruce Jacob, Barry Johnson, Bruce Lowekamp, Greg Morrisett, 
Brian Noble, Bobbie Othmer, Bill Pugh, Michael Scott, Mark Smotherman, Greg 
Steffan, and Bob Wier took time that they did not'have to read* and advise us 
on early drafts of the book. A very special thanks to A1 Davis (University of 
Utah), Peter Dinda-<Northwestern University), John Greinfer (Rice University), 
Wei Hsu (University of Minnesota), Bruce Lowekamp* (College of William & 
Mary), Bobbie Othmer (University of Minnesota), Michael Scott (University of 
Rochester), and Bob Wier (Rocky Mountain College) for class testing the beta 
version. A special thanks to their students as well! 

We would also like to thank our colleagues at Prentice Hall Marcia Horton, 
Eric Frank, and Harold Stone have been unflagging in their support and vision. 
Harold also helped us present an accurate historical perspective on RISC and 
CISC processor architectures. Jerry Ralya provided sharp insights and taught us 
a lot about good writing. 

Finally, we would like to acknowledge the great technical writers Brian 
Kernighan and the late W. Richard Stevens, for showing us that technical books 
can be beautiful. 

Thank you all. 

Randy Bryant 
Dave O’Hallaron 
Pittsburgh, Pennsylvania 







About the Authors 

» V 

.( 


Randal E. Bryailt received his bachelor’s degree from 
the University of Michigan in 1973 and then attended 
graduate school at the Massachusetts Institute of 
Technology, receiving his PhD degree in computer 
science in 1981. He spent three years as an assistant 
professor at the California Institute of Technology, 
-^nd'has been on the faculty at Carnegie Mellon since 
1984. For five of those years he served as head of the 
Computer Science Department, and for ten of them 
he served as Dean of the SchooPof Computer Science. 
He is currently a university professor of computer sci¬ 
ence. He also holds a courtesy appointment with the Department of Electrical and 
Computer Engineering. 

Professor Bryant has taught courses in computer systems at both the under¬ 
graduate and graduate level for around 40 years. Over many years of teaching 
computer architecture courses, he began shifting the focus from how computers 
are designed to how programmers can write more efficient and reliable programs 
if they understand the system better. Together with Professor O’Hallaron, he de¬ 
veloped the course 15-213, Introduction to Computer Systems, at Carnegie Mellon 
that is the basis for this book. He has also taught courses in algorithms, program¬ 
ming, computer networking, distributed systems, and VLSI design. 

Most of Professor Bryant’s research concerns the design of software tools 
to help software and hardware designers verify the correctness of their systems. 
These include several types of simulators, as well as formal verification tools that 
prove the correctness of a design using mathematical methods. He has published 
over 150 technical papers. His research results are used by major computer manu¬ 
facturers, including Intel, IBM, Fujitsu, and Microsoft. He has won several major 
awards for his research. These include two inventor recognition awards and a 
technical achievement award from the Semiconductor Research Corporation, the 
Kanellakis Theory and Practice Award from the Association for Computer Ma¬ 
chinery (ACM), and the W. R. G. Baker Award, the Emmanuel Piore Award, the 
Phil Kaufman Award, and the A. Richard Newton Award from the Institute of 
Electrical and Electronics Engineers (IEEE). He is a fellow of both the ACM and 
the IEEE and a member of both the US National Academy of Engineering and 
the American Academy of Arts and Sciences. 






xxxvi About the Authors 


David R. O’Hallaron is a professor of computer science 
and electrical and computer engineering at Carnegie 
Mellon University. He received his PhD from the Uni¬ 
versity of Virginia. He served as the director of Intel 
Labs, Pittsburgh, from 2007 to 2010. 

He has taught computer systems courses at the un¬ 
dergraduate and graduate levels for 20 years on such 
topics as computer architecture, introductory com¬ 
puter systems, parallel processor design, and Internet 
services. Together with Professor Bryant, he developed 
the course at Carnegie Mellon that led to this book. In 
2004, he was awarded the Herbert Simon Award, for Teaching Excellence by the 
CMU School of Computer Science, an award for which the winner is chosen based 
on a poll of the students. 

Professor O’Hallaron works in the area of computer systems, with specific in¬ 
terests in software systems for scientific computing, data-intensive computing, and 
virtualization. The best-known example of his work is tlie Quake project, an en¬ 
deavor involving a group of computer scientists, civil engineers, and seismologists 
who have developed the ability to predict the motion of the ground during strong 
earthquakes. In 2003, Professor O’Hallaron and the other members of the Quake 
team won the Gordon Bell Prize, the top international prize in high-performance 
computing. His current work focuses on the notion of autograding, that is, pro¬ 
grams that evaluate the quality of other programs. 












A Tour of Computer Systems 


1.1 Information Is Bits + Context 3 

1.2 Programs Are Translated by Other Programs into Different Forms 4 

1.3 It Pays to Understand How Compilation Systems Work 6 

1..4 Processors Read and Interpret Instructions Stored in Memory 7 

1.5 Caches Matter 11 

1.6 Storage Devices Form a Hierarchy 14 

1.7 The Operating System Manages the Hardware 14 

1.8 Systems Communicate with Other Systems Using Networks 19 

1.9 Important Themes 22 

1.10 Summary 27 
Bibliographic Notes 28 
Solutions to Practice Problems 28 


1 


2 Chapter 1 A Tour of Computer Systems 


A computer system consists of hardware and systems software that work to¬ 
gether to run application programs. Specific implementations of systems 
change over time, but the underlying concepts do not. All computer systems have 
similar hardware and software components that perfornx similar functions. This 
book is written for programmers who want to get better at their craft by under¬ 
standing how these components work and how they affect the correctness and 
performance of their programs. 

You are poised for an exciting journey. If you dedicate yourself to learning the 
concepts in this book, then you wiU be on your way to becoming a rare “power pro¬ 
grammer,” enlightened by an understanding of the underlying computer system 

and its,impact on your application programs. 

‘ 'You are.gbing td learn’practical skilly such as how to avoid strange numerical 
errors caused by the way that computers represent numbers. You will learn how 
to optimize your C code by using clever tricks that exploit the designs of modern 
processors and memory systems. You will learn how the compiler implements 
procedure caUs and how to use this knowledge to avoid the security holes from 
buffer overflow vulnerabilities that plague network and Internet software. You will 
learn how to recognize and avoid the nasty errors during linking that confound 
the average programmer. You will learn how Jo write your own Unix shell, your 
own dynamic storage allocation package, and even your own Web server. You will 
learn the promises and pitfalls of concurrency, a topic of increasing importarice as 
multiple processor cores are integrated onto single,chips. 

In their classic text on the C programming language [61], Kerni^an and 
Ritchie introduce readers to C using the hello program shown in Figure 1.1. 
Although hello is a very simple program, every major part of the system must 
work in concert in order for it to run to completion. In a sense, the goal of this 
book is to help you understand what happens' and why when you run hello on 
your system. 

We begin our study of systems by tracing the lifetime of the hello program, 
from the'time it is created by a programmer, until.it runs on a system, prints its 
simple message, and terminates. As we follow tlje lifetime of the program, we will 
briefly introduce the key concepts, terminology, and components that come into 
play. Later chapters will expand on these ideas. ' 


__ ~ _ code/intro/hello.c 

^ ttinclude <stdio.h> 

2 

3 int mainO 

4 { 

5 printf("hello, worldNn"); 

6 return 0; 

7 > 

______ code/intro/hello.c 

Figure 1.1 The hello program. (Source: [60]) 






Section,1.1 Information-Is Bits + Context 3 


# 

i 

n 

c 

1 

u 

d 

e 

SP 

< 

s 

t 

d 

i 

0 


35 

105 

no 

99 

108 

117 

100 

101 

32 

60 

115 

116 

100 

105 

111 

46 

h 

> 

\n 

\n 

i 

n 

t 

SP 

m 

a 

i 

n 

( 

) 

\n 

i 

104 

62 

10 

10 

105 

-no 

116 

32 

109 

97 

105 

no 

40 

41 

10 

123 

\n 

SP 

SP 

SP 

SP 

p 

r 

i 

n 

t 

f 

<( 

ri 

h 

e 

'1 

10 

32 

32 

32 

32 

112 

A computer system consists of hardware and systems software that work to¬ 
gether to run application programs. Specific implementations of systems 
change over time, but the underlying concepts do not. All computer systems have 
similar hardware and software components that perfornx similar functions. This 
book is written for programmers who want to get better at their craft by under¬ 
standing how these components work and how they affect the correctness and 
performance of their programs. 

You are poised for an exciting journey. If you dedicate yourself to learning the 
concepts in this book, then you wiU be on your way to becoming a rare “power pro¬ 
grammer,” enlightened by an understanding of the underlying computer system 

and its,impact on your application programs. 

‘ 'You are.gbing td learn’practical skilly such as how to avoid strange numerical 
errors caused by the way that computers represent numbers. You will learn how 
to optimize your C code by using clever tricks that exploit the designs of modern 
processors and memory systems. You will learn how the compiler implements 
procedure caUs and how to use this knowledge to avoid the security holes from 
buffer overflow vulnerabilities that plague network and Internet software. You will 
learn how to recognize and avoid the nasty errors during linking that confound 
the average programmer. You will learn how Jo write your own Unix shell, your 
own dynamic storage allocation package, and even your own Web server. You will 
learn the promises and pitfalls of concurrency, a topic of increasing importarice as 
multiple processor cores are integrated onto single,chips. 

In their classic text on the C programming language [61], Kerni^an and 
Ritchie introduce readers to C using the hello program shown in Figure 1.1. 
Although hello is a very simple program, every major part of the system must 
work in concert in order for it to run to completion. In a sense, the goal of this 
book is to help you understand what happens' and why when you run hello on 
your system. 

We begin our study of systems by tracing the lifetime of the hello program, 
from the'time it is created by a programmer, until.it runs on a system, prints its 
simple message, and terminates. As we follow tlje lifetime of the program, we will 
briefly introduce the key concepts, terminology, and components that come into 
play. Later chapters will expand on these ideas. ' 


__ ~ _ code/intro/hello.c 

^ ttinclude <stdio.h> 

2 

3 int mainO 

4 { 

5 printf("hello, worldNn"); 

6 return 0; 

7 > 

______ code/intro/hello.c 

Figure 1.1 The hello program. (Source: [60]) 






Section,1.1 Information-Is Bits + Context 3 


# 

i 

n 

c 

1 

u 

d 

e 

SP 

< 

s 

t 

d 

i 

0 


35 

105 

no 

99 

108 

117 

100 

101 

32 

60 

115 

116 

100 

105 

111 

46 

h 

> 

\n 

\n 

i 

n 

t 

SP 

m 

a 

i 

n 

( 

) 

\n 

i 

104 

62 

10 

10 

105 

-no 

116 

32 

109 

97 

105 

no 

40 

41 

10 

123 

\n 

SP 

SP 

SP 

SP 

p 

r 

i 

n 

t 

f 

<( 

ri 

h 

e 

'1 

10 

32 

32 

32 

32 

112 

114 

105 

no 

116 

102 

40 

34, 

104 

101 

108 

1 

0 

t 

SP 

w 

0 

r 

1 

d 

< 

\ 

n 

ri 

) 

i 

\n 

SP 

108 

111 

44 

32 

119 

111 

114 

108 

100 

92 

no 

34 

41 

59 

10 

32 

SP 

SP 

SP 

r 

e 

t 

u 

i 

n. 

SP 

0 

t 

\n 

■> 

\n 


32 

32 

32 

114 

101 

116 

117 

114 

no 

32 

48 

59 

10 

125 

10 



Figure 1.2 The ASCII text representation of hello. c. 


1.1 Information Is Bits + Context 

Our hello program begins life as a source program (or source filey^ai the 
programmer creates with an editor and saves in a text file called hello. c. The 
source program is a sequence of bits; each with a value of 0 or 1, organized in Srbit 
chunks called bytes. Each byte represents some text character in the program. 

Most computer systems represent text characters using the ASCII standard 
that represents each character with a unique byte-size integer value.^ For example, 
Figure 1.2 shows the ASCII representation of the hello. c program. 

The hello. c program is stored in a file as a sequence of bytes. Each byte has 
an integer value that corresponds to some character. For example,-the first byte 
has the integer value 35, which corresponds to the character The second byte 
has the integer value 105, which corresponds to the character ‘i’, and so on. Notice 
that each text line is terminated by the invisible newline character ‘\n’, which is 
represented by the integer value 10. Files such as hello. c that consist exclusively 
of ASCII character^ are known as text files. All bthef files are known as binary 
files. 

The representation of hello. c illustrates a fundamental idea; All information 
in a system—including disk files, programs stored in memory, user data stored in 
memory, and data transferred across'a network—is feiJresented as a bunch of bits. 
The only thing that distinguishes different data objects is the context; in which 
we view them. For example, in different contexts, the same sequence of bytes 
might represent an integer, floating-point number, character string, or machine 
instruction. ' 

As programmers, we need to understand machine representations of numbers 
because they are not the same as integers and real ilumbers. They are finite 


1. Other encoding methods are used to represent text in non-English languages. See the aside on page 
50 for a discussion on this. 






4 Chapter 1 A Tour of Computer Systems 


1, • !•.:» I 

Aside Origins of the Gprogramming.language ^ ^ 

C was developed from 1969,,fo 1973 b^' Dennis Ritchie of ^ell Laboratories. The‘Aineripan National | 
Standards Institute (ANSI)*r^tifi^^'Ae ANSI 0 standhrdinj.989', ahd this staridardi^htiort later became | 
the responsibility of the International Standafds Orga’ijfzation (ISO). The standards define the C | 
language and a set of library functions known'as the'p staniafd library. Kernighan and Ritbirie describe | 
ANSI.C ln theiTciassic book, which is known affectionately as ‘tK&R” [61]. Ifr Ritchie’s words592]„G, | 
is “quirky, flawed, and an«enormous success.’’ So why the success? | 

■i S. it- ■' % t % 


• O was- closely tiM with "the, Unix operating system. C VaS'Be^elopbd'frdm the^ieginning as th& | 
j system prggramming.language fo/ Unix.- Most o( the Unix, kernel (the core part of titje operating^ | 

system), and all of its supporting tools and libraries, wereiWrittpn irtG" As Unix became popular in I 
I ’^universities in the late i970s and early 1980s, many people w^re exposed to C and found that theysl 
liked it. Since Unix was written almost entirely in^, it could be easily ported to new mgchine^* I 
which created an even wider aqdience fprbotliXI! and-Unix*. s • I 

• Cisa small, simple language. The dpign was controIlSl by'h single|)drs6n,’father than a committee, I 
* and the result was a dead, consistent design with littlejbdggage. ThfejjK&R book^ describes the | 

complete language and st&dardlibraiy, with nufherous examples and exercises, in only 261 pages. * 
Tbe simplicityxif C made it relativeli^ easy tp learn and tbport tb different cbjfiputefsj ' 

• C was designedJpr a practical piirpose C was designed to implement the^Unix operating systefli. 

Ldter, other people'found that they could wfife tjie prograrhs they w^ahtedjfwithoutlhe language-si 
getting jn thp wayf ^ ¥ « I « 

C is the language^ of.choice Jot Sysfem^evel’prograitiming, add there is 4’huge installed ba^e of 'I 
application-level programs as well. However, it is not^perfect for.^11 prograthmers%nd all situations. ^ | 
C pointers are a com'moh sodrce qCconfusipp and programming errors. Gjalso lack? explicit support | 
, 'for useful abstractidns sudi'as classesl objects, and pxceptionSiNlevyer languages such ap,G-fc+ and Java 1 


address these issues for application-lev&lprogramsj p 




. ^ •f 




approximations that can behave in unexpected ways. This fundamental idea is 
explored in detail in Chapter 2. 


1.2 Programs Are Translated by Other Programs 
into Different Forms 

The hello program begins life as a high-level C program because it can be read 
and understood by human beings in that form. However, in order to run hello. c 
on the system, the individual C statements must be translated by other programs 
into a sequence of low-level machine-language instructions. These instructions are 
then packaged in a form called an executable object program and stored as a binary 
disk file. Object programs are also referred to as executable object files. 

On a Unix system, the translation from source file to object file is performed 
by a compiler driver: 











Section 1.2 Programs Are Translated by Other Programs into Different Forms 5 


hello.c 


Source 

program 

(text) 


printf .0 



program 

(text) 


(text) 


programs 

(binary) 


program 

(binary) 


Figure 1.3 The compilation system. 


linux> gcc -o hello hello.c 

Here, the gcc compiler driver reads the source file hello. c and translates it into 
an executable object file hello. The translation is performed in the sequence 
of four phases shown in Figure 1.3. The programs that perform the four phases 
(preprocessor, compiler, assembler, and linker) are known collectively as the 
compilation system. 

• Preprocessing phase. The preprocessor (cpp) modifies the original C program 
according to directives that begin with the *#’ character. For example, the 
#include <stdio.h> command in line 1 of hello, c tells the preprocessor 
to read the contents of the system header file stdio .h and insert it directly 
into the program text. The result is another C program, typically with the . i 
suffix. 

• Compilation phase. The compiler (ccl) translates the text file hello, i into 
the text file hello. s, which contains an assembly-language program. This 
program includes the following definition of function main: 


1 

main: 


2 

subq 

$8, %rsp 

3 

movl 

$.LC0, Xedi 

4 

call 

puts 

5 

movl 

$0, %eax 

6 

addq 

$8, 7,rsp 

7 

ret 



Each of lines 2-7 in, this definition describes one low-level ipachine- 
, language instruction in a textual form. Assembly language is useful because 
it provides a common output language for different compilers for different 
high-level languages. For example, C compilers and Fortran compilers both 
generate output files in the same assembly language. 

• Assembly phase. Next^.the assembler (as) translates hello. s into machine- 
language instructions, packages them in a form known as a relocatable object 
program, and stores the result in the object file hello. o. This file is a binary 
file containing 17 bytes to encode the instructions for function main. If we 
were to view hello. o with a text editor, it would appear to be gibberish. 




6 Chapter 1 A Tour of Computer Systems 


r* 


Aside The GNU project i 

Gcc is one of many useful topis developed by the GNU (shortTor GNU’s Not Unix) project. The | 
GNU project is a tax-exempt charity started by Richard Stallman in 1984, with the ambitious goal of | 
developing complete Unix-like system whose source code is unencumbered by restrictions on how | 
it can be modified or distributed. The GNU project has developed an environment with all the major | 
components of a Unix operating system, except for the kernel, which was developed separately by s 
the Linux project. The GNU environment includes the emacs editor, pcc compiler, gdb debugger, j 
assembler, linker, utilities for manipulating binaries, and other components. The gcc compiler has | 
grown to support many different languages, with the ability to generate code for many different | 
machines. Supported languages include C. C++, Fortran, Java, Pascal, Objective-C, and Ada. 

The GNU project is a remarkable achievement, and yet it is often overlooked. The modem open- 
source movement (commonly associated with Linux) owes its intellectual origins to the GNU project’s 
notion of free software (“free” as in “free speech,” not “free beer”). I^urther, Linux owes much of its 
popularity to the GNU tools, which provide the environment for the Linux kernel. 




• Linking phase. Notice that our hello program calls the printf function, which 
is part of the standard C library provided by every C compiler. The printf 
function resides in a separate precompiled object file called printf. o, which 
must somehow be merged with our hello. o program. The linker (Id) handles 
this merging. The result is the hello file, which is an executable object file (or 
simply executable) that is ready to be loaded into memory and executed by 
the system. 


1.3 It Pays to Understand How Compilation Systems Work 

For simple programs such as hello. c, we can rely on .the compilation system to 
produce correct and efficient machine code. However, there are some important 
reasons why programmers need to understand how compilation systems work: 

• Optimizing program performance. Modern compilers are sophisticated tools 
that usually produce good code. As programmers, we do not need to know 
the inner workings of the compiler in order to write efficient code. However, 
in order to make good coding decisions in our C programs, we do need a 
basic understanding of machine-level code and how the compiler translates 
different C statements into machine code. For example, is a switch statement 
always more efficient than a sequence of if-else statements? How much 
overhead is incurred by a function call? Is a while loop more efficient than 
a for loop? Are pointer references more efficient than array indexes? Why 
does our loop run so much faster if .we sum into a local variable instead of an 
argument that is passed by reference? Howcan a function run faster when we 
simply rearrange the parentheses in an arithmetic expression? 








Section 1.4 Processors Read and Interpret Instructions Stored in Memory 7 


In Chapter 3, we introduce x86-64, the machine language of recent gen¬ 
erations of Linux, Macintosh, and Windows computers. We describe how 
compilers translate different C constructs into this language. In Chapter 5, 
you will learn how to tune the perforiqance of yqur C progr^s by making 
simple transforniatjogs tqjthe C code that help the-compiler do its job better. 
In Chapter 6, you will learn about the hierarchical nature of the memory sys¬ 
tem, hoV C compilers store data arrays in memory, and how your C programs 
can explpit'this knowledge run more efficiently. 

• Understanding link-time errors. In our experience;-some of the most perplex¬ 
ing programming errors are related to the operation of the linker, especially 
when you are trying to build large software systems. For example, what does 
it mean wheii the linker reports that it cannot resolve a reference? What is the 
difference between a static variable and a-global variable? What happens if 
you define two globajvvariables in different C files with the same name? What 
is the difierence between a static library and a dynamic library? Why does it 
matter wljat order we list libraries on the command line? And scariest of all, 
why do some>linker-related errors not appear until run time? You will learn 
the answers to these kinds of questions in Chapter 7. 

• Avoiding security holes. For many years, buffer overflow vulnerabilities have 
accounted for many of the security holes in network and Internet servers. 

, 'fhe^e Amlnerabilities ^xist because top few programmers understand the need 
to carefully restrict quantity and forms pi data they accqptffom untrusted 

sources.,A step ^ learning secqre programipiqg is to understand the con¬ 

sequences of the way dat^ and,control information are stored on the program 
stack. We cover the stack discipline and buffer overflow vulnerabilities in 
Chapter 3 as part of our study of assembly language. We will also learn about 
methods that can be used by the programmer, compiler, and operating system 
to reduce the threat of attack. 

sX 

' If 

1.4 Processors. Read and Interpret Instructions 
Stored'in Memory 

> , 

At this point, our hello. c source program has been translated by the compilation 
system into an executable object file called hello that is stored on disk. To run 
the executable file on a Unix system, we type its name to an application program 
known as a shell: 

linux> ./hello* 
hello, world ” 

linux> 

I ) 

The shell* is a command-.line interpreter that prints a prompt, waits for you 
to type a command linej-and then performs the command. If the first word of the 
command line does not correspond to a built-in shell command, then the shell 







8 Chapter 1 A Tour of Computer Systems 


Figure 1.4 

Hardware organization 
of a typical system. CPU; 
central processing unit, 
ALU; arithmetic/logic unit, 
PC; program counter, USB; 
Universal Serial Bus. 


CPU > 



assumes that it is the name of an executable file tha!t it should lohd'ancl run. So 
in this case, the’'shell loads and runs the hello program and then waits for it to 
terminate. The hello program pf ints its message' to the screen and then terminates, 
the shell then prints a prompt arid waiti for the next input command line. 


1.4.1 Hardware Organization of a System 

To understand what happens to our hello program when we run it, we peed 
to understand the hardware organization of a typical system, which is shown 
in Figure 1.4. This particular picture'is modeled after the'family of recent Intel 
systems, but all systems have a similar look and feel. Don’t'worry about the 
complexity of this figure just now. We will get to its various details in stages 
throughout the course of the book. 


Buses 

Running throughout the system is a collection of electrical cqndpits called buses 
that carry bytes of information back and forth between the components. Buses 
are typically designed to transfer fixed-size chunks of bytes known as word^. The 
number of bytes in a word (the word size) is aiundamental system parameter that 
varies across systems. Most machines tod^y have word-sizes of either 4 bytds (32 
bits) or 8 bytes (64 bits). In this book, we do not assume any fixed definition of 
word size. Instead, we will specify what we mean by a “word” in any context that 
requires this to be defined. 




















Section 1.4 Processors Read and Interpret Instructions Stored in Mernory 9 

I/O Devices 

Input/output (I/O) devices are the system’s connection to the external world. Our 
example system has four I/O devices: a keyboard and mouse for user input, a 
display for user output, and a disk drive (or simply disk) for long-term storage of 
data and programs. Initially, the executable hello program resides on the disk. 

Each I/O device is connected to the I/O bus by either a controller or an adapter. 

The ciikinction between the two is mainly one of packaging. Controllers are chip 
sets in the device itself or on the system’s main printed circuit board (often called 
the motherboar^. An adapter is a card that plugs into a slot on the motherboard. 

Regardless, the purfjose of each is to transfer information back and forth between 
the I/O bus and an I/O device. 

Chapter 6 has more to say about how I/O devices such as disks work. In 
Chapter 10, you will learn how to use the Unix I/O interface to access devices from 
your application programs. We focus on the especially interesting class of devices 
known as networks, but the techniques generalize to other kinds of devices as well. 

Main Memory 

The main memory is a temporary storage device that holds both a program and 
the data it manipulates while the processor is executing the program. Physically, 
main memory consists of a collection of dynamic random access memory (DRAM) 
chips. Logically, memory is organized as a linear array of bytes, each with its own 
unique address (array index) starting at zero. In general, each of the machine 
instructions that constitute a program can consist of a variable number of bytes. 

The sizes of data items that correspond to C program variables vary according 
to type. For example, on an x86-64 machine running Linux, data of type short 
require 2 bytes, types int and float 4 bytes, and types long and double 8 bytes. 

Chapter 6 has more to say about how memory technologies such as DRAM 
chips work, and how they are combined to form main memory. 

Processor 

The central processing unit (CPU), or simply processor, is the engine that inter¬ 
prets (or executes) instructions stored in main memory. At its core is a word-size 
storage device (or register) called the program counter (PC). At any point in time, 
the PC points at (contains the address of) some machine-language instruction in 
main memory.^ 

From the time that power is applied to the system until the time that the 
power is shut of^ a processor-repeatedly executes the instruction pointed at by the 
program counter and updates the program counter to point to the next instruction. 

A processor appears to operate according to a very simple instruction execution 
model, defined by its instruction set architecture. In this model; instructions execute 


2. PC is also a commonly used acronym for “personal coropnter." However, the distinction between 
the two should be clear from the context. 


10 Chapter 1 A Tour of Computer Systems 


in strict sequence, and executing a single instruction involves performing a series 
of steps. The processor reads the instruction from memory pointed at by the 
program counter (PC), interprets tlie bits in the instruction, performs some simple 
operation dictated by the instruction, and then updates the PC to point to the,next 
instruction, which may or may not be contiguous in memory to the instruction that 
was just executed. 

There are only a few of these simple operations, and they revolve around 
main memory, the register file, and the arithmetic/logic unit (ALU). The register 
file is a small storage device that consists of a collection of wbrd-si5'e registers, each 
with its own unique name, 'the ALU computes new data and address values. TleTe 
are some examples of the simple operation's that the CPU plight carry out at th'e 
request of an instruction: 

• Load: Copy a byte or a word from main memory into a register, overwriting 
the previous contents o^ the regi^e'r. 

• Store: Copy a b^e or a word from a register to alocation in main memory, 
overwriting the previous contents of that location. 

• Operate: Copy the contents of two registers to the ALU, perform an arithmetic 
operation on the two words, and store the result in a register, overwriting the 
previous contents of that register. 

• Jiirrip: Extract a word from the'instruction itself and copy that word into'the 
program counter (PC), overwriting the previous value of the PC. 

We say that a processor appears fto be a 5 imple.,implementation of jifs inx 
struction set architecture,'-but in fact modern processors uspifar more complex 
mechanisms to speed up program, execution. Thus, <we can distinguish the pro¬ 
cessor’s instruction,§et.architecture, .describing the effect of each machine-code 
instruction, from i^s microarchitecture, describing how the processor is actually 
implemented. When we study machine code-in Chapter 3, we ill, consider the 
abstraction provided by the machine’s instruction set architecture. Chapfer 4\ias 
more to say about how processors are actually implemented. Chapter 5 describes 
a model of how modern processors work that enables predicting and optimizing 
the-{ierformancb of machine-languagfe prograhik. 

1.4.2 Rurihmg t^ 1 e hfellb* Program 

Given this simple view of a system’s hardware organization and opteration, we can 
begimto understand what happens when we run our example program. We must 
omit a lot of details herejthat will be filled in later, Jbut for now we will be content 
with the big picturb. ■ . 

Initially, the shell programis executingits instructions, waiting for us to type a 
command. As we type the characters . /hell ol at the keyboard, the shell program 
reads each one into a register and then stores it in memory, as shown in Figure 1.5. 

When we hit the enter key on the keyboard, the shell knows that we have 
finished typing the command. The sh.ell th^n loads the .executable hello file by 
executing a sequence of instructions that copies the code and data in the hello 
















Section 7.5 Caches Matter 


Figure 1.5 

Reading the hello 
command from the 
keyboard. 


CPU 



object file from disk to main memory. The data includes the string of characters 
hello, world\n that will eventually be printed out. 

Using a technique known as direct memory access (DMA, discussed in Chap¬ 
ter 6), the data travel directly from disk to main memory, without passing through 
the processor. This step is shown in Figure 1.6. 

Once the code and data in the hello object file.are loaded.into memory, 
the processor begins executing the machine-language instructions in the hello 
program’s main routine. These instructions copy the bytes in the hello, world\n 
string from memory to the register file, and from there tp the display device, where 
they are displayed on the screen. This step is shown in Figure 1.7. 


l.S Caches Matter 

An important lesson froih this Simple example is that a system spends a lot of 
time moving information from chie-place to another. The machine instructions in 
the hello program are originally stored on disk. When the program is loaded, 
they are copied to main jnemory. As the processor runs the program, instruc¬ 
tions are copied from main memory into the processor. Similarly, the data string 
hello,world\n, originally on disk, is copied to main memory and then copied 
from main memory to the display device. From a programmer’s perspective, much 
of this copying is overhead that slows down the “real work” of the program! Thus, 

a major goal for system designers is to make these copy operations run as fast as 
possible. 

Because of physical laws, larger storage devices'are slower than smaller stor¬ 
age devices. And faster devices are more expensive to build than their slower 








12 Chapter 1 A Tour of Computer Systems 


CPU 


Register file 







ALU 


Bus interface 


System bus Memory bus 


, ^ , V 





btidg^lV” 

——’ftfemocy 


“hello, worlcl\n” 
hello code 


I/O bus 


USB 


Graphics 


ska’ll 

controller 


adapter 


Site!;;" 


1 r 

Mouse Keyboard Display 




11 Expansion slots for 

. other devices such 

as network adapters 


DIsR 


hello executable 
stored on disk 


Figure 1.6 Loading the executable from disk into main memory. 


CPU 


Register file 


TTEI t >1 


ALU 


. . '^8* -ai 

Bussinterface ■ 




Systerp^bLjs Memory bus, 

1 


'*'v ‘' I ^ ^ Kbpdg^l 


4^ 


iflemoi 


"hello, world\n” 
hello cods. 


I/O bus 


USB 


‘I-WphTc^ 


Di^ 

controller 



* i 

1 'r^A. 

controller 
. i > 1 * 


■Ise Keyboard DispI 


Mouse Keyboard Display 

•ihello, world\n" 


Expansion slots for. « 
other devices si5ch 
as,network adapters. 


Disk 


hello executable 
stored on disk 


Figure 1.7 Writiog the output string from memory to thcKlfsplay. 

I 


i 















Section 1.5 Caches Matter 13 


Figure 1.8 
Cache memories. 


CPU chip 



counterparts. For example, the disk drive on a typical system might be 1,000 times 
larger than the main memory, but it might take the processor 10,000,000 times 
longer to read a word from disk than from memory. 

Similarly, a typical register file stores only a few hundred bytes of information, 
as opposed to billions of bytes in the main memory. However, the processor can 
read data from the register file almost 100 times faster than from memory. Even 
more troublesome, as semiconductor technology progresses over the years, this 
processor-u^emory gap continues to increase. It. is easier and cheaper to make 
processors run faster than it is to make main memory run faster. 

To deal with the processor-memory gap, system designers include smaller, 
faster storage devices called cache memories (or simply caches) that serve as 
temporary staging areas for information that the processor is likely to need in 
the near future. Figure 1.8 shows the cache memories in a typical system. An LI 
cache on the processor chip holds tens of thousands of bytes and can be accessed 
nearly as fast as the register file. A larger L2 cache with hundreds of thousands 
to millions of bytes is connected to the processor by a special bus. It might take 5 
times longer for the processor to access the L2 cache than the LI cache, but this is 
still 5 to 10 times faster than accessing the main memory. The LI and L2 caches are 
implemented with a hardware |echnology known as static random access memory 
(SRAM). Newer and more powerful systems even have three levels of cache: LI, 
L2, and L3. The idea behind caching is that a system can get the effect of both 
a very large memory and a very fast one by exploiting locality, the tendency for 
programs to access data and code in localized regions. By setting up caches to hold 
data that are likely to be accessed often, we can perform most memory operations 
using the fast caches. 

One of the most important lessons in this book is that application program¬ 
mers who are aware of cache memories can exploit them to improve the perfor¬ 
mance of their programs by an order of magnitude. You will learn more about 
these important devices and how to exploit them in Chapter 6. 





14 Chapter 1 A Tour of Computer Systems 


Smaller, 

faster, 

and 

costlier 
(per byte) 
storage 
devices 

( 

Larger, 

slower, 

and 

cheaper 
(per byte) 
storage 
devices 


Figure 1.9 An example of,a memory hiera/^hy. 


CPU registers hold words 
retrieved from cache memory, 


L1 cache holds cache lines 
retrieved from L2 cache. 



L2 cache holds cache lines 
retrieved from L3 cache. 


L3 cache holds cache lines 
retrieved from memory. 


Main memory holds disk blocks^ 
retrieved from local disks. 


Local disks hold files 
retrieved from disks on 
remote network server. 


IK. 
ji ( 1 


i'vi 


1,.6 Storage Devices,F;orm a.Hierarchy 

This notion of inserting a sifililler, faster *stdrage device (e.g.,'ca'che irieniory) 
between the processor aijd a larger, slower device (e.g., main membry)^tirns out 
to’be a general idea. In^iacf, the storage devices'in every computer system are 
organized as^a memory hierarchy similar to'I^igure 1.9. As we move from the top 
pf the hierarchy to the bottom, the device's'become slower, larger' and less costly 
per byte. The register file occupies the top level in the hierarchy, which is k&wn 
as level 0 or LO. We show three levels of caching LI to L3, occupying memory 
hierarchy levels 1 to S.'Maih memory occupies level 4, and ^o oh. 

The main idea of a memory hierarchy is that storage at one level serves as a 
cache for storage at the next lotver level. Thus, the register file is a cache for the 
LI cache. Caches LI and L2 are caches for L2 and L3, respectively. 'Tbe L3 cache 
is ‘a cache for the main memory, which is a cache for the disk. Oh some networked 
systeiris with distributed file systems, the locfal’disk serves asUTcacfie for data stored 
on the di^s of other systems. 

Just a^ programniers can exploit khowledge of the different cachfes to iiriprove 
peffdrmance, programmers caifbxploit their understanding of the entire memory 
hierarchy. Chapter 6 w^ill Jiave much more to say about this. ^ 

1.7 The Operating System Manages the Hardware 

Back to our hello example} 'When the shell Idaded and ran the hello program, 
and when the hello program printed its message, neither program accessed the 















Section 1.7 The Operating System Manages the Hardware 15 


Figure 1.10 
Layered view of a 
computer system. 

f 


Figure 1.11 

Abstractions provided by 
an operating system. 


Application programs 


' 1—■** 

operating system 

Processor 

Main rriemory 

I/O devices 

Processes 

f 

Virtual memory [ 

1 

Files [ 

--^-i 

Processor 

Main memory 

i/0 devices 


j Software 
y Hardware 


keyboard, display, disk, or main memory directly. Rather, they relied on the 
services provided by the operating system. We can think of the operating system as 
a layer of software interposed between the application programjand the hardware, 
as shown ii^ Figure 1.10. All attempts by an application program to manipulate the ‘ 
hardware must go through the operating system. 

The operating system has two primary purposes: (1) to protect the hardware 
from misuse by runaway applications and. (2) to provide applications with simple 
and uniform mechanisms for manipjalating complicated and often wildly different 
low-level hardware devices. The .operating system achieves both goals via the 
fundamental abstractions shown in Figure 1.11: processes, virtual memory, and 
files. As this figure suggests, files are abstractions for 1/6 devices, virtual memory 
is an abstraction for both the main memory and disk I/O devices, and processes 
are abstractions for the processor, main memory, and I/O devices. We will discuss 
each in turn. < 

1.7.1 Processes 

T 

When a program such aa hello nms.on a modern system„the operating system 
provides the illusiomthat the program is the only.one running on the system. The 
program appears to have exclusive use of both the processor, main memory, and 
FQdevices.>The'processor appears to execute the instructions in the program, one 
after the other, without interruption. And the code and data of the program appear 
to be the only objects in the systemls memory. These illusions are provided by the 
notion of a process,- one of the most ifnportant and successful ideas in computer 
science. 

A process is the operating systemls'abstraction for a running.program. Multi¬ 
ple processes can run concurrently on the same system, and each process appears 
to have exclusive use of the hardware. By concurrently, we mean-that the instruc¬ 
tions of one process arp interle^vedjwith the instructions of another process. In 
most systems, there are more processes to run than there are CPUs to run them. 




16 Chapter 1 A Tour of Computer Systems 


Aside Unix, .Ppsixl and the Standard Unix Specification ’ , ^ « 

ITie 1960s was anjsra-of huge, compipx'operating systems, such" as IBM’s OS/360 and Hbneywell’^ * 
Multics systems. While OS/360 was ohe'df thLmost successful >software projects in history, Multics 
dragged on for^years and never achieved wide-scale use. B^l Laboratories was an original partner in 
the Multics project but dropped out in 1969 because,o? concern Over th& complexity of the project 
and the lack of progrQss.,ln reaction to their unpleasant Multics experience, a group of Bell Labs 5 
researchers—Ken Thompson, Dennis Ritchie, Doug Mclhoy, and Joe Ossannair^egan work in 1969 
on a simpler operating.system for a Digital Equipment Corporation PDP-7 computer,Jsyritteif‘entirely ' 
in machine language. Many of the ideas in the ne'^ system, such as the hierarchical file system and the 
notion of a shell as a user-leveLprocess, were borrowed Jrom Iv|ultics "but implemented in a smaller, 
simpler package. Iri 1970; Bri^ Kemi^han dubbe^d the new'systein ‘lUriix” as a pun on the coiiiplexity 
of “Multics.” The.kernel was rewritten in C in 1973, and Unix was annouiiced to the outside world in 
1974(93]. 

Because Bell Labs made the-source code available to sclidols with generous term^ Unix developed 
a large following at ImWersities. The,most influential tyor^ was done'at the University ot California 
at Berkeley in the lafe 1970s and early‘i986s, with* Berke|ey‘researb1lfSrs adding■v’lrtuahmembry and ^ 
the Internet protocols in.a^serfe^'of releases;called Un& 4/xBSD (Berkeley So'ftware'Distribution). 
Concurrently, Bell.Eabs^wa^ T^easing”tlieif own versions, whilji became^known as System V Unix. 
Versions from other vendors, such as'the’SuhkMicrosysfemrSolaris'^stenf' were derived from' theW"* 
original BSD and System V,versioris 

and 

ics Engineers) spohsorellJan effort’tfj standafdize’Umx, late'f dubHed -‘PosiiP’ by Richard Stallman. 
The result was a family of standards, known* a^ ffie Posix stantilifds, that cbvefsucfrissues^as thfeC * 
language interface*for Unix system calls, shelj,|>fograms"and utilities, threads, ahdhetwo]:id»|ird|rami! 
roing. More recently, .a separate standa/dizatipn effort,'kfibwn as the “Stand|f(l,ynix Specification,” ‘ 
has joined forces with Posix to'create a single, unified stah'dard fofUmx systems.. As a’result’of these 
standardization ef&rts, the difference! between UmX '^ersioiis hhve largely ch^appeared. 



Traditional systems could only execute one program at a time, while newer multi¬ 
core processors can execute several programs simultaneously. In either case, a 
single CPU can appear to execute multiple processes concurrently by having the 
processor switch among them. The operating system performs this interleaving 
with a mechanism known as context switching. To simplify the rest of this discus¬ 
sion, we consider only a uniprocessor system containing a single CPU. We will 
return to the discussion of multiprocessor systems in Section 1:9.2. 

The operating system keeps track of all the state information that the process 
needs in order to run. This state, which is knovwi as the context, includes informa¬ 
tion such as the current values of the PQ the register file, and the contents of main 
memory. At any point in time, a uniprocessor system can only execute the code 
for a single process. When the operating system decides to transfer control from 
the current process to some new process, it performs a context switch by saving 
the context of the current process, restoring the context of the new proeess, and 










Section 1 .7 The Operating System Manages the Hardware 17 


Figure 1.12 Process A 

Process context _ _ 

Process B 

switching. X 

read---*- 

User code 

... 


Kernel code 

1 

Disk Interrupt - _1 


' * Return ^ 4 --^ 

Kernel code 

from read 1 

T ^ ^ f 

-__ 1 

1 

User code 


Context 

switch 


Context 

switch 


then passing control to the new process. The new process picks up exactly where 
it left off. Figure 1.12 shows the basic idea for our example hello scenario. 

There are two concurrent processes in our example scenario: the shell process 
and the hello process. Initially, 1,he shell process is running alone, waiting for input 
on the command line. When we.ask it to run the hello program, the shell carries 
out our request by invoking a special function known as a system call that passes 
control to the operating system. The operating system saves the shell’s context, 
creates a new hello process and its'context, and then passes control to the new 
hello process. After hello terminates, the operating system restores the context 
of the shell process and passes control back to it, where it waits for the next 
command-line mput. 

As Figure 1.12 indicates, the transition from one process to another is man¬ 
aged by the operating system kernel. The kernel is the portion of the operating 
system code that is always resident in memory. When an application program 
requires some action by the operating system,, such as to read or write a file, it 
executes a special system call instruction, transferring control to the kernel. The 
kernel then performs the requested operation and returns back to the application 
prp^r,am. Note that the kernel is not.a separate process. Instead, it is a collection 
of code and data structures that the system uses to manage all the processes. 

Implementing the,process abstraction requires close pooperation between 
both thejo^-level hardware and the operating system software. We will explore 
how this works, and how applications can create and control their own processes, 
in Chapter 8. 


1.7.2 Threads 

Although we normally think of a process as having a single control flow, in modem 
systems a proce'ss can actually consist'of multiple exedhtion units, called threads, 
each runninj in the context of th^ process apd sharing the same code and global 
data. Threads are an increasingly important programming model because of the 
requirement for concurrency in network servers, becatisedt is easier to share data 
between multiple threads than between multiple processes, and because threads 
aj-e typically more efficient than processes. Multi-threading is also one way to make 
programs run faster-when multiple processors are available, as we will discuss in 


k 


18 Chapter 1 A Tour of Computer-Systems 


Figure 1.13 

Process virtual address 
space. (The regions are not 
drawn to scale.) 


Program 

start 


Kernel virtual memory 


User stack 
(created at run time) 



Memory-mappe^ region for 
shared libraries 



Run-time heap 
‘(created by malice) 

> I . .. 


'■ Read/write data 


■ Read-only code and data’ 



Memory 
t invisible to 
' user code 


,priiitf function 


I Loaded from (he 
[ hello executable file 




J. 




Section 1.9.2. You will learn the basic concepts of concurrency, including how to 
write threaded programs, in Chapter 12. 

1.7.3 Virtual Memory 

Virtual memory is an abstraction that provides each process with the illusion that it 
has exclusive use of the main memory. Each process has the same uniforih vjiew of 
memory, Which is known as its virtual address space. The virtual address space for 
Linux processes is shown in Figure 1.13. (Cither Unix systems use a similar layou0 
In Linlix, the topmost region of the address space is reserved for code and^data 
in the operating system that is common to all processes. The lower region of the 
address space holds the code and data defined by'the user’s procesk Note that 
addresses in the figure increase from the bottom tottie^tbp. ^ 

The virtual address space seen by each process consists of a number of well- 
defined areas, each with a specific purpose; You will learn more about these areas 
later in the book, but it will be helpful to look briefly at each, starting with the 
lowest addresses and working our way up; 

J * 

• Program code and data. Code begins at the §ame.fixed address for all processes, 
followed by data locations that correspond to global C variables. The code and 
data tg'eas are initialized directly.from the contents of an executable object 
file—in.our case, th&heilo executable. You wjy learn more-about this part of 
the address space when we study linking and loading in Chapter 7- 

• Heap. The code and data areas are followed immediately'by therun-time heap. 
Unlike' the code and data areas, which'are fixed in size once the process begins 

















Section 1.8 Systems Communicate with Other Systems Using Networks 19 


running, the heap expands and contracts dynamically at run time as a result 
of calls to C standard library routines such as malloc and free. We will study 
heaps in detail when we learn about managing virtual memory in Chapter 9. 

• Shared libraries. Near the middle of the address space is an area that holds the 
code and data for shared libraries such as the C standard library and the math 
hbrary. The notion of a shared library is a powerful but somewhat difficult 
concept. You will learn how they work when we study dynamic linking in 
Chapter 7. 

• Stack. At the top of the user’s virtual address space is the user stack that 
the compiler uses to implement function calls. Like the heap, the user stack 
expands and contracts dynamically during the execution of the program. In 
particular, each time we call a function, the stack grows. Each t im e we return 
from a function, it contracts. You will learn how the compiler uses the stack ' 
in Chapter 3. 

• Kernel virtual memory. The top region of the address space is reserved for the 
kernel. Application programs are not allowed to read or write the contents of 
this area or to directly call functions defined in the kernel code. Instead, they 
must invoke the kernel to perform these operations. 

For virtual memory to work, a sophisticated interaction is required between 
the hardware and the operating system software, including a hardware translation 
of every address generated by the processor. The basic idea is to store the contents 
of a process’s virtual memory on disk and then use the main memory as a cache 
for the disk. Chaptpr 9 explains how this works and why it is ,so important to the 
operation of modern systems. 

1.7.4 Files 

A file is a sequence of bytes, nothing more and nothing less. Every I/O device, 
including disks, keyboards, displays, and even networks, is modeled as a file. All 
input and output in the system is performed by reading and writing files, using a 
small set of system calls known as Unix I/O. 

This simple and elegant notion of a file is nonetheless very powerful because 
it provides applications with a uniform view of all the varied I/O devices that 
might be contained in the system. For example, application programmers who 
manipulate the contents of a disk file are blissfully unaware of the specific disk 
technology. Further, the same prograni will run on different systems that ufee 
different disk technologies. You will learn about \jnix I/O* in Chapter 10. 


1.8 Systems Communicate with Other Systems 
Using-Networks 

up to this point in our tour of systems, we have treated a system as an isolated 
collection of hardware and software. In practice, modem systems are often linked 
to other systems by networks. From the point of view of an individual system, the 








20 Chapter 1 A Tour of Computer Systems 




"a? ? „ « ' * 

%j w 






^ ^ >4| 


» »i. 




I ^side* ’tbe,0nux,,p’^j^f:t,j 

I August 1991“ aPmnfsJi^aduSt^stuHeiifriVtpeS ^ifustfop'tl^l^^destfy'announced'a ne^tinix-like 
s Ibgerating system ^rnel:l ^ %= I ^ “* 

trom: *fcorvalds@kla%a.HelsinJ^.FI^, (f!£nus."^ei!edifet iT^r-vklife)* s. * '*' 

Newsgroups: comp.oC-minix* / ■■ ■* ■« ,, | ^ <i» 

Subject: toat* would ydu dilce Ao^^see minix? i , 

.Summary; small poll for iinyrne®»opej?&t:iiig''>sy.st6m ~/’ , 

Date;* 25. Aug 91 26\5tf^8*GMT V*l»| “ *' 

. %* / 

i Hello everybody^out^herp., usin^*mi^i^ss** . ^ 

I'm doing a. (fr^y Qpfra'^injg.|yst’em,.(just *a*hoCby.^.^'wp|}.'|^^e bi^,;4i|^ 
i. prof esbional like "^u^ii^fpl^Sse (486)^ ATJ^J: ones ,fii Tlffis*‘has., be an brewing^ I , 

.since'April, anci is stfcting'to'.get reaSy.-rt'd‘ll&''>.anj feeSback,^on 
”things peo’i3le'*iike4di^like ’ins,minix, as my 0S,«^seinbles ,i,t bpmewhat 
(same physical, laV(Ju4> "bf £lie file^-systemf tdub’'l:p rpractfcil reasons 
among other things ^,%»„ ■s.,.|v ,, ' « 

1 J- ‘ _s»‘^ » '*'«. ' '*- *». 

*1've currepfly. pj^rt‘ed^bash(i ?08X|and ^gfcqC'jC.'fpX, anji/bhing^^’ ‘ 

This implies’that ..I'll’,gs‘t,sbmething piractical withip'A few'months, and 
lid "to ku6w^^uHabs?|6af\ix66''iiiost*''^^ople«/Would-^ant. ^^liy^-suggestions 

are welcome,. but..|J*^bn'#Jpromlsi^‘I«!lf«3.;ppjdmept‘tfi&t"if|)’** 

* ““ Ki ^ “il * “ '*r‘ 


4 

s. 


■# f* *’, ‘ * 

“A* 
6 . 


I 




=v 


Linus ’(torvaldsekruunaVhelsipii ;^f il., ”* 

As’X'i^rvalds infi^atesffiis”staf¥ipVpoinSfoPcFp|tinff£ih^*was jVIinixtail opefatijfigsystenfdevel- 
Oped by Andrew.S: Taqenbaum^fpr.eyucatrqnal.purposes'[y 3jt *' 

The rest' as t^ey say, is^ hfe&ry. Linux hds eYolyf!.d^ntbja technicaland culturaljjhenomenoif. By 
combining fp|-des ,^th the pN13fRroject,|;lie Lihiif^^p|oj'ect,naSgbpveJ^qped»‘a coraiple'te, Ppsor-cbmpliant 
version of the, Unix opepting* system, inclu^in|^ th^TcerneFand^allmf the..supporting- infrastructure^ 
Lin'Ux is available pif;S[ wide, irky oTq6'mputefs,J|-qp *han9held deyice^ t^‘inainframe,gqmpijters. A 

groupaCBMIiasevenT” s. *. | . * '"'V * 


network can be viewed as just another I/O device, as shown in Figure 1.14. When 
the system copies a sequence of bytes from main memory to the network adapter, 
the data flow across the, network to another machine, instead of, say, to a local 
disk drive. Similarly, the system can read data sent from other machines and copy 
these data to its main memory. 

With the advent of global networks such as the Internet, copying information 
from one machine to another has become one of the most important uses of 
computer systems. For example, applications such as email, instant messaging, the 
World Wide Web, FTP, and telnet are all based on the ability to copy information 
over a network. 









Figure 7.14 

A network is another I/O 
device. 


Section 1.8 Systems Communicate.with Other Systems Using Netvvjorks 21 
CPU chip 



1. User types 
“hello" at the 

•“r If’ 

2. Client sends “hello” 
string to telnet server 


3, Server sends ‘hello” 

. ■ string to the shell, which 
j runs the hell,o program 

keyboard ^ 

Local^ 

telnet 

/^emote^ 

telnet 


5. Client prints 
'hello, worldXn” 
string on dispiay 


4. Telnet server sends 
“hello, HorldXn” string 
to client 

\^S6rver 

and passes the output 
to the telnet server 




Figure 1.15 Using telnet to run hello remotely over a network. 


Returning to our hello example, we could use the familiar telnet application 
to run hello on a remote machine.-Suppose we use a telnet client running on our 
local machine ,to connect to ^ telnet se/!ij;er on a remote machine. After we log in 
\o the remote machine and run a shell^ the remote shell is waiting to receive an 
input cpmmand. From this point, running the hello program remotely involves 
the five basic steps sJ[iown in Figure 1.15. i 

After we type in thg hello str^g to the telnet client and’hit the enter key, 
the client sends the string to the telnet server, ^ter the/telnet server receives the 
string from the network, it passes it along to tfie remote shell program. Next, the 
remote sheU runs the hello program and passes the output line back to the telnet 
server. Finally, the telnet server forwards the output string across the network to 
the telnet client, which prints the output string on our local terminal. 

This type df exchange between clients and servers is, typical of all network 
applications. In Chapter J,1 you will learn how .to* build network applications and 
apply this knowledge to build a simple Web server. 










Chapter 1 A Tour of Compufer'Systems 


1.9 Important Themes 

This concludes our initial whirlwind tour of systems. An important idea to Take 
away from this discussion is that a system is more than just hardware. It is a 
collection of intertwined hardware and systems software that must cooperate in 
order to achieve the ultimate goal of running application programs. The rest of 
this book will fill in some details about the hardware and the software, and it will 
show how, by knowing these details, you can writ&programs that are faster, more 
reliable, and more secure. 

To close out this chapter, we highlight several important concepts that cut 
across all aspects of computer systems. We will discuss the importance of these 
concepts at multiple places within the book. 


1.9.1 Amdahl's Law 

Gene Amdahl, one of the early pioneers in computing, made a simple but insight¬ 
ful observation about the effectiveness of improving the performance of one part 
of a system. This observation has come to be known as Amdahl’s law. The main 
idea is that when we speed up one part of a system, the effect on the overall sys¬ 
tem performance depends on both how significant this part was and how much 
it sped up. Consider a system in which executing some application requires time 
^oid- Suppose some part of the system requires a fraction a of this time, and that 
we improve its performance by a factor of k. That is, the component originally re¬ 
quired time aToidi and it now requires time The overall execution time 

would thus be 

^new = (1 - «)^old + (aToidl/k 
= Toid[(l-a) -l-a/k] 


From this, we can compute the speedup S = r^ew as 


5 = 


1 

(1 - q) -b a/k 


'( 1 . 1 ) 


As an example, consid'eP the case where a part of the system that initially 
consumed,60% of the time (a = 0.6) is sped up by a factor of 3 (A = 3). Then 
we get a speedup of'l/[0.4 -|- 0.'6}'3] = 1.67x. Even though we made a'substantial 
improvement to a major part of the system, our net speedup ^yas significantly less 
than the speedup for the one part. This is the major insight of Amdahl’s law— 
to significantly speed up the entire system, we must ifnprove the speed of a very 
large fraction of Ihe^pYerall system'. 


Suppose you work as a truck driver, and you have been hired to carry a load of 
potatoes from Boise, Idaho, to Minneapolis, Minnesota, a total distance of 2,500 
kilometers. You estimate you can average 100 km/hr driving within the speed 
limits, requiring a total of 25 hours for the trip. 










Section 1.9 Important Themes 23 




c „* 

’ y ^ , - '^5> •* ,* *^ v'* • * t- • *”*' ** /*V V ’'■ 

The besrway to’express a‘ perforniahce improveiijeht is as ^ ratiQ of the form^roid/Ti 


newr 


,?where T^iiis, 



,_ufli^^rati 6 ,^li^ is expfresse^Wfe5is^ii^^s’‘‘CidAes?’'^'^^*'^'®^'^^ 5, 

^ '’’^^ilie’mojVltraditidnSl^af^Qf ‘gxpfegsin|,felaiiire’.^cliange|%ssa‘p,efcentage’''works Veil whep'the’ 
cK'an|e is sm^f liiut'Tf^e^lhi^bn^is’ ainbf^felJft.^S^oiifi!; ft; be*ftO''-’|Toi^~ T^wJ/^iew or'posably 
Job'-’jr^id ^r^JlyToid^b^ sSmethYng*el^^^ ad^{tipnritts^lss^'dlftu|^ive foF^ cfi^iges Saying 
'4balf'^plrformandd^niprbveS by,12'0%” is hibre.Mfii^t*td:coiSpreherid thmsinrpiy saying that the 

‘g«fb^feybeiimtlrbVed1Sy;?f2xJ^|$ * 


^55. 






A. You hear on the news that Montana has just abolished ifs speed limit, which 
constitutes 1,500 km o£ the trip. Your truck can travel at 150 km/hr. What 
'willbe your speedup for the trip?J 

B. 'You can buy a new turbocharger fpr your truck at www.fasttl-ucks.com. They 
stock a variety of Models, but the faster you want to go, the mbre it will cost. 
How fast must Vou travel thrbugh Montana to gbt an overall speedup for 

^ your trip oT 1.67 x? 



The marketing departlnent'at your company has promised your customers that 
the nekt software release will show a 2x performance improvement. You have 
bedn aSsi^ed the task of delivering on that promise. You have determined that 
only 80% of the system can be improved.'How much (i.e., what value of k) would 
you need to improve this part to meet the overall performance target? 


^ One interesting specjal case of Amdahl’s law is to cpnsider the effect of setting 
k to oo. That is, we are able to take some part of the system and speed it up to the 
point,at which it takes ^ negligiblei ^ount of time. We then get 

5oo = 7:r^ (1.2) 

(l-a) 

So, for example, if we can speed up’60 Yo of the system to the point where it requires 
close to no time, our net speedup will still only be 1/0.4 = 2.5 x, 

Amdahl’s law describes a general principle for improving any process. In 
addition to its application to speeding up computer systems,!it can guide a company 
trying to reduce the cost of manufacturing razor blades, or a student trying -to 
improve his or her grade point average. Perhaps it is most meaningful in the world 







24 Chapter 1 A Tour of Cornputer Systems 





of computers, where we routinely improve performance by factors of 2 or more. 
Such high factors can only be achieved by optimizing large parts of a system? 

1 , 

1.9.2 Concurrency and Parallelism 

Throughout the history of digital computers, two demands have been constant 
forces in driving improvements; we want them to dq,mqre, and we want them to 
run faster. Both of these factors improve when the processor does more things at 
once. We use the term concurrency to refer to the general corjcept of a system with 
multiple, simultaneous activities, and the term parallelism to refer to the use of 
concurrency to make a system run faster. Parallelism can be exploited at multiple 
levels of abstraction in a computer system. We highlight three levels here, working 
from the highest to the lowest level in the system hierarchy, 

Thread-Level Concurrency 

Building on'the process abstraction, we are able to devise systems where multiple 
programs execute at the same time, leading to concurrency. With threads, we 
can even have multiple control flows executing within a single process. Support 
for concurrent execution has been found in computer systems^since the advent 
of time-sharing in the early 1960s. Traditionally, this concurrent execution was 
only simulated, by having a single computer rapidly switch am^ng its ejcecuting 
processes, much as a juggler keeps multiple balls flying through the air. This form 
of concurrency allows multiple users to interact with a system at the same time, 
such as when many people want to get pages from a single Web server. It also 
allows a single user to engage in multiple tisks concurrently, such as having a 
Web browser in one window, a word processor in another, -and streaming music 
playing at the same time. Until ^recently, roost actual computing was done by a 
single processor, even if that processor had to switch among multiple tasks. .This 
configuration is known as a uniprocmor.^ystem. , . 

When we construct a system consisting of multiple processors all under the 
control of a single operating system kernel, we have a multiprocessor system. 
Such systems have been available for large-scale computing since the 1980s, but 
they have more recently become commonplace with the advent of multi-core 
processors and hyperthreading. Figure 1.16 shows a taxonomy of these different 
processor types.- 

Multi-core processors have several CPUs (referred to as “cores”) integrated 
onto a single integrated-circuit chip. Figure 1.17 illustrates the organization of a 


Figure 1.16 

Categorizing different 
processor configurations. 
Multiprocessors are 
becoming prevalent 
with the advent of multi¬ 
core processors and 
hyperthreading. 


















Section 1.9 Important Themes 25 


Figure 1.17 
Multi-core processor 
organization. Four 
processor cores are 
integrated onto a single 
chip. 




Processor package 



typical multi-core processor, where the chip has/four CPU cores, each with its 
own LI and L2 caches, and with each LI cache split into two parts—one to hold 
recently fetched instructions and one fo hold data. The cores share higher levels of 
cache a^ well’as the interface to main memory. Industry experts predict that they 
will be able to have dozens, and ultimately hundreds, of cores on a single chip. 

Hyperthreading, sometimes ealled simultaneous multi-threading, is a tech¬ 
nique that allows a single ,CPUj execute multiple flows of control. It involves 
having multiple copies of some of the CPU hardware, such as program counters 
and register-files, while>havirfg only single copies of other parts of the hardware, 
such as the'unitsThat perform floating-point arithmetic. Whereas a conventional 
processor requires hrountf 20',000 clock cycles to shift between different-threads, 
a hyperthreaded proeessor decides which of its threads to execute on a cycle-by¬ 
cycle basis. It enables the CPU to take'better advantage of its processing resources. 
For example, if one thread must wait for some data tc/be loaded into a cache, the 
CPU can proceed with the eXfecution of a different thrfead. As an example, the In¬ 
tel Core i7 processor can have each core executing two threads, and so a four-core 
system can actually execute eight threads in parallel. 

The use of-multiprocessing can improve system performance in two ways. 
First, it reduced the need to simulafe'concurrency when performing multiple tasks. 
As mentioned, even a personal computer being used by a single person is expected 
to perform many n^tiyitips concurrently. Spcond, it can run a single application 
program faster, but only if that program is expressed in terms of multiple threads 
that can effectively .execute in parallel. Thus, although the principles of concur¬ 
rency have been formulated and studied for over Sdy ears, the advent of multi-core 
and hyperthreaded systems has greatly increased the desire to find ways to write 
application programs that can exploit the thread-level parallelism available with 











26 Chapter 1 A Tour of Computer Systems 


the hardware. Chapter 12 will look much more deeply into concurrency and .its 
use to provide a sharing of processing resources and to enable "more .parallelism 
in program execution. ■' 

lnstruction-l.evel Parallelism 

At a much lower level of abstraction, modern processors can execute multiple 
instructions at one time, a property known as instruction-level parallelism. For 
example, early microprocessors, such as the 1978-vintage Intel 8086, required 
multiple (typically 3-10) clock cycles to execute a single instruction. More recent 
processors can sustain execution rates of 2-4 instructions per clock cycle. Any 
given instruction requires much longer from start to finish, perhaps 20 cycles or 
more, but the processor uses a number of clever tricks to process as many as 100 
instructions at a time. In Chapter 4, we will explore the use of pipelining, where the 
actions,requir£d to execute an instruction are partitioned into different steps and 
the processor hardware is organized as a series of stages, each performing one 
of these steps. The stages can operate in parallel, working on different parts of 
different instructions. We will see that a fairly simple hardware design can sustain 
an execution rate close to 1 instruction per clock cycle. 

Processors that can sustain execution rates faster than 1 instruction per cycle 
are known as superscalar processors. Most modem processors support superscalar 
operation. In Chapter 5, we will deseribe a high-level model of such processors. 
We will see that application programmers can use this modefito understand the 
performance of their program's. They can then write programs such that the gen¬ 
erated code achieves higher degrees of instruction-level parallelism and therefore 
runs faster. j 

Single-Instruction, Multiple-Data (SIMD) Parallelism 

At the lowest level, many modern processors have*special hardware that allows 
a single instruction to .’cause multiple .operations to be performed in parallel,,a 
mode known as single-instruction, multiple-data (SIMD) parallelism. Fpr example, 
recent generations of Intel and AMD,processors have instructibns that can add 8 
pairs of single-precisjon floating-point-numbers (C data type float) in par.allel. 

These SIMD instructions are provided mostlyJtp speed ,up applications that 
process image, sound, andwidpo data. Although sopie,compilers attempt-to auto¬ 
matically extract SIMD parallelisnufrom Cf progj:fims,-a more reliable method is to 
write programs using special veptordaXa. types supported inporqpilers such as Gcc. 
We describe this style of programmingiii Web Aside opt:simd, as.a supplement to 
the more general presentation .on progr,nm optimization found in Chaptej 5. 

1.9.3 The Importance bf Abstractions iri Cornputer ^ysthms 

The use of abstractions is one of the most important concepts in computer science. 
For example, one aspect of "good programming practice is to formulate a simple 
application prbgram interface‘( API) for a set of functions that allow programmers 
to use the code without having to delve into its inner workings. Different program- 

















Section 1.10 Summary 27 


Figure 1.18 
Some abstractions 
provided,by a computer 
system. A major theme 
in computer systems 
is to provide abstract 
representations at 
different levels to hide 
the complexity of the 
actual implementations.. 


Virtual machine 


( -- 




Processes 


1 \ 

t 1 instruction set 


-s 

J 

1 1 architecture 

1 '-A__ 

Virtual memory | 




1 t 


Files ! 

1 



-^-1 

Operating system 

Processor 

Main memory 

I/O devices 


ming languages provide different forms and levels of support for abstraction, such 
as Java class declarations and C function prototypes. 

We have already been introduced to several of the abstractions seen’in com¬ 
puter systems, as indicated in Figure 1.18. On the processor side, the instruction set 
architecture provides an abstraction of the actual processor hardware. With this 
abstraction, a machine-coc^e program.behaves as if it were executed on a proces¬ 
sor that performs just ope instruction at a time. The underlying hardware is far 
more elaborate, executing multiple instructions in parallel, but always in a way 
that is consistent with the simple, sequential model. By keeping the same execu¬ 
tion model, different processor implementations can execute the same machine 
code while offering a range of cost and performance. 

On the operating system side, we have introduced threfe abstractions: files as 
an abstraction of I/O devices, virtual memory as an abstraction of program mem¬ 
ory, and processes as an abstraction of a running program. To these abstractions 
we add a new one: the virtual machine, providing an abstraction of the entire 
computer, including the operating system, the processor, and the programs. The 
idea of a virtual machine was introduced by IBM in the 1960s, but it has become 
more prominent recently as a way to manage computers that must be able to run 
programs designed for multiple operating systems (such as Microsoft Windows, 
Mac OS X, and Linux) or different versions of the same operating system. 

We will return to these abstractions in subsequent sections of the book. 


1.10 Summary 

A computer system consists of hardware and systems software that cooperate 
to run application programs, Information inside tlje computer is represented as 
groups of bits that are interpreted in different ways, depending on the context. 
Programs are translated by other programs into different forms, beginning as 
ASCII text and then translated by compilers and linkers into binary executable 
files. 

Processors read and interpret binary instructions that are stored in main mem¬ 
ory. Since computers spend most of their time copying data between memory, I/O 
devices, and the CPU registers, the storage devices in a system are arranged in a hi¬ 
erarchy, with the CPU registers at the top, followed by multiple levels of hardware 
cache memories, DRAM main memory, and disk storage. Storage devices that are 
higher in the hierarchy are faster and more costly per bit than those lower in the 





I 


28 Chapter 1 A Tour of Computer Systems 

hierarchy. Storage devices that are higher in the hierarchy serve as caches for (de¬ 
vices that are lower in the hierarchy. Programmers can optimize the performance 
of their C programs by understanding and exploiting the memory hierarchy. 

The operating system kernel serves as an intermediary between the'applica¬ 
tion and the hardware. It provides three fundamental abstractions: (1) Files are 
abstractions for I/O devices. (2) Virtual memory is an abstraction for both main 
memory and disks. (3) Processes are abstractions for the processor, main memory, 

} and I/O devices^ ' • • i. 

Finally, networks provide ways for computer systems to communicate with 
one another. From the viewpoint of a particular system, the network is just another 
I/O device. 

11 , I 

Bibliographic Notes 

■ Ritchie has written'.intefesting firsthand accounts of the early days of C and 

Unix [91, 92]. Ritchie and Thomps6n presented the ^firsf published accouift of 
Unix [93]’. Silberscliatz' Galvin, and Ga^e' fl02]' provide a comprehensive history 
of the differeht flavors of Unix. The GNU (www.gmi.oi'^^ and Lintix (www.linux 
.org)' Web pages 'have loads ‘of current arid historical infofmation. The Posix 
standards are available online at (wv^.unix.org). 

I: ^ 

I Solutions ^o Practice Prpblems 

Solution to Problem 1.1 (page 22) 

This^prbblem illustrates that Amdahl’s law a|)plies to more’than just computer 
systems. ^ , 

t A. In terms of.Equation 1.1, we have a = 0.6-and k = 1.5. Mpre directly, travel- 

' ing the 1,500 kilometers thro,ugh Montana will require 10 hours, and the rest 

I of the trip also requires 10 hours. This will give a speedup qf 25/ (10 + 10) = 

J 1.25 X. - 

' B. -In tdrms of Equation 1.1, we have a. = 0.6, and we require S = 1.67, from 

ij which we can solve for k. More directly, to speed up the trip by 1.67 x, we 

must decrease the overall time to 15 hours. The parts outside of Montana 
will still require 10 hours, so we must drive through Montana in 5 hours. 
This requires traveling at 30Q km/hr, which is pretty fast for a truck! 

Solution to Problerri 1.2 (pagte 23)' 

1 Amdahl’s law is best uncferstood't'y workihg through some examples. This ohe 

I requires you to look At EquatiqnT.l from an unusual perspective. 

This problem'is a simple application of the’ equation. You are given 5 = 2 and 
a = 0.8, and you must then solve for k: 


‘ Cl'*- 0.8) *+ 0.8//c 
(5:4 -h'1.6/it = 1.0 
k = 2.67 












'*r * 

:;^ '■■■"i'S';fv^'*. t,», 
''-T^:'iV >-~ '■»!’.••? K'4 .. 


' I X. 
„■> #"}.■. I;..^*-? * 


rt I 


, 5 - 




, .■■•'',! »<■ > , ■< 
, > y-: S * 


Program Structure 
saftd^Execution 


"%f»r exploration of computer systems starts by studying the com- 

c’k-‘ tTf^-^^Pwter itself, comprising a processor and a memory subsystem. At 
the core, we require ways to represent basic data types, such as 
to integer and real arithmetic. From there, we can con- 
“^^iiin^-ievel instructions manipulate data and how a com- 
‘Vpilef traflsIatesICDroerams into fhfisf* instmr'tirinc - 1 


5 ’—.— iiiaiiui.,iiuiis iiiaiiipuiaie aaia ana now a com- 

" into these instructions. Next, we study several 

imf)lementing a processor to gain a better understanding of 
hardware resources are used to execute instructions. Once we under- 
»^ ®fand compilers and machine-leVel code, we can examine how to maxi- 

program performance by writing C programs that, when compiled, 
^ ^^^chieye^the maximum possible performance. We conclude with the de- 

;>">|igh ?f the memory subsystem, one of the most complex components of 
n jhodern computer system. 

'% i 3 * of-the book will give you a deep understanding of how 

~y * yv';ji/‘ ';'^l^l'ii^alion-programs are represented and executed. You will gain skills 
?rf':#at Imlp you Write programs that are secure, reliable, and make the best 
^ ^use^'^e computing resources. 


Mm 






,, 1 ^ M 

U T .1^ V k" W '■ 











Representing and Manipulating 
Information 


2.1 Information Storage 34 

2.2 Integer Representations 59 

^.3 Integer Arithmetic 84 

2.4 Floating Point 108 

2.5 Summary 1^6 
Bibliographic Notes 127 
Homework Problems 128 
Solutions to Practice Problems 143 


31 


32 Chapter 2 Representing and Manipulating Information 


M odern computers store and process information represented as two-valued 
signals. These lowly binary digits, or bits, form the basis of the digital revo¬ 
lution. The familiar decimal, or base-10, representation has been in use for over 
j 1,000 years, having been developed in India, improved by ‘Arab mathematicians in 

i the 12th century, and brought to the West in the 13ti century by the Italian mathe¬ 

matician Leonardo Pisano (ca. 1170 to ca. 1250), belter known as Fibonacci. Using 
decimal notation is natural for 10-fingered humans, but binary values work better 
when building machines that store and process information. Two-valued signals 
can readily be represented, stored, and transmitted—for example, as the presence 
or absence ofa hole in a' puncKed card, as a high or low voltage on a wire, or as a 
magnetic domain oriented clockwise or counterclockwise. The electronic circuitry 
“ for storiii^ anfd performing computations on two-valued signalsjs very simple and 

reliable, enabling manufacturers to integrate millions, "Or even billions, of such 
. circuits on a single silicon chip. ^ | 

In isolation, a single bit is not very'useful. When we group?-bits together and 
apply some interpretation that gives meaning to the different possible bit patterns, 
however, we can represent the elements of any finite set. For example, using a 
binary number system, we can use groups of bits to encode nonnegative numbers. 
By using a standard character code, we can encode the letters and symbols in a 
document. We cover both of these encodings in this chapter, as well as encodings 
to represent negative numbers and to approximate,real numbers. 

We consider the three most important representations of numbers. Unsigned 
encodings are based on traditional binary notation, representing numbers greater 
than or equal to 0. Two’s-complemerlt encodings'are the m6st common way to 
represent signed integers, that is, numbers that may be either positive or negative, 
j Floating-point encodings are a base-2 version of scientific notation for represent- 

i ing real numbers. Computers implement arithmetic operations, such as addition 

and multiplication, with these different representations, similar to the correspond¬ 
ing operations on integers and real numbers. 

Computer reprekentationsnise'a limitedmumber of bits to encode a number, 
■ and hence some operations can overflow when the results are too large to be rep¬ 

resented. This can lead to some surprising results. For example, on most of today’s 
computers (those using a 32-bit representation for data type int), computing the 
expression 

I 200 ♦ 300 * 400 * 500 

I yields —884,901,888. This runs coimter to the properties of integer arithmetic— 

computing the product of a set of positive numbers has yielded a negative result. 
On the other hand, integer computer arithmetic satisfies many of the familiar 
j properties of true integer arithmetic. For example, multiplication is associative 

I and commutative, so that computing any of the following C expressions yields 


-884,901,888: 

(600 * 

400) * (300 * 200) 

((500 + 

400) * 300) * 200 

((200 * 

600) * 300) ♦ 400 

400 * 

(200 + (300 * 500)) 



















Chapter 2 Representing and Manipulating Information 


33 


The computer might not generate the expected result, but at least it is con¬ 
sistent! 

Floating-point arithmetic has altogether different mathematical properties. 
The product of a set of positive numbers will always be positive, although over¬ 
flow will yield the specid value -boo. Floating-point arithmetic is not associative 
due to the finite precision of the representation. For example, the C expression 
(3.14+le20)-le20 will evaluate to 0.0 on most machines, while 3.14-^(l■e20- 
le20) will evaluate to 3.14. The different mathematical properties of integer 
versus floating-point arithmetic stem from the difference in how, they handle the 
finitenesspf their representations—integer representations can encode a compar¬ 
atively small range of values, but do so precisely, while floating-point representa¬ 
tions can encode a wide range of values, but only approximately. 

By studying the actual number representations, we can understand the ranges 
of values that can be represented and the properties of the different arithmetic 
operations. Th is understanding is critical to writing programs that work correctly 
over the full range of numeric values and that are portable across different combi¬ 
nations of machine, operating system, and compiler. As we will describe, a number 
of computer security vulnerabilities have arisen due to some of the subtleties of 
computer arithmetic. Whereas in an earlier era program bugs would only incon¬ 
venience people when they happened to be triggered, there are now legions of 
hackers who try to exploit any bug they can find to obtain unauthorized access 
to other people’s systems. This puts a higher level of obligation on programmers 
to understand how their programs work and how they can be made to behave in 
undesirable ways. 

Computers use several different binary representations to encode numeric 
values. You will need to be familiar with'these representations as you progress 
into machine-level’programming in Chapter 3. We describe these encodings in 
this chapter and show you how to reason about Jiumber representations. 

We derive several ways to perform arithmetic operations by directly ma¬ 
nipulating the bit-level representations of numbers. Understanding these tech¬ 
niques will be important for understanding the machine-level code generated by 
compilers in their attempt to optimize the performance of arithmetic expression 
evaluation. 

Our treatment of this material is based on a core set of mathematical prin¬ 
ciples. We start with the basic definitions of the encodings and then derive such 
properties as the range of representable numbers, their bit-level representations, 
and the properties of the arithmetic operations. We believe it is important for you 
to examine the material from this abstract viewpoint, because programmers need 
to have a'clear understanding of how computer arithmetic relates to the more 
familiar- integer and real arithmetic. 

The C+-I- programming language is built upon C, using the exact same numeric 
representations and operations. Everything said in this chapter about C also holds 
for C-H-. The Javaianguage definition, on the other hand, created a new set of 
standards for numeric representations and operations. Whereas the C standards 
are designed to allbw a wide range of implementations, the Java sta'ndard is quite 
specific on the formats and encodings of dafa. .We highlight the representations 
and operations supported by Java at several places in the chapter. 










34 Chapter 2 Representing and Manipulating Information 












: 


^lliside.: How to read;this,chaptep s. 

In this chapter, we epininestlie%Uh'dam*en?ahp£Spe^^s'o^f'hoV n4m6Ms*^d“oth"lfIprlnfSl ^ta lre/| 


:5!r - v‘>‘ t. 

»# 4f 
,. 0 > 


..•i^ s 







derivations of important projtertieU* "'"“I ’ ^ ft* 

To help you na’^i|ate^diis’expWti8n|wffe^ave structured tlie'|)re’'^eniSiSnJo‘fir%^state a property' ^ 
as aftririciple in mathematical ndmtiohri^e then illustrate t|ii^'pfinci^le'witfi exdnipl^’and an informal' | 
discussion. We reco&rnend’tKat-'ydu*^ back’and'forth between the' sfa’tetnent^df the principle’aha Old’* | 
examples and discussioii"until ydif'have a solid ihfuiti 9 n.fQr”what’is. beji|g sptfand'whaHsnm'pqrtaiir | 
about the prophrty.’T'dr m6‘re i 
a.,mathematical piroof. Yotf: 

over them on first reading. 1" ^ ^ ^ ^ 

We also encourage jou^o vrorldon th^praclice prdbleiiis a^sydii prdcfeedbhrdhglr the pfesfenfalfon. j 
The practice problems ^hg^^ge^duun active learning, helping you pufttiougfits in!o;ac1jdh. With t^es&y 
as background, you*^lMhd i/muclf'fasifr^fo‘gp'^aick,ahd''follow the denvatiohS^Be asshrld', as weflJ*J 
that.the mathematicaf skills'required to understand this matefiai are within felcfi'of%ohie6ne‘with‘a« | 
good grasp of high &icKooPal|ebraf: ‘ ^ *4,' »■ ■«“ I 



f 


2.1 Infbrmation Storage 


Rather than accessing individual bits in memory, most computers use blocks of 
8 bits, or bytes, as the smallest addressable unit of memory. A- machine-level 
program views memory as a very large array of bytes, referred to as virtual 
memory. Every byte of memory is identified by a unique number, known as its 
address, and-the set of all possible'addresses is known as the virtual address space. 
As indicated by its'name, this virtual address space is just a’conceptfial image 
presented to the machine-level program. The actual implementation (presented 
in Chapter 9) dses a combination of dynamic random-access memory (DRAM), 
flash memory, disk storage, special hardware, and operating system software to 
provide the program with what appears to be a monolithic byte array. 

In subsequent chapters,.we will cover how the compiler and run-time system 
partitions this memory space into more-manageablefunitsho store the different 
program objects, that is, program data, instructions, and control information. 
Various mechanisms are used to allocate and manage the storage for different 
parts of the program. This management is all performed within the virtual address 
space. For example, the value of a pointer in C—whether it 4 )oints taan integer, 
a structure, or some other program object—is the virtual address of the 'first bytd 
of some block of storage. The C compiler also associates Type-information with 
each pointer, so that it can generate different machine-level code to access the 
value stored at the location designated by the pointer dependington the type of 
that value. Although the C'compiler maintains'-this type information, the actual 
machine-level program it generates has no information about data types. It simply 
treats each program object as a-block of bytes and the program itself as a sequence 
of bytes. 














I 

i 


Section 2.1 Information Storage 35 






I Aside The evolution of the C programming language* 

j Aswas described in an aside on page 4, theC programming langtiage was first developed by Dennis 
I Ritchie of Bell Laboratories for,use with the Llnix operating system Xalso developed at‘Bell Labs). At 

i the time, most system programs, such as operating systems, had to be written largely in assembly code 
in order toihave access to the low-level representations.,of different data*types.* j^or.example, it was 
I not feasible to write a membry allocator, such as is provided by,the malloc library function, in other 
I high-level languages of that’era. • '* 

I The original Bell l^bs version pf C was documented in the first edition of the book by,Brian 
j Kernighan and Dennis Ritchie [60]. OVer time^ P hks evolved tfirpugh the efforts of several standard- 
I ization groups. The fijst major revision of fhe original Bell Labs d led^to the ANSI C standard in 1989,' 
by a’group working imder the auspices oLthe ^iherican National Stan&ard? fnstitute. ANSI C was a 
major departure from Bell Dafcs C, Specially in tlhe way functions'are declared. ANSI C is described, 
in tfaetsecpnd.e^itiop of Kernighan and Ritcliie’s book [6)], which’is still considered one of the best, 
references on C.;„ ^ ^ * , 

'The^lnternatipnal jStapdards^Organization took oyer jesponsitjility for, standardizing the C lan¬ 
guage, adopting a version thaf was suhstanjially’the sarnp as Ah^Sl Q in anS-hence is referred to 
as“ISOpk’;‘ I * f: 

This spipp organization sponsored an updating o’f th^language in 1999, yjeldmg “ISO C99.” Among 
other tltings,^this, vision introduced sopip new data ^yp|s and provided support for text strings requiring 
c^arqpters not fquj\i^ in the English Iqpgu^ge. ^ more a;i^cent sfat^dard was apjjroved in 2011, and hence 
is named ITSO Cll,” again'adding’m,ore data types and'fea(ures .Most of these recent additions have 
.beeahacAfivatid meaning,that programs, written according fo the earlier standard (at least 

as far back-as |Sp C90)‘y[ill hav?|he same Bbhivfor when complled'accordinglo the newer standards. 

jT^le^GNlJ Conjpiier CoUection^(GCc) can coinpile prp^gm^A^^^t^ipg to the conventions of seyeral 
differentwersmris pf the Cl^nguagb^bajed on different papnin^qd-line op|y3ns, as shown in Figure 2.1. 
For example, to cqm^il^ prpgram'prdg^c acOTjdiif^fp^SO^Cli, ^YP CI?uld giye^thexomraand line 

d.in.ux>i''gcc -%td=clli^rpg.C'‘ ’ T’ , ... . j,. 

F ' fa*'* f, 

^e'b^tions -ansi and -st;d=c8^ haVe'idfefltical effect—the’code id compiled d&ording to the ANSI 
or ISO^^o!st^nda£d.'(t9d'\s sometlih^s referred to as ‘‘€89^ sirice'its' standa/difeation effort began in 
T989.)'Thp^optibn -.s^'d=q9& causes the’pompifer tcf'fo)ldvy']the 1^0 C99convention. 

^ of the wEitiiig of this book; 'Jfhen’no option is sp|cified,‘the'f)ro^rant vilili fee compiled according 
tQ a yersibji’BJ C’feased on ISO^C90, ibur.ihcluding soine'features* of G99,'>some of ’Cll, some of 
C++, and.bthers specific to acc.'The GNU pro|ect,is developing aversion that combines ISO Cll, 
pluSjOther features, that can b 9 «sR?cified wjtH the command-ling option,-std^gnujll. (Currently, this 
-impleinentation is incpmpletb:) Thjs wiU become the»default versioft. 


C version 

ccc command-line option 

GNU 89 

none, -std=gnu89 

ANSI, ISO C90 

-ansi, -std=c89 

ISO C99 

-std=c99 

ISO Cll 

-std=cll 


Figure 2.1 Specifying different versions of C to ccc. 




36 Chapter 2 -Representing and Manipulating Information 


New’to C?' The role of pointers In €v » 1 

^Point^rs^axd' a c’entrdl feature* of C.^TTieyj|ftoyide the mechanism for reffere.nciifg,elemfents’of data I 
structures, including’arrays,*-Just like” a variable,* a pointer has two aspects: its value and 1fs-type. The J 
value indicates the location of’some object|*whileats^typeindicates whaj Icind-of object (e.g., jntegdi or ' 
floatihg-^pint number) is stored at fhSf location. «" *•* 

Tbuly‘understanding pointers requires, examining their representation andimplemeritation at the 
machine level. Ais will be a major focus in Chapter 3, culminating in an in-d”epth presentation in Section 
53.10.T. . - » * 


2.1.1 Hexadecimal Notation 

A single byte consists of 8 bits. In binary notation, its value ranges from OOOOOOOO 2 
to 111111112 - When viewed as a decimal integer, its value ranges from O^o to 255io- 
Neither notation is very convenient for describing bit patterns. Binary notation 
is too verbose, while with decimal notation it is tedious to convert to and from 
bit patterns. Instead, we write bit patterns as base-16, or hexadecimal numbers. 
Hexadecifnal (or simply “hex”) uses digits ‘0’ through ‘9’ along with characters 
‘A’ through ‘F’ to represent 16 possible values. Figure 2.2 shows the decimal and 
biliary values associated with the 16 hexadecimal digits. Written in hexadecimal, 
the value of a single byte can range from to FF 15 . 

In C, numeric constants starting with Ox or OX are interpreted as being in 
hexadecimal. The characters ‘A’ through ‘F’ may be written in either upper- or 
lowercase. For example, we could write the number FAlD37Big as 0xFAlD37B, as 
Oxf ald37b, or even fnixing upper- and lowercase (e.g., 0xFalD37b). We will use 
the C notation for representing hexadecimal values in this book. 

A common task in working with machine-level programs is to manually con¬ 
vert between decimal, binary, and hexadecimal representations of bit patterns. 
Converting between binary and hexadecimal is straightforward, since it can be 
performed one hexadecimal digit at a time. Digits can be converted by referring 
to a chart such as that shownTn Figure 2.2. One simple trick for doing the conver¬ 
sion in your head is to memorize the decimal equivalents of hex digits A, C, and F. 


Hex digit 

0 

1 

2 

3 

4 

5 

6 

7 

Decimal value 

0 

1 

2 

3 

4 

5 

6 

7 

Binary value 

0000 

0001 

0010 

0011 

0100 

0101 

0110 

0111 

Hex digit 

8 

9 

A 

B 

c 

D 

E 

F 

Decimal value 

8 

9 

10 

11 

12 

13 

14 

15 

Binary value 

1000 

1001 

1010 

1011 

1100 

1101 

1110 

nil 


Figure 2.2 Hexadecimal notation. Each hex digit encodes one of 16 values. 













Section 2,1 Information'Storage 37 

The hex values B, D, and E can be translated to decimal by computing their values 
relative to the first three. 

For example, suppose you are given the number 0xl73A4C. You can convert 
this to binary format by expanding each hexadecimal digit, as follows: 

Hexadecimal 1 7 3 A 4 C 

Binary 0001 0111 0011 1010 0100 1100 

This gives the binary representation 000101110011101001001100. 

Conversely, given a binary number 1111001010110110110011, you convert it 
to hexadecimal by first splitting it into groups of 4 bits each. N'dte| hdtvever, that if 
the total number bf bits is not £t multiple of 4, you'should make the leftmost group 
be the one with fewfer than 4 bits, effectively padding the number with leading 
zeros. Then you translate each group of bits into trie corresponding hexadecimal 
digit: ‘ 

Binary 11 1100 1010 llOl 1011 0011 

Hexadecimal 3 C A D B 3 



Perform the following number conversions: 


A. 0x39A7F8 to binary 

B. bipary 1100100101111011 to hexadecimal 

C. 0xD5E4C to binary ^ 

D. binary 1001101110011110110101 to hexadecimal 

When a value x- is a power of 2, that‘is, x = 2" for some ijonnegative integer 
h, we can readily write x in hexadecimal fbrm by remembering that the binary 
representation of x is simply 1 followed by n zeros. The hexadecimal digit 0 
represents 4 binary zeros. So, for n written in the form 1 + 4y, where 0 < ij< 3, 
we can write x with a leading hex digit of 11* = 0), 2 (i = l), 4 (i = 2)’, or 8 
(i = 3), followed by j hexadecimal Os. As an example, for x = 2,048 = 2^^ we 
have rt = 11 = 3 + 4 • 2, giving hexadecimal representation 0x800. 



Fill in the blank entries in the following table, giving the decimal and hexadecimal 
representations of different powers of 2; 










38 Chapter 2, Representing and Manipulating Information 



n 

T 

19 


17 


2" (decimal) 2" (hexadecimal) 
512 0x200 


i6,384 

_ 0x10000 


32 _ 

_ 0x80 


Converting between decimal and hexadecimal representations requires using 
multiplication or division to handle the general case. To convert a decimal num¬ 
ber X to hexadecimal, we can repeatedly divide x by 16, giving a quotient q and a 
remainder/, such that x=q--\6 r,. We then use the hexadecimal digit represent¬ 

ing r as the least significant digit and generate the remaining digits by repeating 
the process on q. As an example, consider the conversion of decimal 314,156: 


'314,156 = 19,634 • 16‘-h 12 (G) 

19,634 = 1,227-16-1-2 (2) 

1,227 = 76-16-h 11 (B) 

76 = 4 • 16 -f 12 (C) 

4 = 0 • 16 + 4 (4) 


From this we can read off the hexadecimal representation as 0x4CB2C. 

Conversely, to convert a hexadecimal number to decimal, we can multiply 
each of the hexadecimal digits by the appropriate power of 16. For example, given 
the number 0x7AF, we compute its decimal equivalent as-7 ■ 16^ -b 10 ■ l6 -1-15 = 
7 • 256 -f 10 ■ 16 + 15 = 1.792-4-160 +15 = 1,967. 


A.single^byte can„be represented by,2 hexadecimal digits..Fill in the missing 
entries in the follojving table, giving the decimal, binary, and hexadecimal values 
of different byte, patterns; r 


Decimal 

Binary 

Hexad*ecim‘al 

0 

0000 0000 

0x00 

167 



62 



188 






00110111 
10001000 
11110011 















Section 2,1 Information Storage 39 


^i^%ifonyerting^e^(^n^de^imafand hexac%^ » ."%t % 

Fo^ convfefting larger vaJu'es^lj^tvsjeen decimal and hexadecimal, ft is bestft'o let’"^ Computer or calculafor* * 
do the w 6 rfe There nre ntimeroijs jtools,that can do; this. One simple^ way js tp*use,any of the standard * 
‘seardh engiiles^ witii qi^ri'es §uch ks' " ^ ' " * * 

' ‘ * * * 



1. tv '^Cmxverhdiiabcd tb'^^iftiSl 


i 

as. 


J2^ip he^ 




4 ^ 'y^ ' 

>»i '.‘»i‘f ^ 


% 

S' 




Decimal feinary Hexadecimal 

___:_ 0x52 

_L_ OxAC 

^__ 0xE7 




i 







msas 


--,-- ,--r---- 

Without converting the nijmbers.to decimal or binary, try to solve the following 
arithmetic, problems, giving the answers in hexadecimal. Hint: Just modify the 
methods you use for performing, decimal addition and subtraction to use base 16. 


A. 0x503c + 0x8 =_ 

B. 0x503c - 0x40 = _ 

C. 0x503c + 6,4 =,_ 

D. 0x50ea — 0x503c = 


2.1.2 Data Sizes 

Every computer has a word size, indicating the nominal size of pointer data. Since 
a virtual address is encoded by such a word, the most important system parameter 
determined by the word size is the maximum size of the virtual address space. That 
is, for a machine with a lu-bit word size, the virtual addresses can range from 0 to 
2^ — 1 , giving the program access to at most 2 *" bytes.- 

In recent -years, there has been a widespread shift from machines with 32- 
bit word sizes to those with word sizes of 64 bits. This occurred first for high-end 
machines designed for large-scale scientific and database applications, followed 
by desktop and laptop machines, and most recently for the processors foupd^in 
smartphones. A 32-bit word size limits the virtual address space to 4 gigabytes 
(writteh 4 GBX that is, just over 4 x 10® bytes. Scaling up to a 64"bit word size 
leads to a*virtual address space of 16 exabytes, dr arourid 1.84 x 10^® bytes. 








Chapter 2 Representing and Manipulating Information 


Most 64-bit machines can also run programs compiled for use on 32-bit ma¬ 
chines, a form of backward coinpatibility. So, for example; when a program prog. c 
is compiled with the directive 

linrnO gcc -m32 prog.c 

then this program will run correctly on either a 32-bit or a 64-bit machine. On the 
other hand, a program compiled with the directive 

linux> gcc -m64 prog.c 

will only run on a 64-bit machine. We will therefore refer to programs as being 
either “32-bit programs” or “64-bit programs,” since the distinction lies in how a 
program is compiled, rather than the type of machine on which it runs. 

Computers and compilers support multiple data formats using different ways 
to encode data, such as integers and floating point, as well as different lengths. 
For example, many machines have instructions for manipulating single bytes, as 
well as integers represented as 2-, 4-, and 8-byte quantities. They also support 
floating-point numbers represented as 4- and 8-byte quantities. 

The C language supports multiple data formats for both integer and floating¬ 
point data. Figure 2.3 shows the number of bytes typically allocated for different C 
data types. (We discuss the relation between what is guaranteed by the C standard 
versus vrhat is typical in Section*2.2.) The exact numbers of bytes for some data 
types depends on how the program is' compiled. "show sizes for typical 32-bit 
and 64-bit pro^ams. Integer data can be either signed, able to represent negative', 
zero, and positive values, or unsigned, only allowing nonnegative values. Data 
type char represents a single byte. Although the name char derives from the fact 
that it is used to store a single character in a tekt string, it can also be used to store 
integer values. Data types short, int, and long are intendedio provide a range of 


C declaration 


Bytes 

Signed 

Unsigned 

32-bit 64-bit 

[signed] char 

unsigned char 


1 

short 

^iinsigned short 

2 

2 

int 

unsigned 

4 

4 

long 

unsigned long 

4 

8 

int32_t 

uint32_t 

4 

4 

int64_t 

uint64_t 

8 

8 

char’ + 


4 

8. 

float 


4 

4 

double 


8 

8 


Figure 2.3 Typical sizes (in bytes) of basic Cdata types. The number bf bytes allocated 
varies with’hoW the program is compiled. This chart shows the values typiqal of 32-bif 
and 64-bit programs. 















Section 2.1 Information Storage 41 


/i ,! 


!N^wtq^? Declaring pQinter? 

For any data type T, the declaration s- . t 

*, V it. , i a ^ * 

T ^ » ’ 'I * ,, 

indicates that p js a'pointer variable, pointing to an object of type T. For example, 

^ % <«. '-f. 

char *p;,, . ^ i 

is the declaration of h pointer.to'an object of type char. 


&». 




sizes. Even when compiled for 64-bit systems, data type int is usually just 4 bytes. 
Data type long commonly has 4 bytes in 32-bit programs and 8 bytes in 64-bit 
programs. , 

To avoid the vagaries of relying on “typical” sizes and different compiler set¬ 
tings, ISO C99 introduced a class of data types where the data sizes are fixed 
regardless of compiler and machine settings. Among these are data types int32_t 
and int64_t, having exactly 4 and 8 bytes, respectively. Using fixed-size integer 
types is the best way for programmers to have close control over data represen¬ 
tations. 

Most of the data types encode signed values, unless prefixed by the keyword 
unsigned or using the specific unsigned declaration for fixed-size data types. The 
exception to this is data type char. Although most compilers and machines treat 
these as signed data, the Gstandard does not guarantee this. Instead, as indicated 
by the square brackets, the programmer should use the declaration signed char 
to guarantee a 1-byte signed value. In many contexts, however, the program’s 
behavior is insensitive to whether data type char is signed or unsigned. 

The C language allows a variety of ways, to order .the keywords and to include 
or omit optional keywords. As examples,' all- of the following declarations have 
identical meaning: 

unsigned long 
unsigned long int 
long unsigned 
long unsigned int 

We will consistently use the forms found in Figure 2.3. 

Figure 2.3 also shows that a pointer (e.g., a variable declared as being of 
type char *) uses the full word size of the program. Most machines also support 
two different floating-point formats: single precision, declared in C as float, 
and double precision, declared in C as double. These formats use 4 and 8 bytes, 
respectively. 

Programmers should strive to make their programs portable actoss different 
machines and compilers One aspect of portability is to make the-program insensi¬ 
tive to the exact sizes of the different data types. The C standards set lower bounds 


42 Chapter 2 Representing and Manipulating Information 


on the numeric ranges of the different data types, as will be covered later, but there 
are no upper bounds (except with the fixed-size types). With 32-bit machines and 
32-bit programs being the dominant combination from around 1980 until around 
2010, many programs have been written assuming the allocations listed for 32- 
bit programs in Figure 2.3. With the transition to 64-bit machines, many hidden 
word size dependencies have arisen as bugs in migrating these programs to new 
machines. For example, many programmers historically assumed that an object 
declared as type int could be used to store a pointer. This works fine for most 
32-bit programs, but it leads to problems for 64-bit programs. 

2.1.3 Addressing and Byte Ordering 

For program objefcts that span multiple bytps, we must establish two conventions: 
what the address of the object will be, and how we will order the bytes in memory. 
In virtually all machines, a multi-byte object is stored as a contiguous sequence 
of bytes, with the address of the object given by the smallest address of the bytes 
used. For example,*suppose a variable x of type int has address 0x100; that is, the 
value of the address expression &x is 0x100. Then (assuming data type int has a 
32-bit representation) thef 4 bytes of x would be stored in memory locations 0x100, 
0x101,0kl02; and 0x103. ' ' 

For ordering the bytes representing an object, there are two common conven¬ 
tions. Consider a lo-bit integer having a bit representation [xu,_i, x„,_ 2 ,.... xj.'Xo], 
where Xy,_i is the most significant bit and xq is the least. Assuming u; is a multiple 
of 8, these bits can be grouped as bytes, with the most "Significant byte having bits 

■ ■ ■ • the least sighificant byte having bits [X 7 , X(, .xq], and 

the other bytes having bits from the middle. Some machines choose to store the ob- 
ject in memory ordered from least significant byte to most, while other machines 
store them from most tb least. The former convention—where the least significant 
byte comes first—isTeferred to as little'endian. The latter conventiOn-^where the 
most significant byte comes first—is"referred*to as big endian'. 

Suppose the variable x of type int and at address 0x100 has" a hexadecimal 
value of 0x01234567. The ordering of the bytes within the address range 0x100 
through 0x103 depends on the type of machine: 

Big endian 


0x100 0x101 0x102 0x103 



Little endian 



Note that in the word 0x01234567 the high-order byte has hexadecimal value 
0x01„while the4ow-order byte has value 0x67. 

Most Intel-compatible machines operate exclusively in little-endian mode. On 
the other hand, most machines from IBM and Oracle (arising from their acquisi- 


J 













Section 2.1 Information Storage 43 


I Asid^^ Origin of "endian" ^ 

f’Here is (iow Jonathan Swift, writing in 1726* described the history of the controversy between big and 
I little endiahs: 

I '* i’ 

I ... Lilliput and Blefuscu . . . have, as I was going to tell you, been engaged in a most obstinate war 

i *‘' for sbr^and-thirty inoons.past. It began upon the following occasion. It is allowed on all hands, that 
the primitive way of breaking eggs, before we eat them, was upon the larger end; but his present 
s majesty’s grandfather, while he was a boy, going to eat an egg, and breaking,it according to the 
f ^ancient practice, happened to*cut one of his fingerS. Whereupon the empdror his father published 
I an edict, commanding all his subjects, upon great penalties, to break the smaller end of their eggs. 

I Jlie people so highly resented this law.Jhat our histories tell us, there have been six rebellions raised 

I ‘ on that account; wherein one emperor lost his life, and another his crown. These civil commotions 
j were constantly fomented by the monarchs of Blefuscu; and when they were.quelled, the exiles 

always fled for refuge to that empire. It is computed that eleven thousand persons have at several 
I times suffered death, rather than submit to break their eggs at the smaller end. Many hundred 

} 'large volumes have befen^ptiblished upon this controversy: but the books of thfe Big-endians have 

i been long fdrbidden, and the whole party rendered incapable by law of holding employments. 

I (Jonathan Swift. Gulliverls Travels, Benjamin Motte,*1726:) 

I In his day. Swift was satirizing the continued conflicts between England (Lilliput) and France (Blefuscu). 
I Danny Coh,en, an early pioneer in networking protocols, first applied.these terms to refer to byte 
j ordering [24], and the terminology has been .widely adopted. 


tion of Sun Microsystems in 2010) operate in big-endian mode. Note that we said 
“most.” The conventions do not split precisely along corporate boundaries. For 
example, both IBM and Oracle manufacture machines that use Intel-compatible 
processors and hence are little endian. Many recent microprocessor chips are 
bi-endian, meaning that they can be configured to operate as either little- or 
big-endian machines. In practice, however, byte ordering becomes fixed once a 
particular operating system is chosen. For example, ARM microprocessors, used 
in many cell phones, have hardware that can operate in either little- or big-endian 
mode, but the two most common operating systems for these chips—Android 
(from Google) and lOS (from Apple)—operate only in little-endian mode. 

People get surprisingly emotional about which byte ordering is the proper one. 
In fact, the terms “little endian” and “big endian” come from the book Gulliver’s 
Travels by Jonathan Swift, where two warring factions could not agree as to how a 
soft-boiled egg should be opened—by the little end or by the big. Just like the egg 
issue, there isho technological reason to choose one byte ordering convention over 
the other, and hence the arguments degenerate into bickering about sociopolitical 
issues. As long as one of the conventions is selected and adhered to consistently, 
the choice is arbitrary. 

For most application programmers, the byte orderings used by their machines 
are totally invisible; programs compiled for either class of machine give identi¬ 
cal results. At times, however, byte ordering becomes an issue. The first is when 



44 Chapter 2 Representing and Manipulating Information 

binary data are communicated over a network between different machines. A 
common problem is for. data produced by a little-endian machine to be sent to 
a big-endian machine, or vice versa, leading to the bytes within the words being 
in reverse order for the receiving program. To avoid such problems, code written 
for networking applications must follow established conventions for byte order¬ 
ing to make sure the sending machine converts its internal representation to the 
network standard, while the receiving machine converts the network standardlo 
its internal representation. We will see examples of these conversions in Chap¬ 
ter 11. H 

A second case where byte ordering becomes important‘is when looking at 
the byte sequences representing integer data. Thisioccurs often when inspecting 
machine-level programs. As an example, the following line occurs in a. file that 
gives a text representation of the machine-level code for an Intel x86-64 processor: 

4004d3: 01 05 43 Ob 20 00 add %eax,Ox200b43C/.rip) 

This line was generated by a disassembler, a tool thaCdetermines the instruction 
sequence represented by an executable program file. We will learn more about 
disassemblers and how to interpret lines such as this in Chapter 3.-For now, we 
simply note that this line states that the hexadecimal byte sequence 01 05 43 Ob 
20 00 is the byte-level representation of an instruction that adds a word of data 
to the value stored at an address computed by adding 0x200b43 to the 6urrent 
value of the program counter, the address of the next instruction to be executed. 
If we take the final 4 bytes of the sequence 43 Ob 20 00 and write them in reverse 
order, we have 00 20 Ob 43. Dropping the leading 0, we have the value 0x200b43, 
the numeric value written on the right. Having* byte's appear* in reverse order 
is a common occurrence when reading machine-level program representations 
generated for little-endian machines such as this one. The natural way to .write a 
byte sequence is to have the lowest-numbered byte on the left and the highest on 
the right, but this is contrary to the normal way of writing numbers with the most 
significant digit on the left and the least on the right. 

A third case where byte ordering becomes -visible is when* programs are 
written that circumvent the normal type system. In the C language, this* can .be 
done using a cast or a union to allow an object to be referenced according to 
a different'data typefrdm which it was created. Such’boding* tricks are strongly 
discouraged for most application programming, but they can be quite liseful and 
even necessary for* system-level programming. 

Figure 2.4 shows C code-that uses casting to access and print the byte rep¬ 
resentations of different program objects. We use typedef .to define data type 
byte.pointer as a pointer to an object of type unsigned char. Such a byte pointer 
references a sequence of bytes where each byte is considered to be a nonnega¬ 
tive integer. The first routine showj.bytes is given the address of a sequence of 
bytes, indicated by a byte pointer, and a byte count. The byte count is specified as 
having data type size_t;, the preferred data type for expressing the sizes of data 
structures. It prints the individual .bytes in hexadecimal. The C formatting direc¬ 
tive % - 2x indicates that an integer should be printed in hexadecimal with at least 
2 digits. 










Section 2.1 Information Storage 45 


1 ftinclude <stdio.h> 

2 

3 typedef unsigned char *byte_pointer; 

4 

5 void show_b5rtes(byte_pointer start, size_t len) { 

6 int i; 

7 for (i = 0; i < len; i++) 

8 printfC" %.2x", start[i]); 


9 

printf("\n”); 


10 

11 

} 


12 

void show_int(int ,x), { 


13 

show_bytes((byte pointer) &x, 

sizeof(int)); 

14 

} 


15 



16 

void show_float(float x) { 


17 

show_bytes((byte_pointer) &x. 

sizeof (float)'); 

18 

> 


19 

y 


20 

void show_pointer(void *x) { 


21 

show_bytes((byte.pointeb) &x. 

sizeof(void *)); 

22 

> 



Figure 2.4 Code to print the byte representation of program objects. This code 
uses casting to circumvent the type system. Similar functions are easily defined for other 
data types. 


Procedures show_int, show_f loat, and show_pointer derrionstrate how to 
use procedure show_bytes to print the byte representations of C'firogram objects 
of type int, float, and void irrespectively. Observehhat they simply pass sh6w_ 
bytes a pointer &x to their argument x, casting t6e pointer to be of type unsigned 
char *. This cast indicates to the compiler that tfig program shofild consider the 
pointer to be to a'sequence of bytes rather than to an object oPthe original data 
type. This pointer will then be to the lowest byte address occupied by the object. 

These procejiures use the C sizepf operator to dejtprmine the number of bytes 
used by the object. In general, the expression si,zeofiD returns the number,of 
bytes required to store an object of type “T. Using si^eof rather than a fixed yalue 
is one st^p toward vyfitipg code that is pfir^able acrqss ^^ifferent machine,jyp^s. 

We rap the cqde shown in Figure-2.5 op,seyjeral different machines, giving the 
results shown in-Figure 2.6. The following ma^ijine^ were used: 

Linux 32 , Intel lX32 processor running Linux. 

Windows Intel IA32 processor gunning Window^. ^ ( 

Sun Sun Microsystems SPARC processor running Solaris. (These machines 

are now produced by Oracle.) 

Linux 64 Intel x86-64 processor running Linux. 


46 Chapter 2 Representing and Manipulating Information 


----- code/data/show-bytes.c 

1 void test_show_bytes(iiit val) { 

2 int ival = val; 

3 float fval = (float) ival; 

4 int *pval = feivai; 

5 sh.ow_int(ival); 

6 show_float(fval) ; 

7 show_pointer(pval): 

8 > 

--- code/data/show-bytes.c 

Figure 2.5 Byte representation examples. This code prints the* byte representations 
of sample data objects. 

V 


Machine 

Value 

Type 

Bytes (hex) 


Linux 32 

12,345 

iqt 

39 30 bo 00 


Windows 

12,345 

int 

39 30 00 00 


Sun 

12,345 

dnt 

00 00 30 39 


Linux 64 

12,345 

int 

39^30 00 00 


Linux 32 

12,345.0 

float 

00 e4 40 46 


Windows 

42,345.0, 

float 

i00.e4'40 46 

ri 

Sun 

12,345.0' 

float 

46 40*64 00 


Linux 64 

12,345.0 

float 

00 64 40 46 


Linux 32 

aival 

int * 

e4 f 9 ff bf 


Wincjpws 

aival 

int * 

,b4 cc 22 00 


Sun 

aival 

int.* 

ef ff fa Oc 


Lirjux 64 

aival 

int,*'t 

-bS 11 e5 ff ff 7f 00 00 


Figure 2.6 Byte representatiohs, of different data values. Results for int and float 
are identical, except for byte ordering. Pointer values are machine dependent. 

Our argument 12‘3’45 has hexadecimal representation OxOOO‘03039. For the int 
data, we get identical results for all machines, e'ixcept for the byte ordering. In 
particular, wd''can see that the least significant byte value of 0x39 is printed first 
for Linux 32, Windows, and Linux 64, indicating little-endian machines, and last 
for Sun, indicating a big-endian machine. Similarly, the bytes of th'e float data 
are identical, except for the byte Ordering. On the other hand, the iTointer values 
are completely different. The different machine/operating system configurations 
use different conventions for storage allocation. One feature to note is that the 
Linux 32, Windows, and Sun michines use 4-byte addresses, while the Linux 64 
machine uses 8-byte addresses. ^ 












Section 2.1 Information Storage 47 


I New to C? Naming data types with typedef 

I The typedef deciaration in p provides a way of giving a name to a datatype. This can be a great help 
I in improving code readability, since deeply nested type declarations can be difficult to decipher., 

I The syntax for typedef is exactly like that of declaring a variable, except that it uses a type name 
^ rather than a variable name. Thus, the declaration of byte_pointor in Figure 2.4 has the same, form as 
I the declaration of a variable.of type unsigned chai;>. 
f For example, the declaration 

I 

typedef int ♦int.pointer; 
t int.pointer ip; 


defines type int .pointer to be a pointer to an int, and declares a variable ip of this type. Alternatively, 
^ we could declare this variable directly as 

*ip; 


} 


int 


New to C7 FortfiaUed printing with printf 




) 


t The printf’function (along witli its cousins fp'rintf and sprintf) provides a way to print information 
I with considerable cdntrohover the formatting details. The first argument is a format string, while any 
I remaining arguments are values to bfe printed. Within the format string, each character sequence 
i starting with indicates how to format the next argument. Typical examples include 7.d to print a 
decimal integer, Xf to print a floating-point number, and Xc to print a character having the character 
( code given by the argument. 

1 Specifying the formatting^^of fixed-size data types, such as int_32t, is a'bit more involved, aS" ik 
? describedm the aside on page 67. , 

j. * 

VU <41^ w ■:*J VV'#' enSK*-* •• 'f. 


Observe that although the floating-point and the integer data both encode 
the numeric value 12,345, they have very different byte patterns; 0x00003039 
for the integer and Ox4640E400 for floating point. In general, these two formats 
use different encoding schemes. If we expand these hexadecimal patterns into 
binary form and shift them appropriately, we find a sequence of 13 matching bits, 
indicated by a sequence of asterisks, as follows: 

00003039 

OOOOOOOOOOOOOOOOOOllOOOOOOlllOOl 

4640E400 

01000110010000001110010000000000 

This is not coincidental. We will return to this example when we study floating¬ 
point formats. 



48 Chapter 2 Representing and Manipulating Information 




New to C? Pointers,and arrays ‘ « 4 ' % -■ « “* % 

Ip'futiction'shdw^bytes (Fi'gure,2?4), we s*eetJid3:Iose*coiine9^tipp betweeq''point‘Srs altdarraysf'asj\vill 
be discussed in detail in Section "3^8. Weisdenhatithis functipfflias'an argument*start of4ype*byte_ 
pofntei; (which* has been;define4^o b|J pointer-fo.iinai|n^ffchaf),*(!ut'''we*see"the‘ array referpnce 1 
staCrt |i3\>n (in.e 8. Ih' p,*we'fcai(defeferenc^ a ppfntenVyith arrh^notation, and we'fcarf reference'array | 
elements with pointer notation. In this exampje; the'r6ference start [1] indicates lhafwe want to rea*d I 
the'byte*that is*i positions beyond ^e Ibcatibn'pointedtbjby’ptairt, sfe* ' 'p, * t 








I ■’Arnm" Min 




<# 




New .to tt' "Pbihfer'creatibn 'af^d’ de1‘eferen(i!ng'’ 

In lines 13,17, and 21 of Figure 2.4 we see»uses*of two operations that.givp C (and therefore G-fe+) its'l 
distinctive character. The^C “.address off operator creates a pointer.'fehall three fines, the expression | 

&x creates a pointer Iq.the locatioti holding thfe object iridfcated variable x. The type’bf this,pointejs‘ | 
depends on the t ^9 of‘x,'and hencp these three pointers'are^'of typednt float *, and void**', | 
respectively.' (Data type void * is a special kind of pointer with no associated typepnfof mafion.) 4 
The cast operator converts from.one data,typd to’rSibtfi^. Th'fts,‘thd d'a"St‘(l5yte_pcilnteF)'&x 1 
indicates that whateYfer^t^pe|lje,rpintbr &x had before, the. program w^I pow, reference a pointer to, | 
data gf type .uns igned ch^. The casts sKown here dq.nof change^thp ^ctuarpointer; tliey»sijnply birect I 
the compiler,to.fefer to the^data'bqing poinfed |o accofding’to the new,jdata-typ?. 












. *!> t4* ... , .S' ..ij.. I 


s Aside Generating an, A^CIllable jp 

. , ' . . 

You can display a table showing the ASClicharacter code by exeputing the comlnand man afsbil. 






Consider the following three'calls to show_bytes; 
int val - 0x87664321; 

byte_pointer valp = (byte_pointei') &val; 
siiow_bytes(valp, 1); /* A. */ 

show_bytes(valp, 2); /* B. */ 

sliow_bytes(valp, 3); /* C. */ 

Indicate the values that will be printed by each call on a little-endian machine 
and on a big-endian machine: 


A. Little endian:. 

B. Little endian:. 

C. Little endian:. 


Big endian: 
Big endian: 
Big endian:. 















Sectioa2.1 Information Storage 49 



Using show_int and show_f loat, we determine that the integer 3510593 has hexa¬ 
decimal representation 0x00359141, while the floating-point number 3510593.0 
has hexadecimal representation 0x4AB64S04. 


A. Write the binary representations of these two hexadecimal values. 

B. Shift these two strings relative to one another, to maximize the number of 
matching bits. How many bits match? 

C. What parts of the strings do not match? 


2.1.4 Representing Strings 

A string in € is encoded by an array of characters terminated by the null (having 
value 0) character. Each character is represented by some standard encoding, with 
the most common being the ASCII character code. Thus, if we run our routine 
show_bytes with arguments "12345" and 6 (to include the terminating character), 
we get the result 31 32 33 34 35 00. Observe that the ASCII code for decimal digit 
X happens to be 0x3x-, and that the terminating byte has the hex representation 
0x00. This same result would be obtained on any system using ASCII as its' ' 
character code, independent of the byte ordering and word size conventions. As 
a consequence, text data are more platform independent than binary data. 



- - - --------— r — '--'y- -'■■—-I — r'-T ■ ■ ■——™ 

What y^uld be printed as a result of the J^o^lowing call to show.bytps? 


const char *8 = "abcdef"; 
show_bytes((byte_pointer) s, strlents)); 

i 

Note that letters ‘a’hhrough ‘z’ have ASCII codes 0x61 through 0x7A. 


2.1.5 Representing Code 

Consider the following C function: 

1 Int sumCint x, int y) { 

2 refurii x + y; 

3 > 

< 

When conipiled on our sample machines, we generate machine code having 
the following byte representations: 

Linux 32 55 89 e5 8b 45 Oc 03 4^ 08 c9 c3 

Windows 55 89 e5 8b 45 'oc 03 45 08 5d c3 

Sun 81 _c3 eO 08 90 02 po 09 

Linux 64 55 48 89 e5 89 7d f c 89 75 f8 03 45 f c c9 c3 







50 Chapter 2 Representing and Manipulating Information 






¥ I« 




Aside The Unicode.stanclard for text encoding „ 

‘ ■' ■' ^ '■ /•* '*;^* 5’%®” # *« *■ ' » 'fc j 

^ 'ITie ASCII character,set i&*suitabl§'.for.e^codmg'En^rsh:!l^g:u’age4'^^uni^nt|,;ljut.it jdoe? lipt have, | 
j fnuch'in the -way of spigal pHkracters, such^‘as'the'Rejtch^;‘”It=is,)Wholly.uri?uii^d£fpr encoding | 
j documents in languages such as Greek, Russiani’and Cnipe&iGvef the y^rs;M vafiety^pf methods ‘J 
“have been developed to encode texffSr^diffdt^lAanguageS/^ltj^illideiCbiisbrti&h has devised thfi 
most comprehensible" and vttiaely«adce^tp4‘*sI&dard''fori.encoding te}tt.tTiie current^'Unicode standard I 
* “(version 7.0) has a repertoire of over 100)000 characters sup^pftmg.a3viderai|ge'of langi&ges,*iilcl}iding^ i 
^ the ancient languages of Egypt and Babylbn) Tojtheir d^edit),tfe,yjpic^deye9|^cal rejected " 

a proposal td include^a standard Whiting fdcKlingon^ a’fictiobal civilization.from Ihe^television series 
? Sfur Trek.- 

g Thefrase encoding, known as4he “Utaiver|aFCHaracf'dr Set””of yni||t:)de, pfes^a’ 32-bit represent^- ^ 
tion of characters.'‘ljiis y'o'uld seem,to require every strihg^pf text’ to consist of/bytesjer chatacter.* ^ 
Hfowevpr, alternative codings-are posMble,where comniori charac|^rs«re^uire.justl qt 2;. bytes, whiles 
less common ones re^uireanofe. Ir^lrticmlarb theJJJTE^Sltepr'^eptation encodes eacfrcharacter a^ a 
sequence of,bytes, such that the^standard-^ASGII p^aracters.use the stime Sihye;'byle encodings as they 


h'ave in'ASCII^ implying thatall ASCI| byte sequen6es,haVe4be sarne meaning in'UTF^ as:they“do ih'«| 

Asai’ ‘ ^ %. ." “ 


it 


Irhe Jav^ prbgtaimtiing languag^e’pses I5nic6de irf its representations oLsteings.'P'rogram libraries 
are also available for C to support Unicode., 










Here we find that the instruction codings are different. Different machine types 
use different and incompatible instructions and encodings Even identical proces¬ 
sors running different operating systems have differences in their coding conven¬ 
tions and hence are not binary compatible. Binary code is seldom portable across 
different combinations of machine and operating system. 

A fundamental concept of computer systems is that a program, from the 
perspective of the machine, is simply’a sequence of bytes. The machine has no 
information about the original source program, except perhaps some auxihary 
tables maintained to aid in debugging. We will see this more clearly when we study 
machine-level programming in Chapter 3. 

2.1.6 Introduction to Boolean Algebra 

Since binary values are at the core of how computers encode, store, and manipu¬ 
late information, a rich body of mathematical knowledge has evolved around the 
study of the values 0 and 1. This started with the work of George Boole (1815- 
1864) around 1850 and thus is known as Boolean algebra. Boole observed that by 
encoding logic values true and false as binary values 1 and 0, he could formulate 
an algebra that captures the basic principles, of logical reasoning. 

The simplest Boolean algebra is defined over the two-element set {0,1). 
Figure 2.7 defines several operations in this algebra. Our symbols for representing 
these operations are chosen to match those 'used by the *C bit-level operations. 




















Section 2.1 Information Storage 51 


- 


& 

0 

1 

1 

0 

1 


0 

1 

0 

1 

0 

0 

0 

0 

0 

1 

0 

0 

1 

1 

0 

1 

0 

1 

1 

1 

1 

1 

1 

0 


Figure 2.7 Operations of Boolean algebra. Binary values 1 and 0 encode logic values 
TRUE and FALSE, while operations I, and “ encode logical operations not, and, or, 
and EXCLUSIVE-OR, respectively. 

as will be discussed later. The Boolean operation ~ corresponds to the logical 
operation not, denoted by the symbol That is, we say that -‘P is true when 
P is not true, and vice versa. Correspondingly, ~p equals 1 when p equals 0, and 
vice versa. Boolean operation & corresponds to the logical operation and, denoted 
by the symbol A. We say that P A Q holds when both P is true and Q is true. 
Correspondingly, pkg equals 1 only when p = 1 and ^ = 1. Boolean operation 
I corresponds to the logical operation or, denoted by the symbol v. We say that 
P V Q holds when either P is true or Q is true. Correspondingly, p \ q equals 
1 when either p = 1 or ^ = 1. Boolean operation " corresponds to the logical 
operation exclusive-or, denoted by the symbol ©. We say that P ®Q holds when 
either P is true or Q is true, but not both. Correspondingly, p" q equals 1 when 
either p = 1 and ^ = 0, or p = 0 and q~\. 

Claude Shannon (1916-2001), who later founded the field of information 
theory, first made the connection between Boolean algebra and digital logic. In 
his 1937 master’s thesis, he showed that Boolean algebra could be applied to the 
design and analysis of networks of electromechanical relays. Although computer 
technology has advanced considerably since. Boolean algebra still plays a central 
role in the design and analysis of digital systems. 

We can extend the four Boolean operations to also operate on bit vectors, 
strings of zeros and ones of some fixed length w. We define the operations over bit 
vectors according to their applications to the matching elements of the arguments. 
Let a and b denote the bit vectors [a„_i, au,_ 2 ,..., oq] and b„,_ 2 , .... *o]. 
respectively. We define a & f> to also be a bit vector of length w, where the ith 
element equals a,- & fc,-, for 0 < j < it>. The operations I, and ~ are extended to 
bit vectors in a similar fashion. 

As examples, consider the case where w~4, and with arguments a = [0110] 
and b = [1100]. Then the four operations a kb, a \ b, a" b, and -b yield 

0110 0110 0110 

i£ 1^ I 1100 - 1100 ~ 1100 

'0100 1110 1010 om 




em 2. 



Fill in the following table showing the results of evaluating Boolean operations on 
bit vectors. 


I 





52 Chapter 2 Representing and Manipulating information 


Web Aside DATA:BOOL> More‘on Boolean algebra and Boolean rings 

The Boolean operations I, &, and - operating oiiijbit vectors of length w form ^^Boolehn algebra, 
for any integer if > 0. The ^simplest iS|fhe case^where w~l Md,there^are‘j^st two elements^ b^t for | 
the more general case there are 2“" bit vectors'of length w. Boolean algetira has’many of jie same ; 
properfies as»arithmetic over integers.'Fo^ example, ju§t as multiplication distributes over, addition, ] 
written a ■ (h + c) = (a ’b) 4- (a ‘cl.Booleanbperationfe distributes over K written a & (b ! cX—(a &h) 1 I 
(a & c). fn addition, However. Boolean operation i distributes over &, and so we can write a ] (b&c) = ; 
(a 1 h) & (a I c), whereas we cannot say that a-+ (b ■ c) —’(a +‘b\ - (a 4- c) holds foY all integers. | 

.When we consider 6 'perations'‘, &, and^-“operating on “bit vectors of length we get a differenf | 

mathematical form, knojvij as a Boolean rmg. Boolean rings have many’properties in bbfnmon with, | 
integer arithmetic. For example, onaproperty of integer arithmetic is^thaT evjsfy value x'has ah additive 1 
inverse -x, such that x 4- r-x‘= 0. A similar property holds for* Boolean rings, wkdre ~ is the “|ddition*’ | 
operation, but in this case each dementis its pwn additive,itiverse. That is, a " n — 0 fpf any value a, I 
where we use OJiere to represent a bit “vector bf allj^eros, W? can “see this holds foKsihgle bits, since | 
0 ~ 0 = 1~1 = 0 , and it extends to bit vdctorg as wel^ This property holds'even when.we rearrange terms I 
and combine tljem in a different order, anrf so Xa~ b)~ a~ b. This property leads lo som^mteresting I 
results and clever tricks, as ,v?e will ixpfdrp in Pro.blem 2,}0. ,, ! 








Operation Result 

a [ 01101001 ] 

b [ 01010101 ] 


~a 

~b 

a kb 
a I b 
a ~ b 


One useful application of bit vectors is to represent finite sets. We can encode 

any subset A c [0,1,..., in — 1} with a bit vector [flu,_i. a^, aj]. where a,- = 1 if 

and only iff ^ A. For example, recalling that we write on the left and oq on'the 

right, bit vector a ~ [01101001] encodes the set A = {0, 3, 5, 6 ), while bit vector b = 
[01010101] encodes the set B = [0, 2, 4, 6 }. With this way of encoding sets, Boolean 
operations I and & correspond to set union and intersection, respectively, and - 
corresponds to set complement. Continuing our earlier example, the operation 
a&b yields bit vector [01000001], while A n B = {0, 6 ]. 

We will see the encoding of sets by bit vectors in a number of practical 
applications. For example, in Chapter 8 , we will see that there are a number of 
different signals that can interrupt the execution of a program. ;We can selectively 
enable or disable different signals by specifying a bit-vector mask, where a 1 in 
bit position i indicates that signal i is enabled and a 0 indicates that it is disabled. 
Thus, the mask represents the set of enabled signals. 












Section 2.1 Information Storage 




Computers generate color pictures on ^ video screen or liqu^i^ crystal display 
by mixing three different colors of ligh^ red, green, and blue. Imagine a simple 
scheme, with three different lights, each of which can be tjimed on or^off, project- 
ing^onto a glass screen: 


\ 



A 


n 




We canjhen create eight different colors basedpn the absence (0) onpresence 
(1) of hght sources R,G, and B: 

RGB Color 

0 0 0 Black 

0 0 1 Blue 

0 10 Green 

0 1 1 Cyan 

10 0 Red 

10 1 Magenta 

1 1 0' Yellow 

1 1 1 White 

t 

Each of these colors can be represented ak a bit vector of length 3, and we can 
apply Booleamoperations'to.theih. 

A. The complement of a color is formed by turning off the lights that are on and 
turning on the lights that are oft What would oe the complement of each of 
the eight colors listed above? 

B. Describe the effect of applying boolean operations on the following colors; 

Blue I Green =_ 

Yellow k Cyan =_ 

Red - Magenta =_ 


I 

L 




54 Chapter 2 Representing and Manipulating Information 

2.1.7 Bit-Level Operations in C 

One useful feature of C is that it supports bitwise Boolean operations. In fact, the 
symbols we have us'ed for the Boolean operations are exactly those used by C: 
1 for OR, & for AND, - for not, and ~ for exclusive-or. These can be applied to 
any “integral” data type, including all of those listed in Figure 2.3. Here are some 
examples of expression evaluation for data type char: 

C expression Binary expression Binary result Hexadecimal result 


-0x41 

-[0100 0001] 

[10111110] 

OxBE 

-0x00 

-[0000 0000] 

[11111111] 

OxFF 

0x69 & 0x55 

[01101001] & [01010101] 

[01000001] 

0x41 

0x69 1 0x55 

[01101001] 1 [01010101] 

[01111101] 

0x7D 


As our examples show, the best way to determine the effect of a bit-level ex¬ 
pression is to expand the hexadecimal arguments to their binary representations, 
perform the operations in binary, and then convert back to hexadecimal. 

As an application of the property that a '- a = 0 for any bit vector a, consider the 
following program: ' 

1 void inplace_swapCint *x, int *y) { 

2 *y = ~ *y; /* Step 1 */ 

3 x-x = *x ~ ♦y; /* Step 2 */ ' 

4 ♦y = X«X *y; /* Step 3 */ 

5 y 

As the name implies, we claim that the effect of this procedure is to swap 
the values stored at the locations denoted by pointer variables x and y. Note 
that unlike the usual technique for swapping two values, we do not need a third 
location to temporarily store one value while we are moving the other. There is 
no performance advantage to this way of swapping; it is merely an intellectual 
amusement. 

Starting with values a and b in the locations pointed to by x and y, respectively, 
fiU in the table that follows, giving the values stored at the two locations after each 
step of the procedure. Use the properties of " to show that the desired effect is 
achieved. Recall that every element is its own additive inverse (that is, a “ a = 0). 

♦X 


Step 

Initially 
Step 1 


a 


HL 

b 





















Section 2.1 Information Storage 55 



Amed with the function inplace_swap from Problem 2.10, yoii*decide to write 
code that will reverse the elements of an array by swapping elements from opposite 
ends of the array, working toward the middle. 


You arrive at the following function; 

,1 void •reverse_eirray(int a[], diit cnj) { 

2 int first, last; 

3 for (first* t 0, last >= cnt-1; 

•4 first <= last; 

Si first++,last —) 

6 inplace_swap(&a[first] , &a[l,ast]); 

7 } 

When you apply your function to an array containing elements 1, 2, 3, and 4, 
you find the array now has, as exjpected, elements 4,3,'2, and i. When you try it 
on an array with elements 1,2^ 3,4, ahd 5, however, you are surprised to see that 
the array now has elements 5,4, 0| 2, and 1. In fact, you discover that tl\e code 
always works correctly on arrays of even length, but it sets the middle element to 
0 whenever the array has odd length. 

C ‘ » 

A. For an array of odd length cnt = 2k + 1, what are the values of variables 
first and last in the final iteration of function revQrse_array? 

B. Why does this call to function inplace_swap set the array element to 0? 

C. What simple modificatidn to the code for reverse_array would'eliminate 
this problem? 

One common use of bit-level operations is to implement masking operations, 
where a mask is a bit pattern that indicates a selected set of bits within a word. As 
an example, the mask OxFF (having ones for the least-significant 8 bits) indicates 
the low-order byte of a word. The bit-level opferatiom x & OxFF yields a value 
consisting of the least significant byte of x, but with all other bytes set tV-0. For 
example, with x = 0x89ABCDEF, the expression would yield OxOOOOOOEF. The 
expression -0 will yield a mask of all ones, regardless of the size of the dati 
representation. The same mask can be written OxFFFFFFFF when data type int is 
32 bits, but it would not be as portable. 



Write C expressions, in terms of variable x, for the following values.* Your code 
should work for any word'Size ly > 8 . For-reference, we show the result of evalu¬ 
ating the expressions for x = 0x87654321, with w = 32. 


A. The least significant byte of x, with all other bits set to 0. | 0 x 0000002 l/ , 

B. All but the least significant byte-of x complemented, with the least significant 
byte left unchanged. [0x789ABC2l] 





56 Chapter 2 Representing and Manipulating Information 


C. The least significant byte set to all ones, and all other bytes of x left .un¬ 
changed. [0x8765435'F] 



The Digital Equipment VAX computer was a very popular machine frolm the late 
1970s until the late 1980s. Rather than instructions for Boolean operations and 
and OR, it had in structions bis (bit set) and bic (bit clear). Both instructions take 
a data word x and a mask word m. They generate a result z consisting of the bits of 
X modified according to the bits of m. With bis, the modification involves setting 
z to 1 a.t each bit position where m is 1. With bic, the modification involves setting 
z to 0 at each bit position where m is 1. 

To see how these operations relate to the C bit-level operations, assume we 
have functions bis and bic implementing the bit set and bit clear operations, md 
that we want to use these to implement functions computing bitwise operations I 
and without using any other C operations. Fill in^the mssing code below. Hint: 
Write expressions for the operations bis and bic. ^ 

/* Declarations of functions implementing operations bis and bic */ 
int bis(int x, int m); 

int bic(int x, int m); 

/* Compute x|y using only calls to inunctions bis and bic ,*/ 
int bool_or(int x, int y) { 

int result = _; 

return result; 

} 

t n .1 ■ 

/* Compute x“y using o^liy cplj,s to functions bis and bic */ , 

int bool_xor(int ^x; inf y)- { 

int* result =_; 

return result; 

>• . 


2.1.8 Logical Operations in C 

C also provides <a set of logical operators [ I, &&, and !, which coprespond to the 
OR, AND, and NOT operations of logic. These can easily be confused with the bit- 
level operations, but their behavior is quite different. The logical operations tre^t 
any nonzero argument as representing true and argument 0 as representing false. 
They return eith'et 1 or 0, iddicatih^ a result of either true of false; respectively. 
Here are some examples of expression evaluation: 















Section 2,1 Information Storage 


Expression 

Result 

!0x41 

0x00 

!0x00 

0x01 

!!0x41 

0x01 

0x69 && 0x55 

0x01 

0x69 1 1 0x55 

0x01 


I 


I 

I 

I 

I 

I 

I 

I 

I 

I 


I 

I 

I 

I 

I 

I 


Observe that a bitwise operation will have behavior matching that of its logical 
counterpart only in the special case in-which the arguments are restricted to 0 
or 1 . 

A second important distinction between the logical operators *&&’ and ‘ 11 ’ 
versus their bit-level counterparts and ‘ I ’ is that the logical operators do not 
evaluate their second argument if the result of the expression can be determined 
by evaluating the first argument. Thus, for example, the,expression a && 5/a will 
never cause a division by zero, and the expression p && +p++ will never'cause the 
dereferencing of a null pointer. 


t t 


Suppose that x and y have b^te values 0 x 66 and 0x39, respectively. HU in the 
following table indicating the byte values of thb different C expressions: 


Expression 'Value Expression Value '< 


X & y ^ 

X 1 y 


X && y 

X 1 j y 


-x 1 -y 

X &■ !y 

-^ 

tx 1 1 !y 
X && ~y 


f 



Using only bit-level and logical operations, write a C expression that is equivalent 
to X == y. In other words, it will return 1 when x and y are equal and 0 otherwise. 


2.1.9 .Shift»Operations inC- 

. f f' , • I • ,j 

C also provides a se^pf j/zjj^,opjerat^Qn§ for shiftji^ bit patterns to the left and,to 
the right. For an operand x having bit representation x^- 2 > • • •,• J^o]> C 
expression x « k yields a value with bit representation •••,xq, 

0,..., 0], That is, x is shifted k bits to the left, dropping off the k most significant 
bits and filling the right end with k zeros. The shift amount should be a value 
between 0 arid iu — 1 . Shift operations associate from left to right, so x ■« j << k 
is equivalent to (x « j) « k. 

There is a corresponding right shift operation, written in C as x » k, but it has 
a slightly subtle behavior. Generally, machines support two forms of right shift: 








58 Chapter 2 Representing and Manipulating Information 


Logical. A logical right shift fills the left end with k zeros, giving a result 

[0 . 0 , x^^2> ■ ■ ■ 

Arithmetic. An arithmetic right shift fills the left end with k repetitions of the 
most significant bit, giving a result ..., x^_i, x^_ 2 > • • • -«*]• 

This convention might seem peculiar, but as we will see, it is useful for 
operating on signed integer data. 

As-examples, the following.table showsrihe effect of applying the.different 
shift operations to two different values'jof.an'8-bit argument x: 


Operation 
Argument x 
X « 4' 

X » 4'(logical) 

X » 4 (arithmetic) 


Value } ^ Value 2 

[01100011] [ld()10101] 

[miwooo] [Qimooo] 
[00000110] {oCdo'mi] 
[00000110] [inimi] 


The italicized digits indicate the values that fill the right (left shift) or left (right 
shift) ends. Observe that all but one entry involves filling with zeros. The exception 
is the case of shifting [10010101] right arithmetically Since its most significant bit 
is 1, this will be used as tljp fill value. 

The C standards do not precisely define which type of right shift should be 
used with signed numbers^—either arithmetic-or logical shifts may be used. This 
unfortunately means that any code assuming one form or the other will potentially 
encounter portability problems. In practice^ however, almost all compiler/machine 
combinations use arithmetic right shifts for 'signed data, and many programmers 
assume this to be the case. For unsigned data, on the other hand, right shifts must 
be logical. 

In contrast to C, Java has a precise definition of how right shifts should be 
performed. The expression x » k shifts x arithmetically by k positions, while 
X »> k shifts it logically. 


lEcadtis 


Fill in the table below showing the effects of the different shift operations on single¬ 
byte quantities. The best way to think about shift operations is to work with binar5* 
representations. Convert the initial values to binary, perform the shifts, and then 
c^)nVert back^o hexadeciinal. Eaich of the drisw'ers sh'o'uld be 8 binary digits or 2 


hexadecimal digits. 


1 


Logical 

Arithmetic 

X X « 3 

X » 2 

X » 2 

Hex Binary Binary Hex 

Binary Hex 

Biliary Hex 


0xC3 


















Section 2.2 Integer Representations 59 




I Aside Shifting by Moxlarge,yalu^s^pf^ ' '''" 

j For a data type consisting of a; 4s, whdt sh6\ild be t4 effect of sf ifting by some value A > «;9 -por 
example, what should be;he effect of computing thef^lfowing expressions, assuming data-type int has 

a ly — jA. ’ .V 


into Ival* = 0xFEDCBA98^,« 32; ** 

I int aval = OxFEDCBAIs >> 36; ® 

I ipisigned uval = 0xFEDCBi^9& !>'> 4 ol '' 

? ' A ' 

Thet staftcfefds carerulVa:void stating'^hat shbuld b;^ done in sticK a case.Dn many machines, the 
t «^onsider only the lower I5g2 w Bits of the shift amount when shifting a uf^bit value, an^ 

I Kshift Amount is computed as* A^od ii;..Fof e^a'mple, with 32, the above tWee shifts would 
I be computed a^rf they were b^ai^nts^a 8, t^I^ctiyel^ giving results 

llilal 0xFEfcCBA98 , . 

I ".aYar .OxFFEDCBAS"' . 4 , , “ 

uva» OxOOFEDQBA, *• , j, ^ ^ 

I TOs behaviorjs notguarantbSd |of ffprograrns: Ivjiyevef.-and s'p'shift amounts should be keptless than " 
I the word size.s « . p , . ^ 

1 f I, — specifically requires that shift amounts s^uld 4 commuted in the modular 

I fashion we have shown. I*' % .f, 

s‘ , i i * ^ 

J... fe .i. . . s,, , 


I Aside Operator prececienc'e issues,with shift op^fatidns^ 

I 4ght b?jeplpfihg to wlite’theexpression“l<<'2*'3<<4! intending it to’mean (1<<2) + (3«4) How- 

I T!^l’ C fce former expressm^j^ equivalent to 1 «• (^33 << 4, since addition (and'subtraction) have 
ftian shifts. The left-to-right associativity ruld then causes this to be parenthesized' 
I « 4, giving value 512, rather ithp the intendedlSZ.i 

I C eXpre|si4s,js a cdmmpnsource of program errors, and often 

l^these^aib difficuft^ spot^jr uispfection:^|h itf doubt! piit^n pdre^pSeS! 

ra *•• 1 ' **; 1 It rfe S ^ ...S 1-^ . & 


2.2 Integer Representations 

In this section, we describe two different ways bits can be used to encode integers— 
one that can only represent nonnegative numbers, and one that can represent 
negative, zero, and positive numbers. We will see later that they are strongly 
related both in their mathematical properties and their machine-level implemen¬ 
tations. We also investigate the effect of expanding or shrinking an encoded integer 
to fit a representation with a different length. 

Figure 2.8 lists the mathematical terminology we introduce to precisely de¬ 
fine and characterize how computers encode and operate on integer data. This 




60 Chapter 2 .Representing and Manipulating Information 


Symbol 

Type 

Meaning 

Page 

B2T^ 

Function 

Binary to two’s complement 

64 

B2U^ 

Function 

Binary to unsigned 

62 

U2B^ 

Function 

Unsigned to binary 

64 

U2T„ 

Function 

Unsigned to two’s complement 

71 

T2B„ 

Function 

Two’s complement to binary 

65 

T2U^ 

Function 

TVvo’s complement to unsigned 

71 

TMin^ 

Constant 

Minimum two’s-complement value 

65 

TMax^ 

Constant 

Alaximiun two’s-complement value 

65 

UMaXy) 

Constant 

Maximum unsigned value 

63 

W 

Operation 

TVo’s-complement addition 

90 

w 

Operation 

Unsigned addition 

85 

uf 

Operation 

Tvo’s-complement multiplication 

97 

w 

Operation 

Unsigned multiplication 

96 

w 

Operation 

Two’s-complement negation 

95 

w 

Operation 

Unsigned negation 

89 


Figure 2.8 Terminology for integer data and arithmetic operations. The subscript 
w denotes the number of bits in the data representation. The "Page" column indicates 
the page on which the term is defined. 


I! terminology will be introduced over the course of the presentation. The figure is 

included here as a reference. 

! 2.2.1 Integral Data Types 

C supports a variety of integral^data types—ones that represent finite ranges of 
1 integers. These are shown in Figures 2.9 and 2.10, along with the ranges of values 

“ they can have for “typical” 32- and 64-bit programs. Each type can specify a 

* size with keyword char, short, long, as well as an indication of whether the 

represented numbers are all nonnegative (declared as unsigned), or possibly 
negative (the default.) As we saw in Figure 2.3, the number of bytes allocated for 
the different sizes varies according to whether the pTograrfi is compilfed for 32 or 
64 bits. Based on the byte allocations, the different sizes allow different ranges of 
, valued to be represented. The only machine-dependent range'indicated is for size 

designator long. Most 64-bit programs use' an 8-byte representation, giving a much 
wilier range of values than the 4-byte representation used with 32-bit programs. 

One important feature“t6hofe in Figures'2.9 and 2.10 is that the ranges are not 
symmetric—the 'range of negative numbers,extends one further iJian the range of 
positive numbe;rs. We will see why this happens' when we consider how negative 
numbers are represented. ^, 


III 










Section 2.2 Integer Representations 61 


C data type 

Minimum 

Maximum 

[signed] char 

-128 

127 

unsigned char 

0 

255 

short 

-32,768 

32,767 

unsigned short 

0 

65,535 

int 

-2,147,483,648 

2,147,483,647 

unsigned 

0 

4,294,967,295 

long 

-2,147,483,648 

2,147,483,647 

unsigned long 

0 

4,294,967,295 

int32_t 

-2,147,483,648 

2,147,483,647 

uint32_t 

0 

4,294,967,295 

int64_t 

-9,223,372,036,854,775,808 

9,223,372,036,854,775,807 

uint64_t 

0 

18,446,744.073,709,551,615 


Figure 2.9 Typical ranges for C integral data types for 32-bit programs. 


C data type 

Minimum 

Maximum 

[signed] char 

-128 

127 

unsigned char 

0 

255 

short 

-32,768 

32,767 

unsigned short 

0 

65,535 

int 

-2,147,483,648 

2,147,483,647 

unsigned 

0 

4,294,967,295'. 

’ .1 i * 

long 

-9,223,372,036,854,775,808 

9,223,372,036,S54,775,807 

unsigned long 

0 

18,446,744,073,709,551,615 

int32_t 

-2,147,483,648 

2,147,483,647 

uint32_t 

0 

4,294,967,295 

int64_t 

-9,223,372,036,854,775,808 

9,223^372^03,6,854,775^07 

uint64_t 

0 

18,446,744,073,709,551,615 


Figure 2.>10 Typical ranges for C integral data types for 64-bit programs. 


The C standards define minimum ranges of values' that ^ach data type must 
be able to represent' As shown in Figure 2.11, their ranges are thfe same or smaller 
than the typical implementations shown in Figures 2.9 and 2.10. In particular, 
with the exception of the fixed-size data types, we see that they require only a 


62 Chapter 2 Representing and Manipulating Information 


New to C? 


^gnedar1a>DSlgne^^^n^^3gfe4^le,Q^+;^^c|ja^^ ^ ^ 

.s,mnortsirfted(thddifault)an4weai^1^rs.Jajca supports oiily^^nu^ 


Both C and C++ suppprt 


C data type 
[signed]'char 
unsigned char 

short 

unsigneji short 


unsigned 

long 

unsigned long 

int32_t 

uint32_t 

int64_t 

uint64_t 


Minimum 


-32,767 

0 

-32,767 

0 

-2,147,483,647 

0 

-2,147,483,648 

0 

-9,223,372,036,854,775,808 

0 


Maximum 


32,767 

65,535 

32,767 

65,535 

2,147,483,647 

4,294,967,295 

< 

2,147,483,647 

4,294,967,295 

9,223,372,036,854,775,807 

18,446,744,073,709,5.51,615 


Figure 2.11 Guaranteed ranges for C integral data types. The C standards require 
that the data types have at least these ranges of values. 

> 

symmetric range of positive and negative numbers. We also see that data type int 
cSid b-e implemented with 2-byte numbers, although this is mostly a throwback 
to the days of*16-bit machines. We also seeHhat size long can be implemented 
wU 4 byS numbers, and it typically is for 32-bit programs. THe fixed-s.e data 
types guarantee that the ranges of values will be exactly those ^ven by the ^ical 
numbers of Figure 2.9, including the asymmetry between negative and positive. 

2.2.2 Unsigned Encodings 

Let us consider an integer data type of m bits. We write a bit vector as either x to 
^note the entire vector, or as ■ - - ^o] to denote the ti'dividual bits 

within the vector. Treating 5 as a number written in bmary notation we obtam Ae 
unsigned interpretation of it. In this encodmg, each bit x,- has value 0 or 1, with th 
latter case indicting that value 2' should be included as part of the numeric Value. 
We can express-this interpretation as a function B2U^ (for “binary to unsigned, 

length w): * i 










Section 2.2 Integer Representations 63 


Figure 2.12 

Unsigned number 

“2^ = 8 

1 

: 

examples for w = 4. 

2==4 


When bit i in the binary 

2’ =2 

F’.'D 

representation has value 1, 

2"=1 

( 

S' 

it contributes 2' to the 
value. 

3 1 2 3 4 i 


[0001f 

U" ' ' ' 

. 

. [0101] 


' 

[10J1] 



[1111] 


PRINCIPLE: Definition of unsigned encoding 

For vector x = [xi„_i, x^_ 2 , . 

- - -. -to]: 



-1 —4 I I 


H—I—f- 


lU-l 

B2U^(x) = ^ XjT 

i=0 


( 2 . 1 ) 


In this eguation, the notation = me^S'that the left-hand side is defined to be 
equal to the right-hand side. The function maps strings ofizeros and,ones 
of length w to nonnegative integers. As examples, Figure 2.12 shows the mapping, 
given by B2U, from bit vectors to integers for the following cases: 

B2U^{[m\]) = 0-23 + 0-22-l-0-2i + 1.2‘^ = 04-0-t-0-|-l = 1 

52f/4([0101]) = 0.2^ -Hi- 22-^0-21 + 1-2° = 0 + 4 + 0 + 1 = 5 

B 2 t / 4 ([ 1011 ]) = 1-23 + 0-22 + ;.21 + 1 - 2 ° = 8 + 0 + 2 + 1 = 11 

B2U^{[11\1]) =- 1-23 + 1-22 + 1.21 + 1-2° = 8 + 4 + 2 + 1 =. 15 

( 2 . 2 ) 

In the figure, we represent each bit position i by a rightward^pomting blue bar of 
length 2‘. The numeric value associated with a bit .vector then equals,the sum qf 
the lengths of the bars for which the cofresponding bit values are 1. 

Let us consicier the range of values that can be represented using w bits. The 
least value is ^ven by bit vector [00 - ■ - 0] having integer value 0, and the greatest 
value 1s given by bit vector [11 - • *1] having integer value UMax^ = 2‘ = 

2“' - 1. Using the 4-bit case as an example, we have UMax^ = 52 U4 ([1111]) = 
2^* - 1 = 15. Thus, the function 52can be defined as a mapping B2U^: {0,1}® 

{0,..., UMax^}. ” 

The urfsignecFbinary representation has the imp(2>rtant property that'-'every 
number between O'and 2’" - 1 has a unique encoding as a ly-ljit value. For example; 




64 Chapter 2 Representing and Manipulating Information 


there is only one representation of decimal value 11 as an unsigned 4-bit number— 
namely, [1011]. We highlight this as a mathematical principle, which we first state 
and then explain. 

PRINCIPLE: Uniqueness of unsigned encoding 

Function 52(7^ is a bijection. ■ 

The mathematical term bijection refers to a function / that goes two ways: 
it maps a value x to a value y where y = fix), but it can also operate in reverse, 
since for every y, there is a unique value x such'that f(x) = y. This is given by 
the inverse function f~^, where, for our example, x — f~^(y). The function B2U,^ 
maps each bit vector of length u; to a unique number between 0 and 2*" — 1, and 
it has an inverse, which we call (for “unsigned to binary”), that maps each 

number in the range 0 to 2“^ - 1 to a unique pattern of w bits. 

2.2.3 Two's-Complement Encodings 

For many applications, we wish to represent negative values as well. The most com¬ 
mon computer representation of signed numbers is known as two’s-complement 
form. This is defined by interpreting the most significant bit of the word to have 
negative weight. We express this interpretation as a function B2r^ (for “binary 
to two’s complement” length lu): 

PRINCIPLE:- Definition of two’s-complement encoding 
For vector x = Xu,_2 ,..., xq]: 

w—2 

B2T,(x) = H- YL (2.3) 

■ 

The most significant bit x„,_i is also called the sign bit^. IK “weight” is -2“'“’-, 
the negation of its weight in an unsigned representation. When the sign bit is set 
to 1, the represented value is negative, and when set to 0, the value is nonnegative. 
As examples. Figure 2.13 shows the mapping, given by B2T, from bit vectors to 
integers for the,fpllowing. cases: 


527’4([0D01]) = 

-0 

2 ^ -H o' 

• 2^ + 0 

■2^ + 1 

.•20 

O+O+O+l 

( j 

= 1 

52r4([0101]) = 

-0 

•23 + 1 

• 22 + 0 

■2^ + 1 

•2° = 

0 + 4/0 + 1 


52r4([1011]) 

-1 

•23 + 0 

■ 22 + 1- 

■2^ + 1- 

•2° = 

-8+6+2+1 

= -5 

52r4([iiii]) = 

-1 

•23 + 1; 

r,22 + 1 • 

• 21 + 1 • 

2*3 = 

-8 + 4 + 2 + 1 

= -1 








(2,4) 




In the figure, we indicate that the sign bit has, negative weight by showing it as 
a leftward-pointing gray bar. The numeric value associated with 9 bit'vector is 
then given by the combination of the, possible leftward-pointing gray, bar and the 
rightward-pointing blue bars. 













Section 2.2 Integer Representations 


65 


Figure 2.13 
Two's-complement 
number examples for 
w = 4. Bit 3 serves as a 
sign bit; when set to 1, it 
contributes -2^ = -8 to 
the value. This weighting 
is shown as a leftward¬ 
pointing gray bar. 


<■.. J-2^ = -8 

2^-4 
2 ^ =2 
2“=1 ^ 

- 8 - 7 - 6 - 5 - 4 - 3 - 2-1 0 1 2 3 4 5 6 7 8 
I—I— I —I—I— i— I —I—I—I—t—I—I—|—I— I —I 



We see that the bit patterns are identical for Figures 2.12 and 2.13 (as well as 
for Equations 2.2 and 2.4), but the values 'differ when the most significant bit is 1, 
since in one case it has weight -|-8, and in the other case it has weight —8. 

Let us consider the range of values that can be represented as a ui-bit two’s- 
complement number. The least representable value is given by bit vector [10 ■ • • 0] 
(set the bit with negative weight but clear all others), having integer value 
TMin^ = -2“'~L Tlie greatest value is given by bit vector [01 • • ■ 1] (clear the bit 
with negative weight but set all others), having integer value TMax^ = YX=o “ 
2“'-! — 1. Using the 4-bit case as an example, we have FMm4 = 52r4([1000]) = 
-2^ = -8 and TMax^ = B27’4([0111]) = 2^ -f 2^ -f 2° = 4 -h 2 -f 1 = 7. 

We can see that B2T^ is a mapping of bit patterns of length w to numbers be¬ 
tween TMin^ and TMax^, written as 827^,: {0, I}*" -> [TMin^, .... TMax^]. As 
we saw with the unsigned representation, every number within the representable 
range has a unique encoding as a ly-bit two’s-complement number. TTiis leads to 
a principle for two’s-complement numbers similar to that for unsigned numbers: 

PRINCIPLE: Uniqueness of two’s-complement encoding 

Function B2T^ is a bijection. I 

We define function 728^, (for “two’s complement to binary”) to be the inverse 
of B27^. That is, for a number x, such that TAfiny, <x < 7Max^^, 72B^(x) is the 
(unique) ly-bit pattern that encodes x. 


Assuming ly = 4, we can assign a numeric value to each possible hexadecimal 
digit, assuming either an unsigned or a two’s-complement interpretation. Fill in 
the following table according to these interpretations by writing out the nonzero 
powers of 2 in the summations shown in Equations 2.1 and 2.3: 



66 Chapter 29‘Representing and Manipulating Information 


Hexadecimal Binary 

OxE [1110] 

0x0_ 

0x5 _ 

0x8 _ 

OxD _ 

OxF _ 


B2U^{T) B2T^{x) 

23 + 22 + 2 ^ = 14 - 22+22 + 2 i =-2 


Figure 2.14 shows the bit patterns and numeric values for several important 
numbers for different word sizes. The first three give the ranges of representable 
integers in terms of the values of UMax^, TMin^, and TMax^. We will refer 
to these three special values often in the ensuing discussion. We will drop the 
subscript to .and refer to the values UMax,^TMin, and TMax when w can be inferred 
from context or^is not central to the discussion. 

A fejv points are worth highlighting about these numbers. First, as observed 
in Figures 2.9 and 2.10, the two’s-complem.ent range is.asygnmetric; \TlfIin\ = 
I TMaxi + 1; th^t is, there is no positive counterpart tb' TMin. As we shall see, this 
leads to some peculiar properties.pf two’s-complement arithmetic andean be the 
spurce of subtle program bugs. This as^metiy arises because half the bit patterns 
(those wi& the sign'bit. set to 1) represent negative numbers, while half (those 
with the sign bit set to 0) represent nonhegative numbers. Since 0 is nonnegative, 
this means that it can'represent one less positive number than negative. Seconcl, 
the maximum unsigned value is just over twice the'maximum two’s-complement 
value; UMax — 2TMax'+ 1. All of the^bit patterns'that denote negative numbfers in 
two’s-complement notation becomfe positive values in an unsigned representation. 





Word size w 


Value 

8 

16 

32 

64 

UMax^ 

OxFF 

255 

OxFFFF 

65,535 

OxFFFFFFFF 

4,294,967,295 

oxFff;fffffffffffff 

18,446,744,073,709,551,615 


0x80 

-128 

0x8000 

-32,768 

0x80000000 

-2,147,483,648, 

Ojf^OOOOOOOOOOpOOOO 

-9^223,372,036,854,775,808 

TMaXy, 

0x7F 

127 

0x7FFF 

32,767 

0x7FFFFFFF '' 
2,147,483,647 

0x7FFFFFFFFFFFFFFF 

9,223,372,036,854,775,807 

-1 

OxFF 

OxFFFF 

OxFFFFFFFF 

OxFFFFFFFFFFFFFFF'F 

0 

0x00 

0x0000 

0x00000000 

oxoo6ooooooooooooo 

.. .J — 


Figure'2.14 Important numbers. Both numferit values and-hexadecima! representa¬ 
tions are shown. 















Section 2.2 Integer Representations 67 


Aside More on fixed-size integer types. # 

For some pro]grams, it is essential that data types be encoded using representations with specific sizes. 
For example, when writing {Programs to enable a raacWne to communicate over the Internet according 
to a standard protocol, it is important to have-data typ^s cofiipatible with those specified tiy the protbc'ol. 

* We have seen that some C data types, especially long, have different ranges on different machines, 

* and in fact the G standards only specify the minimum ranges for any data type, not the exact ranges. 
Although we can choose data types that will be compatible twith standard representations on most 

I machines, there is no guarantee of portability. 

; ^e have already encountered the.32- and 64-bit versions of fixed-size integer types (Figure 2.3); 
^ thdy are part of a larger .class of data types. The IS© C99 standard introduces this clas| of integer ty^es 
in the file' stdint. h. This file defines a set of data types with declarations of'the fofm ihtand 
uintlV^t, specifying IV-bit signed andaujsigned integers, for flifferent values of N. The exact values of 
'■ N are impleinentation dependent,hut.most compilers allow values of 8,16, 32, and 64. Thus, we can 
unambiguously declare*an utfsigned 16-6ir variable by giving it type uintl6_t, and a signed variable 
. of 32'bits as int32 t. 

Along with the^e data types are a set of ipacrps defining the minimum and niaximum valuqis for 
eachyalue of N,. These have hames'of'the. formTNTA^'_MIN, INTA_MAX, and UIHTJV_MAX.*. * 

Formatted printing with fixed-width types requires use of macros that expand into format’strings 
I in a system-dependent mapner. So, for example, the values of variables' x and j of type int32_t and 
5 fiint64_t can l?e printed by the'followihg call to printf: 

printf("x'„7,°4;:«PRId32'‘^, y = •/," PRIu64''-»’'\n''', x‘, yX;* 

When compiled as'a 647bit pfogrdih,.maci'b PRIdS2 expands to the stfing "d", whife PR|u64f expands 
j to the pair of strings "1" "u". When the “C preprocessor encounters a sequeif*^ of kring cbnstants 
separated only by spaces (or other whitespace characters), it concatenates them., together. Thus, the 
I above call to printf becomes 

’printf ("x f'yid, y*= %lu\n'',' x'i y)V'^ 

^ Dkng the macjoa.ensures that a.cbrreptvfqrmat^trmg ^11 be generatedpegardlesSjOf how the code is 


Figure 2.14 also shows the representations of constants —1 and 0. Note that —1 
has the same bit representation as UMax —a string of all ones. Numeric value 0 is 
represented as a string of all zeros in both representations. 

The C standards do not require signed integers to be represented in two’s- 
complement form, but nearly all machines do so. Programmers who are concerned 
with maximizing portability across all possible machines should not assume any 
particular range of representable values, beyond the ranges indicated in Figure 
111, nor should they assume any particular representation of signed numbers. 
On the other hand, many programs are written assuming a two’s-complement 
representation of signed numbers, and the “typical” ranges shown in Figures 2.9 
and 2.10, and these programs are portable across a broad range of machines 
and compilers. The file <liinits.h> in the C library defines a set of constants 










68 Chapter 2 Representing and Manipulating Information 


Aiide Alternative representations of signed numbers • 

There are two other standarcf.represeritatiqns for signectiiumbersu ^ 

^ f , .. . '■ 4. i, 

‘ complpmentf ^This is,the s;^|g;as two‘’s cqijjpieirient, excep|,that,the.mostsignificant bit has 

„f .weighty-(2“"V.” I)‘r3thert}i^-2“'“Y ' ' 

'fS'' s .„~2 

s ,-=o *» 

V • ^ ^ ' ■ • * '* '4 

Sign magnitude. Thq^paoU significant bit is a «|ign bit that determines whether^the rfemainlng bits 
S,fipiil£i.be,give|l negative or positive Weight; ^ 


n } 




B2S^(x)’= 


w—2 

1=0 . 


Bqth of these representations have the curious property that there hre two different epcodin'gs of the 
nifniber 0, For both representations, [00 • • • 0] isjn'terpreted as’^O. TOe value —O'can be, represented 
in sign-magnitude'fornCas 110^- • ■ Oj^a^id p oqes’ complement as [11 • • • 1]. Although machifies based 
ompnes'’-complemeht representations whre built in the past, -almost all modern niachines use two’s 
complement. We will see^that signrmagnitude eneddihg is used with floating-point numhiers. 

Note the different po§jtion of apostrophe^- two’s complement versus ones’ complement, tlie term 
“tjvo’s complement” arises from the fact that fon-nonnegative x we compute a'lu-bit representation 
of — jc 2*“ — X (a single two.} The term.“ones’ complement” cgnies frpnj 'the property»that we can 
compute -X in this notation as [:j,ll ■ IJ.-^ x (multiple ones).' 






delimiting the ranges of the different integer data types for the particular machine 
on which the compiler is running. For example, it defines constants INT_MAX, INT_ 
MIN, and UINT_MAX describing the ranges of signed and unsigned integers. For a 
two’s-complement machine in which data type int has w bits, these constants 
correspond to the values of TMax^, TMin^, and UMax^^. 

The Java standard is quite specific about integer data type ranges and repre¬ 
sentations. It requires a two’s-complement representation with the exact ranges 
shown for the 64-bit case (Figure 2.10). In Java, the single-byte data type is called 
hyte instead of char. These detailed requirements are intended to enable Java 
programs to behave identically regardless of the machines or operating systems 
running them. 

To get a better understanding of the two’s-complement representation, con¬ 
sider the following code example: 

1 short X = 12345; 

2 short mx = -x; 

3 

4 shoM_bytesC(hyte_pointer) iix,' sizeof (short)) ; 

5 show_bytesC(byte.pointer) &mx, sizeof(short)); 















Section 2.2 Integer Representations 69 


Weight 


12,345 


-12,345 


53,191 

Bit 

Value 

Bit 

Value 

Bit 

Value 

1 

1 

1 

1 

1 

1 

1 

2 

0 

0 

1 

2 

1 

2 

4 

0 

0 

1 

4 

1 

4 

8 

1 

8 

0 

0 

0 

0 

16 

1 

16 

0 

0 

0 

0 

32 

1 

32 

0 

0 

0 

0 

64 

0 

0 

1 

64 

1 

64 

128 

0 

0 

1 

128 

1 

128 

256 

0 

0 

1 

256 

1 

256 

512 

0 

0 

1 

512 

1 

512 

1,024 

0 

0 

1 

1,024 

1 

1,024 

2,048 

0 

0 

1 

• 2,048 

1 

2,048 

4.096 

1 

4,096 

0 

0 

0 

0 

8,192 

1 

8,192 

0 

0 

0 

0 

i6,384 

0 

0 

1 

16,384 

1. 

16,384 

±32,768 

0 

0 

1 

-32,768 

1 

32,768 

Totil 


12,345 


-12,345 


53,191 


Figure 2,15 Two's-complement representations of 12,345 and -12,345, and 
unsigned representation of 53,191. Note that the latter two have identical bit 
representations. 


When run on a big-endian machine, this code prints 30 39 and cf c7, indi¬ 
cating that X has hexadecimal representation 0x3039, while mx has hexadeci¬ 
mal representation 0xCFC7. Expanding these into binary, we get bit patterns 
[0011000000111001] for x and [1100111111000111] for mx. As Figure 2.15 shows. 
Equation 2.3 yields values 12,345 and —12,345 for these two bit patterns. 

In Chapter 3, we will look at listings generated by a disassembler, a program that 
converts an executable program file back to a more readable ASCII form. These 
files contain many hexadecimal numbers, typically representing values in two’s- 
complement form. Being able to recognize these numbers and understand their 
significance (for example, whether they are negative or positive) is an important 
skill. 

For the lines labeled A-I (on the right) in the following listing, convert the 
hexadecimal values (in 32-bit two’s-complement form) shown to the right of the 
instruction names (sub, mov, and add) into their decimal equivalents: 







70 Chapter 2 Representing 'and Manipulating Information 


4004d0 

48 

81 

ec 

eO 

02 

00 

00 

sub 

$0x2e0,%rsp 

A. 

4004d7 

48 

8b 

44 

24 

a8 



mov 

-0x58(%rsp),Xrax 

B. 

4004dc 

48 

03 

47 

28 




add 

0x28(%rdi),%rax 

C. 

4004e0 

48 

89 

44 

24 

do 



mov 

Xrax, -0x30 C/.rsp) 

D. 

4004e5 

48 

8b 

44 

24 

78 



mov 

0x78(%rsp),%rax 

E. 

4004ea 

48 

89 

87 

88 

00 

00 

00 

mov 

%rax, 0x88 (7,rdi) 

F. 

4004fl 

48 

8b 

84 

24 

f8 

01 

00 

mov 

Oxlf8(Xrsp),Xrax 

G. 

4004f8 

00 










4004f9 

48 

03 

44 

24 

08 



add 

0x8 (%rsp) ,7.rax 


4004fe 

48 

89 

84 

24 

cO 

00 

00 

mov 

%rax,OxcO(%rsp) 

H. 

400505 

00 










400506 

48 

8b 

44 

d4 

b8 



mov 

-0x48(‘4rsp,%rdx,8) , '/.rax 

I. 


j 2,2.4 Conversions between Signed and Unsigned 

I C allows casting between different numeric data types. For example, suppose 

I • variable x is declared as int and u as unsigned. The expression (unsigned) x 

I converts the value of x to an unsigned value, and (int) u converts the value of u 

to a signed integer. What should be the effect of casting signed value to unsigned, 
or vice versa? From a mathematical perspective, one can imagine several different 
conventions. Clearly, we want to preserve any value that can be represented in 

( both forms. On the other hand, converting a negative value to unsigned might yield 

zero. Coilverting an unsigned value that is too large to be represented in two’s- 
complement form might yield TMax. For most implementations of C, however, 
the answer to this question is based on a bit-level perspective, rather than on a 
' numeric one. 

For example, consider the following code: 

1 short int , v, = -12345; 

2 unsigned short-uv = (unsigned short) y; 

t 3 printf("v = %d, uv = */.u\n",, v,_ uv); 

' When run on a two’^-c'dlfnplement machine, it generates the following output: 

t * 4 

^ V « -12345, uv = 53191 

I What we see here is that the effect of casting is to keep the bit values identical 

but change how these bits are interpreted. We saw in Figure 2.15 that the 16-bit 
twd’s-complenient'representation‘of —12,345 is identical to the T6-bit unsigned 
representatiqh of* 53,191. Castjng’ from short' to unsigned short changed the 
numeric value, but riot the bit -representation. ' 

Similarly, cdnsider’thfe followihg code: 

f 

1 unsigned u = 4294967295u; /* UMax */ • 

2 -int tu ='-(int) 'u; 


I 






















Section ~2.2 Integer Representations 71 


3 printfC'u = %u, tu = %d\ii'', u, tu); 

When run on a two’s-complement machine, it generates the following output: 
u = 4294967295, tu = -1 

We can see from Figure 2.14 that, for a 32-bit word size, the bit patterns represent¬ 
ing 4,294’967,295 (UMaxy 2 ) in unsigned form and —1 in two’s-coraplement form 
are identical. In casting from unsigned to int, the underlying bit representation 
stays the same. 

This is a general rule for how most C implementations handle conversions 
between'Signed and unsigned numbers with the same word size—the numeric 
values-might cha'nge, but the bit patterns do not. Let, us capture this idea in 
a more mathematical form. We defined functions U2B^ and T2B^ that map 
numbers to their bit representations in either unsigned or two’s-complement form. 
That is, given an integer x in the ranfee'O < a: <*UMaXyj, the function UlBf^ix) 
gives the unique ui-bit unsigned representation^of x. Similarly, whence is.in the 
range TMin^j <x'< TMax^, the function T2B^{x) gives the unique u)-bit two’s- 
complement representation oh a;. 

Now define the function T2U^ as T2^J^„{x) = B2U^(T2B^(x)). This function 
takes a number between TMin^ and TMaXyj and yields a number between 0 and 
UMax^, where the two numbers have identical bit representations, except that 
the argument has a two’s-complement representation while the result is unsigned. 
Similarly, for a: between 0 and UMax^, the funetion U2T^, defined as U2T^{x) = 
B2T^(U2B^(xy^;yields the number having the same two’s-complement represen¬ 
tation as the unsigned representation of x. 

Pursuing our'earlier examples, we see from*Figure 2.15 that 7212,345) 
= 53,191, and that 1727^^6(53,191) = —12,34^. That is, the 16-bit pattern writtep in 
hexadecimal as OxCFCT is both the-t^o’s-con^plement representation of -12,345 
and the unsigned representation of 53,19,1. Note also that 12,345,-1-53,191 = 
65,536 = 2^^. This property generalizes to a relationship between the two nu¬ 
meric values (two’s complement and unsigned) represented by a given bit pat¬ 
tern. Similarly, from Figure 2.14, we see that T2(732(-l) = 4,294,967,295, and 
(72r32(4,294,967,295) = —1. That is, UMax has the same Bit representation in un¬ 
signed form as does —1 in two’s-complement form. We can also see the relationship 
between these two numbers: 1 -f UMax^ = 2“'. * 

We see, then, that function 721/ describes the 'conversion of a two’s- 
complement number to its unsigned counterpart, while U2T converts in the op¬ 
posite direction. Th'ese 'describe the effect of casting between these data tyjjes in 
mosf G implementations. 

i \ 



Using the table you filled in when solving Problem 2.17, fill in the following table 
describing the function T2U^: 








72 Chapter 2 Representing and Manipulating Information 
;c T2U4ix) 


The relationship we have seen, via several examples, between the two’s- 
complement and unsigned values for a given bit pattern can.be expressed as a 
property of the function T2U: 

PRINCIPLE: Conversion from two’s complement to unsigned 
F6rx'such that TMin]^, <x< TMaX^\ 


T2U^ix) = 


x + i<0 


For example, we saw that 72£/i6(—12,345) = —12,345 + 2^^ — 53,191, andalso 
that 72f/u,<-l) = “1 + 2“’= t/MnXu,. _ , _ , .a- 

This property can be derived by comparing Equations 2.1 and 2.3. 

D E R1VATIO N: Conversion from two’s complement to unsigned 
Comparing Eqtations'i.l and 2.3, we can see that for bit pattern x, if we compute 
the difference - B 2 T,„(x),lhe weighted sums for bifs from 0 foie -2vyiR 

caiicel each’dther, leaving a value B2l]^{x) B2'F\„{x) = x„j_i(2'" '— -2 >'— 

This givefe a relationship B2U^{x) =B2T^{x) +Xu,_i2“'. We-therefore 

have 

B2U^(T2B^{xy) = T2U^(x) = x + x,„-_i2"’ (2.6) 

In a two’s-complement representation of x, bit Xu,_i determines whether or not x 
is negative, giving thertwo cases of Equation 2.5. * 

As examples. Figure 2.16 compares how functions B2 U and B2T‘^s^gn values 
to bit patterns for w =4. For the two’s-complement case, the most significant bit 
serves as the sign bit, which we diagram as a leftward-pointing gray bar. For the 
unsigned case, this bit has positive weight, which we show as a rightward-pointmg 
black bar. In going from two’s complement to unsigned, the most significant bit 
changes its weight from —8 to +8.. As a consequence, the values that are nega¬ 
tive in a two’s-complement-representation increase by 2"* = 16 with air unsigned 
representation. Thus, —5 becomes +11, and —1 becomes +15. 















Section 2.2 Integer Representations 73 


Figure 2.16 
Comparing unsigned 
and two's-complement 
representations for le = 4. 
The weight of the most 
significant bit is -8 for 
two's complement and +8 
for unsigned, yielding a net 
difference of 16. 



Figure 2.17 

Conversion from two's 
complement to unsigned. 
Function T2U converts 
negative numbers to large 
positive numbers. 



Figure 2.17 illustrates the general behavior of function T2U. As it shows, when 
mapping a signed number to its unsigned counterpart, negative numbers are con¬ 
verted to large positive pumbers, while nonnegative numbers remain unchanged. 

i ( 




•v'rntrr-srrt 


Explain how Equation 2.5 applies to the entries in the table you generated when 
splving Problem 2.19. ., 


Going in the other direction, we can state the relationship between an un¬ 
signed number u and its signed counterpart U2T^iu): 


PRINCIPLE: Unsigned to two’sTComplement conversion 
For u such that 0 < « < UMax^: 


U2T^iu)- 


u, 

«- 2 “', 


u < TMaXy^ , 
M > TMax^t, 



74 Chapter^ Representing and Manipulating Information 


Figure 2.18 
Conversion from 
unsigned to two's 
complement. Function 
U2T converts numbers 
greater than 2'"“^ - 1 to 
negative values. 


Unsigned 



Two’s 

complement 


This principle can be justified as follows: 

DERIVATION: Unsigned to two’s-complement conversion 

Let u = U2B^{ju,). This bit vector will also be the two’s-complement representation 
of U2Tyj{u). Equations 2.1 and 2.3 can be combined to give 

t/2r„(«) = -«^_i2“+M ,(2.8) 

In the unsigned representation of w, bit determines whether or not m is greater 

than TMax^ = 2"'"^ — f, giving the two cases of Equation 2.7. ■ 

The behavior of function ^U2T is illustrated in Figure 2.18. For small 
(< TMax^) numbers, the conversion from unsigned to signed preserves the nu¬ 
meric value. Large (> TMax^) numbers are converted to negative values. 

To summarize, we considered the effects of converting in both directions 
between unsigned and two’s-complemeht representations. For values x ifi the 
range 0 < x < TMax^y, we have T2U^ix^ = x. and. U2T^{x) ^iX.-'That is, num¬ 
bers in this range have identical unsigned and two’s-complement representations. 
For values outside of this range, the conversions either add or subtract 2*". For 
example, we have 72f/u,(-l) = -1-1-2“' = UMax^—the negative number clos¬ 
est to zero maps to the largest unsigned number. At the other extreme, one 
can see‘th^t T2U^iTMinJ -H 2*" ^ 2“’-i = TMca^ ^ l^^\he most neg¬ 

ative number maps to an unsigned number just outside the range of^pdsitivd 
two’s-complement numbers. Using the example of Figure 2.15, we can see that 
r2t/i6(-12,345) = 65,536 -p -12,345 = 53,191. 

2.2.5 Signed versus Unsigned in C 

As indicated in Figures 2.9 and 2.10, C’supports both signed and unsigned arith¬ 
metic for all of its integer data types. Although the C standard does not spec¬ 
ify a particular representation of signed numbers, almost all machines use two’s 
complement. Generally, most numbers are signed by default. For example, when 
declaring a constant such as* 12345 or 0xlA2B, the value is considered signed. 
Adding character ‘U’ or ‘u’ as a suffix creates an unsigned constant; for example, 
12345U or 0xlA2Bu. 






















Section 2.2 Integer Representations 75 

C allows conversion between unsigned and signed. Although the C standard 
does not specify precisely how this conversion should be made, most systems 
follow the rule that the underljnng bit representation does not change. This rule has 
the effect of applying the function U2 when converting from unsigned to signed, 
and T2U^ when converting from signed to unsigned, where w is the number of 
bits for the data type. 

Conversions can happen due to explicit casting, such as in the following code: 

1 int tx, ty; 

2 unsigned ux, uy; 

3 

4 tx = (int) ux; 

5 uy = (unsigned) ty; 

Alternatively, they can happen implicitly‘when an expression of one type is as¬ 
signed to a variable of another, as in the following code: 

1 int tx, ty; 

2 unsigned ux, uy; 

3 

4 tx = ux; /* Cast to slgned^'i/ 

5 uy = ty; /* Cast to unsigned */ 

‘When printing numeric values with printf, the directives %d, %u, and ’/.x 
are used to print a number as a signed’ decimal, an unsigned decimal, and in 
hexadecimal format, respectively. Note that printf does not make use of any 
type information, and so it is possible to print a value of type int with directive 
7,u and a value of type unsigned with directive %d. For example, consider the 
following code: 

1 iiit X = -1; 

2 unsigned u 2147483648; /* 2 to the 31st */ 

3 

4 printf("x = %u = 7.d\n", x, x); 

5 printf C'u = 7.U = 7,d\n", u, u); 

When compiled as a 32-bit program, it prints the following: 

X = 4294967295 = -1 
u = 2147483648 = -2147483648 

In both cases, printf prints the word first as if it represented an unsigned number 
and second as if it represented a signed number. We can see the conversion 
routines in action: 72(732(^1) = UMaxj 2 = 2 ^ 2-1 and 172132(2'^^}'= 2^^ - 2^2 = 

-2^^ = TMin32- 

Some possibly nonintuitive behavior arises due to C’s handling of expres¬ 
sions containing, combinations of signed andf unsigned quantities. Wken ah op¬ 
eration IS performed where one operand is signed and the other is unsigned, C 
impUcitly casts the signed argument' to unsigned and performs’ the operations 


76 Chapter 2 Representing and Manipulating Information 


Expression 


Evaluation 

0 

== 

OU 

Unsigned 

i 

-1 

< 

0 

Signed 

1 

-1 

< 

OU 

Unsigned 

.0* 

2147483647 

> 

-2147483647-1 

Signed 

1 

2147483647U 

<> 

-2147483647-1. 

.Unsigned 

0* 

2147483647 

> 

(int) 2147483648U 

Signed 

1* 

-1 

> 

-2 

Signed 

1 

(unsigned) -1 

> 

-2 

Unsigned 

1 


Figure 2.19 Effects of C promotion rules. Nonintuitive cases are marked by When 
either operand of a comparison is unsigned, the other operand is implicitly cast to 
unsigned. See Web Aside data.-tmin for why we write TMin^ii as -2,147,483,647-1. 


assuming the numbers are nonnegative. As we will see, this convention makes 
little difference for standard arithmetic operations, but it leaJls to nonintuitive 
results for relational operators such as < and >. figure 2.19 shows some sample 
relational expressions and their resulting evaluations, when data type int has a 
32-bit two’s-complement representation. Consider the comparison -1 < OU. Since 
the second operand is unsigned, the first one is implicitly cast to unsigned, and 
hence the expression is equivalent to the comparison 4294967295U < OU (recall 
that 72 f7(/,(—!) =jUMax^), which of course is false. The other cases can be under¬ 
stood by similar analyses. i> 


Assuming the expressions are evaluated when executing a 32-bit program on a ma¬ 
chine that uses two’s-complement arithmetic, fill in the following table describing 
the effect of casting arid relational operations, in-the style of Figure 2.19: 


Expression TVpe Evaluation 

-2147483647-1 == 2147483648U__ 

-2147483647-1 < 2147483647 ^ _ 

-2147483647-lU < 2147483647 __ 

-2147483647-1<-2147483647 _ _ 

-2147483647-lU < -2147483647 _ _ 


2.2.^ Expanding the Bit ^epresentation.-of a Number 

One common operation is to convert between integers having different word sizes 

while retainirig'the same numeric value. Of course, this may hot be possible when 

the destination data type is too small to represent the" desired value. Converting 

from a smaller to a larger'data type, however, should always be possible. 

> f r * • 




















Section 2.2 Integer Representations 77 






j Web Aside p;^:T3VltN^:^itlpg77ti^^ 

I In Figufe.^.19 an'din.’ProbIem 2fel,-we,careffilly '^rdte tlie value'crTM/^^l-Tis -2s 147,483 647^1. i!Vhy» ’ 
a ^rite it as either -2,147,483^64*8 or,0x8ppp0000?»Lpoking“at»the Cheadepfile'limits*?ih, 

* wesee thatthey use-a-similaniheihod’as wehayd“to.}vrite^p|firt 3 |,ahd|H/^ 32 : ' „ ’ 

I /♦ Minim^‘aiid ma^amum'yalues a., f^^gnrd« lnt.'» .cad Hold. «i ♦/?’ 

I tfdefjne -INT^MAX *2147483647 ^ ^ 4 | ^ 

I Iltdenne*INTfMra -i Js * , S'* 

f .Tf i'''" ,, ^ 'a® ,’’' s 

5 ‘Unfortuijately^a'curio® inferaetion,between'tHd,As^ine o^|he.t^6!s-,compIqineht.rdpi'dsenta-, 

* [“^1 ‘’f 9 abyay. Although understanding 

this issdeirequiffs uA.to delve.int(ione'Ofetfie murkieiicorhers' df the G'language standards, it will help” 


«ie» ^ si^y'SW/q*'. 


I fci' -‘'o- -1—r*- I—w^w tJu^AViwixvvrxiiu-ia VA kXAQ iali^Ua 




%* 


To convert an unsigned number to a larger data type, we can simply add 
leading zeros to the representation; this operation is known as zero extension, 
expressed by the following principle: ® 

PRINCIPLE: Expansion of an unsigned number by zero extension 

Define bit vectors m = u^_ 2 , .... mq] of width in and «' = [0.0, 

^w -2 .Mq] of width in', where in' > in. Then BlU^iu) = B2U^{u'). ■ 

This principle can be seen to follow directly from the definition of the unsigned 
encoding, given by Equation 2.1. 

For converting a two’s-complement number to a larger data type, the rule 
is to perform a sign extension, adding copies of the most significant bit to the 
representation, expressed by the following principle. We show the sign bit Xu,_i in 
blue to highlight its role in sign extension. 


PRINCIPLE: Expansion of a two’s-complement number by sign extension 


Define bit vectors x = [x„,_i, • • ■, ^o] of width w and x' = [x,„_i,..., x„,_i 

Xu,_i, x ,„_2 .xq] of width lu', where ui' > w. Then B2T,^{x) = B2T,^{x'). I 


As an example, consider the following code; 


1 

2 

3 

4 

5 

6 

7 

8 
9 

10 


short sx = -12345; 
unsigned short usx = sx; 
int X = sx; 
unsigned ux = usx; 


/* -12345 */ 
/* 53191 */ 
/* -12345 */ 
/* 53191 */ 


printfC'sx = %d:\t'', sx); 
show_bytes((byte_pointer) &sx, sizeof(short)); 
printfC'usx = 7.u:\t", usx); 

show_bytes((byte_pointer) &usx, sizeof(unsigned short)); 
printfC'x = %d:\t", x) ; 



78 Chapter 2 Representing and Manipulating Information 



I 

i 

! 


n shoM_bytes((byte_poiiiter) &x, sizeof (int)^ ; 

12 printfC'ux = ux); 

\3 show_bytes((byte_pointer) &ux, sizeof(unsigned)); 


When run as a 3Z-bit program on a big-endian machine that uses a two’s- 
complement representation, this code prints the output 


sx = -12345: 
USX = 53191: 
X = -12345; 
ux = 53191: 


cf c7 
cf c7 

if if cf c7 
00 00 cf c7 


We see that, although the two’s-complement representation of -12,345 and the 
unsigned representation of 53,191 are identical for a 16-bit word size, they dif¬ 
fer for a 32-bit word size. In particular, —12,345 has hexadecimal representation 
0xFFFFCFC7, while 53,191 has hexadecimal representation OxOOOOCFC7. The for¬ 
mer h^s been sign extended—16 copies of the most significant bit 1, having hexar 
deci^l representation OxFFFF, have been added as leading bits. The latter has 
been extended with 16 leading zeros, having hexadecimal representation 0x0000. 

As an illustration, Figure 2.20 shows the'result of expanding from word size 
«; = 3 to UJ = 4 by sign extensioii. Bit vector [101] represents the value —4 -p 1 = —3. 
Applying sign extension givesrbit vector [1101] representing the value —8 -t- 4 -I- 
1 = — 3. We can see that, for w = 4, the combined value of the two most significant 
bits, —8 4- 4 = -4, matches the value of the sign bit for u> = 3. Simil,arly, bit vectors 
[111] and [1111] both represent the value -1. 

With this as intuition, we can now show that sign extension preserves the value 
of a two’s-complement number. 


Figure 2.20 
Examples of sign 
extension from w = 3 
to u; = 4, For u; = 4, the 
combined weight of the 
upper 2 bits is -8 -i- 4 = -4, 
matching that of the sign 
bit for w — 3. 



i 




m 















,Section 2.^ h-Integer^Representations 


DERIVATIONrjExpansion Qf a two’s-complementnumber by sign extension 
Let u; + £ What we' want to pr6ve'is that ” 

(f 

^■2£«;+/:([^-1, . . . , Xui-l, X^_i, Xy,_2, . . . , Xq]) = B27'u,([Xu,_,, X,„_2 .Xnl) 

•v '' 

k times 

The proof follows by induction on k. That is, if we can prove that sign extending 
by 1 bit preserves the numeric value, then this .property will- hold when sign 
extending by an arbitrary number of bits. Thus, the task reduces to proving that 

x^_i, x^_2 ,..., xq]) = B2T^([xyj_i, x^_2, ..., xq]) 
Expanding the left-hand expression with Equation 2.3 gives the following: 

r 

10-1 

E27’y,^.^{[Xjy_J, X„j_J, Xy,_2, . . . , X^q]) = - X^_]2^ -t- 

/=0 

!«—2 

= -x,_i2- + x,,_i2-i +J3 x,.2' 

. -i=0 

if *-2 

=--^>o-i(2‘"-2«'-l)-f-^x,2' 

1=0 

iu-2 • 

1=0 

([xyj_j, X„,_2.^o]) 

The key property we exploit is that 2“^ - 2“-! = 2'"~\ Thus, the combined effect 
of adding'a bit of sleight ^2*" and of converting the'bit having weighf -2“'“^”to be 
one with w^ight^*""^ is to preserve the original numeric value. ■ 



Show that each of the f8llbwing bif Vectors is-a twh’s-complertient representation 
of -5 by applying Equation 2.3: 


A. [1011] 

B. [11011] 

C. [111011] 

Observe that the second and third bit vectors can be derived from the first by sign 
extension. 



80 Chapter 2 Representing and Manipulating Information 


One point woirth making Is that the relative order of conversion' from one 
data size to another and between unsigned pnd signed can affect the behavior of 
a program. Consider the following code: 

1 short sx = -12345; /* -12345 */ 

2 unsigned uy = sx; /* Mystery! */ 

3 

4 printfC'uy = uy); 

5 show_byt^s((byte_pointer) &uy, sizeof(unsigned)); 

When run on a big-endian machine, this code causes the following output to be 
printed: 

uy = 4294954951: ff ff cf c7 

This shows that, when converting from short to unsigned, the program first 
changes the size and then the type. That is, (unsigned) sx is equivalent to 
(unsigned) (int) sx, evaluating to 4,2^4,954,951, not (unsigned) (unsigned 
short) sx, which evaluates to 53,191. Indeed, this convention is required by the 
C standards. 






Consider the following C functions: 


int fiinl (unsigned word) { 

return (int) ((word « 24) » 24); 

} 


int fun2(unsigned word) { 

return ((int) word « 24) >V 24; 

> 

Assume these are ^executed as a 32-bit^rogram on a m,achmf tha,t uses two’s- 
complement arithmetic. Assume also that right shjfts pf signed values are pep; 
formed arithmetically, while right shifts of unsigned values are performed logically. 

A. Fill in the following table showing the effect of these functions for several 
example arguments. You wULfinS it more-convenient to w6rk with a hexa- 

ii deqimal fepresentation.-Jjist rejnember that hex digits 8 through F have, their 
most significant bits equal to 1. 

w funl(w) fun2(w) 

0x00000076 _ _ 

0x87654321 _ _ 

0x00000pC9 _ _ 

0xEDCBA987 __ 

B. Describe in words the useful computation each of these fvmctions performs. 






















Section 2.2 Integer Representations 81 


2.2.7 Truncating Numbers 

Suppose that, rather than extending a value with extra bits, we redufe tbe number 
of bits representing a number. This occurs, for example, in the following code: 

1 int X = 53191; 

2 short sx = (short) x; /*’-12345 */ 

3 inty = kx; ' -i 2345 *'/ 

Casting x to be? short will truncate a 32-bit int to a 16-bit short. As we saw 
before, this 16-bit pattern' is’the twb’s-complement representation of —12,345. 
When casting this back to int, sign extension will set the high-drder 16 bits to 
ones, yielding the 32-bit two’s-comple,ment'representation of —12,345. 

When truncating a m-fc'it number x = [x^_'{:%_ 2 , ., ., .I'o] to ^/fc-bit‘number, 
we drop the highrorder w-k bits, giving a bit vector x' = Xk_ 2 , , Xq]. 

Truncating a number can alter its value—a form of overflow. For an unsized 
number, we can readily characterize the numeric value that will result. 


PRINCIPLE: Truncation of an imsigned number 

Let X pe the^bit vectoi; x^_ 2 , ..., xq], an^i let x' be the result of,truncating 

to k bits: x' = [x^_i, x *_2 .xq]. Let x = B2lJy^{x) and x' = Then 

X =xmod2*. j 

The intuition behind this principle is simply that all of the bits that were 
truncated have weights of the, form’2', where i > k, and therefore each of these 
weights reduces to zero under the modulus operation. This is formalized by the 
following derivation: 


bERIVATIC^N: Huncation of hn unsigned number 
Applying the modulus operation to Equation 2.1 yields 


x^)_ 2 , . 1 .., xq]) mod 2^ 


'UJ-l 


Li=0 

^x,-2' 


Li=0 


mod^* 
mod 2* 


k-l 

= E»/2' 

i=0 

B2Uii{[xi^_i, Xj^^2t ■ • • t xq ]) 

In this derivation, we make use of the property that 2' mod 2* = 0 for any i > k. 

■ 

A similar property holds for truncating a two’s-complerhent number, except 
that it then converts the most significant bit into a sign bit: 


82 Chapter 2 Representing and Manipulating Information 


If 



! 


! 


s 

\ 


t 


PRINCIPLE: Truncation of a two’s-complement number 

Let X be tl;iej^it vqctor x„,_ 2 , - • ■, J:o]> and let (x be the result p^t/uncafing 
it to k bits: x' = ^i_2> • • • > ^ol- ^2ru,(x),and x' = BlTh^lx'). ThQn 

x' = U2Tkix mod 2*). ■ 

In this formulation, x mod 2^ will be a number between 0 ?ind 2* - 1. Ajjplying 
function f/2Tt to it will have the effect of converting the most significant bit Xk_i 
from having .weight to having weight —2*“^. We can see this with the example 
of converting value x = 53,191'from int fo short. Sippe 2^® = 65,536 > we have 
X mod 2^®.= r..But when we convert this niimber,to a 16-bit two’s-complement 
number, we get jc^-=:'53;191 — 65,536 = —12,345. i 

DERI VAT I p N: Truncation of a’ two’s-complement number 

Using a similar argument to the one we used for truncation of an unsigned number 

shows that 

)) .V- .. 

2> ■ ' * 9 ^ol) 2 = B2U^k—2'> • • * > -^oJ) 

That is, X mod 2* can be represented by an unsigned number having bit-level rep- 

resentatidri'[4_i, .xo].,Converting this to a two’s-complement numbhl 

gives r' = f/2 Tjt (x mod 2*). ■ 

Summarizing, the effect of truncation for unsigned numbers is 

B2Uk{[xk-i, Xk-2> ■ • • - -^o]) = B2U^i[x^_k, x^_2\ f • •, “od 2* (2.9) 

i’ J, ^ 

while the effect for two’s-complement numbers is 

B2Tki\xk-l^ Xk-i^ • • •» ^o]) = U2Tk(B2U^i[xy,_i^x^_2, . • -, xo])^mod 2^) (2.10) 








"ST* 


mAmJ 


Suppose we truncate a 4-bit value (represented by hex digits 0 through F) to a 3- 
bit value (representeci as hex digits 0 through 7.) Fill in the table below showing 
the effect of this truncation for some cases, in terms of the unsigned and two’s- 
complement interpretations of those bit patterns. 

'* I 

Hex Unsigned Two’s complement 

Original Truncated Original Truncated Original Truncated 


0 

2 

9 

B 

F 


0 

2 

1 

3 

Y 


0 

2 

9 

11 

15 


0 

2 

-7 

-5 

-1 


Explain how Equations 2.9 and 2.10 apply to these cases. 

























Section 2.2 Integer Representations 83 


2.2.8 Advice on Signed .versus Unsigned 


As we have seen, the implicit casting of signed to unsigned leads to some non- 
intuitive behavior. Nonintuitive features often lead to program bugs, and ones 
involving the nuances of implicit casting can be especially difficult to see. Since the 
casting takes place without any clear indication in the code, programmers often 
overlook its effects. 

The following two practice problems illustrate some of the subtle errors that 
can arise due to implicit casting and the unsigned data type. 










Consider the following code that attempts to sum the elements of an array a, where 
the number of elements is given by parameter length: 


1 /* WARNING: This is buggy code •/ 

2 float sum^elements(float a[], unsigned length) i 

3 int i; 

4 float-result = 0; 

5 

6 for (i = 0; i <= length-l; i++) 

7 result += a[i]; 

8 return result; 

9 } 

When run with argument length equal to 0, this code should return 0.0. 
Instead, it encounters a memory error. Explain why this happens. Show how this 
code can be corrected. 






You are given the assignment of writing a function that determines whether one 
string is longer than another. You decide to make use of the string library function 
strlen having the following declaration: 


/* Prototype for library function strlen */ 
size_t strlenCconst char ^s); 


Here is your first attempt at the fonction: 

/* Determine whether string s is longer than string t */ 

/♦ WARNING: This function is buggy */ 
int strlonger(char *s, char ♦t) { 
return strlen(s) - strlen(t) ’> 0; 

} 

1 

When you test this on some sample data, things do-not seem to work quite 
right. You investigate further and determine that, when compiled as a 32-bit 




84 Chapter 2 Representing'and Manipulating Information 

program, data type size_t is defined (via tjrpedef) ifi header file stdio .h to be 
unsigned. 

A. For what ca^es will this function produce an incorrect rpsult? 

B. Explain how'this incorrect result comes about. 

C. Show liow to fix the code so that it will work reliably. 

We have seen multiple ways in which the subtle features of unsigned arith¬ 
metic, and especially the implicit conversion of signed to unsigned, can lead to 
errors or vulnerabilities. One way to avoid such bugs is to never use unsigned 
numbers.'In fact, few languages other than C support unsigned integers. Appar¬ 
ently, these other' language designers viewe4 them as jnore trouble than they are 
worth. For example, Java supports only signed integers, and it requires that they 
be implemented with two’s-complement arithmetic. The normal right shift oper¬ 
ator » is guaranteed to perform an arithmetic shift. The special operator'»> is 
defined to perform a logical right shift. 

Unsigned values are very useful when we want t'o think of words as just col¬ 
lections of bitsr with no numeric interpretation. This occurs, for example, when 
packing a word with flags describing various Boolean conditions. Addresses are 
naturally unsigned, so systems programmers find unsigned'types to be helpful. 
Unsigned values are also useful when implementing mathematical packages for 
modular arithmetic and for multiprecision arithmetic, in which numbers are rep¬ 
resented by arrays o| words. 

23 Integer Arithmetic 

Many beginning programmers are surprised to find that adding two positive num¬ 
bers' can yield a negative result, and that the comparison x < y can yield a different 
result than the comparison ,x;-y < 0. .'These properties are artifacts of the finite na¬ 
ture offcomputer arithmetic. Understanding the nuaijce§ of computer arithmetic 
can help programmers write more reliable code. 

2.3.1 Unsigned Addition ^ 

Consider two nonnegative integers x and y, .such that 0 < x, y < 2“. Each of 
these values can be represented by a lu-bit unsigned number. If we coinpute "their 
sum, however, we have a possible range 0 < x -f y < — 2. Representing this 

sum could require w + 1 bits. For example^, Figure 2.21 shows a; plot of the func¬ 
tion X -t- y when x and y have 4-bit representations. The^arguments (shown on 
the horizontal axes) range from 0 to 15,^6ut the sum ranges from 0 to 30. The 
shape of the function is a sloping plane (the function is linear in both dimen,- 
sions). If we were to maintain the sum as a (lu -|- l)-bit number and add it to 
anothdnvalue, we may require u) -f- 2 bits, and so on. This continued ‘'word size 
























Section 2.3 integer Arithmetic 85 



Figure 2.21 Integer addition. With a 4-bit word size, the sum could require 5 bits. 


inflation means we cannot place any bound on the word size required to fully rep¬ 
resent the results of arithmetic operations. Some programming languages, such 
as Lisp, actually support arbitrary size arithmetic to allow integers of any size 
(within the memory limits of the computer, of course.) More commonly, pro¬ 
gramming languages support fixed-size arithmetic, and hence operations such 
as “addition” and “multiplication” differ from their counterpart operations over 
integers. 

Let us define the operation for arguments x and y, where 0 < y < 2'", 
as the result of truncating the integer sum x + y to be w bits long and then 
viewing the result as an unsigned number. This can be characterized as a form 
of modular arithmetic, computing the sum modulo 2“ by simply discarding any 
bits with weight greater than 2'"-i in the bit-level representation of x + y. For 
example, consider a 4-bit number representation with x = 9 and y = 12, having 
bit representations [1001] and [1100], respectively. Their sum is 21, having a 5-bit 
representation [10101]. But if we discard the high-order bit, we get [0101], that is 
decimal value 5. This matches the value 21 mod 16 = 5. 





86 Chapter 2 Representing and Manipulating Information 


Aside Security vulnerability in getpeername 

In 2002, programmers involved* in the FreeBSD open-source operating*sjStem^ project realized that 
their implementation 6f the gelpeemame library function had a security vulnerability. A simplified 
version of their code went something like this; 

1 /+ ^ 

2 * Illustration of code vulnerability similar lo that found in 

3 * FreeBSD's^implementation of, getpeernamfeO 

4 */ 

^ / 

6 /* Declaration of library function memcpy */, 

7 , void *ineiacpy(void *dest, void, *src, size_'t n) ; 

8 « 

9 /* Kernel memory region holding user-accessible "data */ 

10, #def ine KSIZE *1024 

11 char "kbuf [KSIZE] ; 

12 

13 /* Copy at most maxlen bytes from kernel region to user*buffer */ 

14 int copy_fromikernel(void *user_dest, int, maxlen) { 

•)5 /* Byte count len is minimum ,of buffer size “^d'daxleh */ 

16 int len = KSIZE" < idaxleh ? KSIZE : maxlen; 

17 memcpy (user„deSt, kbuf, len); * 

18 return len; 

^9 y , ,, t li 

In this code, we show the protofype for library function memcpy on line 7, \yhich is designed to copy 
a specified number'^bf bytes n from one region of membry to another.. 

The function copy_f rom_kernel, starting at line 14, is designed to copy some of the data main- 
tained by the operating systefii kernel to ^' designated regibh of memory accessible to the user. Most 1 
of the dath structures mainfained by the'kernel shotfld not be'readable by amser, since they may cop-* | 
tain sensitive information about other users and about Otheojobs runliing on thfe System, Wt the region 
shown as kbuf was intended to be one that the user could f ead. Thd parameter maxlen iS intended to be i 
the length of the buffer allocated by the user and indicated by argument userLdest;t The computation i 
at line 16 then makes shre that no more bytes are copied than'are available in eithef the So\irq,e 0r the | 

destination buffer. * 

Suppose, however, that some malicious programmer writes code that calls copy_f rom_kernerwith * 
a negative value of maxlen. Then the minimum coriiputation'on line^lb will compute*this value for len, 
which will then be passed ak the parameter n to memcpy. Note, hOwever, that pafa,meter n isdeclkredTas 
having data type size_-6. This datatype is* declared (via typedef) in the libraryfile stdio .-h. Typically, if * 
is defined to be unsigned for 32-bit programs and unsigned long foE 64-bit.programs. Since argument | 
n is unsigned, memcpy will treaf il as a very large positive nupiber and attempt to'copy that many bytes' 
from the kernel region to the-user’s buffer. Copying that mdny bytes (at least 1^'^) will not actually * 
work, because the program will encounter invalid addresses in the procOss, butahO program could read » 
regions of the kernel memoty for wfiich it'is not authorized. 

-y «• ■ irWKKwC'- H": ^4 





Section 2.3 Integer Arithmetic 87 


Aside ^Security vulnerability jrf getpeername (continued),- 

We can see ftaj this problem arises due to (he mismatch betw^dn.data types: in one place the 
length paiameteji§,signed;»in’another place ihis*unsigned. Such,mismatches can be 'h source of bugs 
and, as this example shows, can evep^jead to security vulnerabilities. Fortunately, there were no reported 
cases where a programmer had exploited ihe vulnerability in FreeBSD. They issued a security advisory 
“FreeBSD-SA-;{)2:38^signecl-eiTor'” advising system administrators on how to apply a patch that would 
remove tl^e .vulnerability. The,bug“carilie fixed by declaring parameter maxlen to copylfrom^kamel 
to be of tyjpe size_t, to be consistent with pardmetej n of memcpy. We should also declare local variable 
leu'and the returnwalue to be of t^d size_t. « t I 


We can characterize operation as follows: 

PRINCIPLE: Unsigned addition 
For X and y such that 0 < x, y < 2“: 

^ ^ X + .y < 2"' Normal 

“'^“U + y-2“’, 2“'<X + y < 2’"+! Overflow 

■ 

The two cases of Equation 2.11 are illustrated in Figure 2.22, showing the 
sum X + y on the left mapping to the unsigned m-bit sum x y on the right. The 
normal case preserves the value of x + y, while the overflow case has the effect of 
decrementing this sum by 2'". 

DERIVATION: Unsigned addition 

In general, we can see that if x + y < 2“', the leading bit in the (w + l)-bit represen¬ 
tation of the sum will equal 0, and hence discarding it will not change the numeric 
v^lue. On the other hand, if 2.'" < x + y < the leading bit in the (w + l)-bit 
representation of the sum will equal 1, and hence discarding it is equivalent to 
subtracting 2"' from the sum. I 

An arithmetic operation is said to overflow when the full integer result cannot 
fit within the word size limits of the data type. As Equation 2.11 indicates, overflow 



Figure 2.22 Relation between integer addition and unsigned addition. When x + y 
is greater than 2“" - 1, the sum overflows. 






88 Chapter 2 Representing and Manipulating Information 



I Figure 2.23 Unsigned addition. With a 4-bit word size, addition is performed 

I modulo 16, 

i 


occurs when the two operands sum to 2“ or more. Figure 2.23 ^hows a plot of the 
unsigned addition function for word size w = 4. The sum is computed modulo 
2^* = 16. When x + y <1^, there is no overflow, and x +4 y is simply x + y. This is 
shown as the region forming a sloping plane labeled “Normal,” When jc -h y > 16, 
the addition overflows, having the effect of decrementing the sum by 16. This is 
shown as the region forming a sloping plane labeled “Overflow.” 

When executing C programs, overflows are not signaled as errors. At times, 
however, we might wish to determine whether or not overflow has occurred. 

PRINCIPLE: Detecting overflow of unsigned addition 

For X and y in the range 0 < x, y < UMax^,, let 5 = x y. Dien the computation 
of s overflowed if and only if j < x (or equivalently, s < y)- * 


As an illustration, in our earlier example, we saw that 9 + 4 12 = 5. We can see 
that overflow occurred, since 5 < 9. 

















.Section 2.3 Integer Arithmetic 89 


DERIVATION: Detecting overflow of unsigned addition 

Observe that a: + ;y > x, and hence if s did not overflow, we will surely have s >x. 
On the other hand, if s did overflow, we have s = x + y - 2'^. Given that y < 2“ 
we have y - 2'" < 0, and hence 5 = a: + (y - 2®) < i. ■ 


Write a function with the following prototype: 


/* Determine whether arguments can be added without overflow */ 
int uadd_ok(unsigned x, unsigned y); 


This function should return 1 if arguments x and y can be added without 
causing overflow.. 


Modular additiotrforms a mathematical-stfucture knovra,as an^belian group, 
named after the Norwegian*mathematician Niels Henrik Abel (1802-1829). That 
is, it is commutative (that’s where' the “abelian” part comes in) and associative; 
it has an identity, element 0, and every element has an 'additive inverse. Let us 
consider the set of w-bit unsigned numbers with addition operation +“. For every 
value X, there must be some value -I X silch that a: x = 0. This additive 
inverse operation can be characterized as follows: 


PRINCIPLE: Unsigned negation 

For any number a: such that 0 < x < 2'", its ly-bit unsigned negation x is given 
by the following: 


[a:, x=0 

l2“’-x,. a:>0 


( 2 . 12 ) 

■ 


This result can Readily be derived by case analysis: 


DERI VAT ION: Unsigned negation 

When a: = 0, the additive inverse is clearly.6. For a; > 0, consider,the value 2“ - x. 
Observe that this number,^ in the yange;'o'< 2«’ - x < 2“:>e can also see that 
(x + 2“ — x) mod 2’" ^ 2f^mod 2“’ = 0. Hence it is the inverse of x under +^. ■ 

< f .f 



We can represent a bit pattern of length tu =,4.with a single hex digit. Fpr an 
unsigned interpretation of these digits, use Equation 2.12 to fill in the following 
table giving the values and tlie bit representations (in hex) of the uhsigned additive 
inverses of the digits shown. 





90 Chapter 2 Representing and Manipulating Information 


1 


R 


R 

I 


1 


I 


Hex Decimal Decimal Hex 

I — 

0 _ _ _ 

s _ _ _ 

8 _ __ 

D _ _ _ 

F _ _ _ 


2.3.2 Two's-Complement Addition 

With two’s-complement addition, we must decide what to do when the result is 
either too large (positive) or too small (negative) to represent. Given integer 
values X and y in the range —2’"“^ <x,y< 2““^ — 1, their sum is in the range 
—2“ < X + > < 2“ — 2, potentially requiring lu +1 bits to represent exactly. As 
before, we avoid ever-expanding data sizes by truncating the representation .to w 
bits. The result is not as familiar mathematically as modular addition, however. 
Let us define x +'-^ y to be the result of truncating the integer sum x'-|- y to be lu 
bits long and then viewing the result as a two’s-complement dumber. 

PR IN CIP L f,: Two ’s-complement addition 

For integer values x and y in the range —2"'“^ < x, y < 2'""^ — 1: 



X y — 2“', 2'"“^ 5 X -(- y Positive overflow 
X -H y, -2"'“^ < X + y < Normal 

X -h y -1- 2“, X 4- y < —2““^ Negative overflow 


1 

(2.13) 


This principle is illustrated in Figure 2.24, where the sum x -1- y is shown on the 
left, having a value in the range —2’" < x 4- y < 2*" — 2, and the result of truncating 
the sum to a u)-bit two’s-complement number is sho\yn on the right. (The labels 
“Case 1” to “Case 4” in this flgure are for the case analysis of the formal derivation 
of the principle.) When the sum x 4- y exceeds Tftiax^ (ca,se 4), we say that positive 
overflow has occurred. In this case, the effect of truncation is to subtract 2“’ from 
the sum. When the suih x -(■ y is'less than'TMin^ (case“l), we say that negative 
overflow has occurrfe’d. In thiS case, the effect of truncation is to add 2“ fo th'e suih. 

Tire w-bit two’s-complement sum of two number^ has the exact same bit-level 
representation as the unsigned sum. In fact, most computers use the same machine 
instruction to perform either unsigned or signed addition. 

derivation: Two’s-compleirient addition 

Sincq tyvq’s-complement addition has the exact saipe bit-level^ representation as 
unsigned addition, we can characterize the operation as one of converting its 
arguments to unsigned, performing unsigned addition, and then converting back 
to two’s complement: 


4 
























Section 2.3 Integer Arithmetic 91 


Figure 2.24 

Relation between integer 
an^ two's-complement 
addition. When a: + y is 
less than there is a 

negative overflow. When 
it is greater than or equal 
to 2 '"~\ there is a positive 
overflow. 



Af 3' = U2T^{T2U^{x) +; T2U^{y)\ (2.14) 

By Equation 2.6, we can write T2U^ix) as x^_{l'>^ + a: and TlU^iy) as 
+ y. Using the property that +“ is simply addition modulo 2“, along with 
the properties of modular addition, we then have 


a: y = U2T^<pu^{x) T2U^{y)) 

— + At 4- yiii_i2’" + y) mod 2“*] 

= U2J'w[(x + y) piod 2“'] 

The terms At,„_i2“' and 3'a,_i2'" drop out since they equal 0 modulo 2*". 

To better understand this quantity, let us define zhs the integer sum z = x + y 
z as z =z mod'2'", and z" as z" = U2T^{z'). The value z" is equal to x +‘ y. We 
can divide the analysis into four cases as illustrated in Figure 2.24:" 




+ 


-2'" < zj< -2*" \ pidn we will have z' = z + 2“'. This gives 0 < z' < -2“' -r 
~ ^- Examining EqUation-2.7; we see that z/ is in the range such that 

z = z. This is the case of negative overflow. We have added two negative 
numbers x and y (that’s the only-way'we can have z < and obtained 

a nonnegative result z" = a: + y + 2*". 


2. 2“' ^ < z < 0. Then we will again have z' = z + 2*", giving —2“'~1 + 2“ = 

^ - 2 < 2“. Examining Equation 2.7, we see that z' is in such a range that 

z =z'~ 2“’, and therefore z" = z' - 2’^-= z + 2“" - 2“' = z. That is, our two’s- 
complement sum z" equals the integer sum x + y. 

3. 0 < z < 2'^-\ Then we will have z' = z, giving 0 < z' < 2“-'i, and hence z" = 
z —z. Again, the two’S-complement sum z" equals the integer sum x -I- y. 

4. 2“'-! < z < 2“'. We will again hav^ z' - z, giving 2*"-^ < z' < 2“’. But in this 

range we have z" = z' - 2“^, giving z" = x + y-2<^. This is the case of positive 
overflow. We have added two positive numbers a: and y (that’s the only way 
we can have z > 2“ and obtained a negative result z" = ^ 4- ^ _ 2*". ■ 




92 Chapter 2 Representing and Manipulating Information 


X 

y 

X -by 


Case 

-8 

-5 

-13 

3 

1 

[1000] 

[1011] 

[10011] 

[0011] 


-8 

-8 

-16 

0 

1 

[1000] 

[1000] 

[loood] 

[0000] 


-8 

5 

-3 


2 

[1000] 

[0101] 

[11101] 

[1101]-, 


2 

5 

7 

7 

3 

[0010] 

[0101] 

[00111] 

[0111] 


5 

5 

10 

-6 

4 

[0101] 

[0101] 

[01010] 

[1010] 



figure 2.25 Two's-complement addition examples. Th^ bit-level representation of 
the 4-bit two's-complement sum can be obtained by performing binary addition of the 
operands anfi truncating the result to 4,bits. 


As illustrations of two’s-complement addition, Figure 2.25 shows some exam¬ 
ples when u; = 4. Each example is labeled by the case to which it corresponds in 
the derivation of Equation 2.13. Note that 2"^ = 16v and hence negative overflow 
yields a result 16 more than the integer sum, and positive overflow yields a result 16 
less. We include bit-level representations of the operands and the result. Observe 
that the result can be obtained by perfprming tjinary addition of the operai\d^ and 
truncating the result to 4 bits. 

Figure 2.26 illustrates two’s-complement addition for word size lu = 4. 'Pie 
operands range between —8 and 7. ,When -l- y < —8; two’s-complement addition 
has a negative overflow, causing the sum to be incremented by 16. When -8 < 
X -b y < 8, the addition yields x -i- y. When x 4- y > 8, dhe addition has a positive 
overflow, causing the sum to be decremented by .16. Each of these three ranges 
forms a sloping plane.in the figure. » 

Equation 2.13 also lets us identify the cases where overflow has occurred: 

PRINCIPLE: Detecting overflow in two ’s-complement addition 

For X and y in the range TMin^ < x, y < TMaXy;,let i = x +‘^ 1 , y. 'Then the compu¬ 
tation of s has had positive overflow if and only if > 0 and y > 0 but 5 < 0. The 
computation has had negative overflow if and only if x < 0 and y < 0 but j > 0. ■ 

Figure 2.25 shows several illustrations of this principle for w = 4. 'The.first 
enjpf shows a case of negative overflovy, where two negative numbers sum to a 
positive one. The final entry shows a case of positive overflow, where two positive 
numbers sum to a negative one. 





















Section 2.3 Integer Arithmetic 93 



Figure 2.26 Two's-complement addition. With a 4-bit word size, addition can have a 
negative overflow when j: -F y < -8 and a positive overflow when ;c -f- y > 8. 


DERIVATION: Detecting overflow of two’s-complement addition 

Ixt us first do the analysis for positive overflow. If both ;c> 0 and y > 0 but 5 < 0 
then clearly positive overflow has occurred. Conversely, positive overflow requires 
(D that ^ > 0 and y > 0 (otherwise, x + y < TMaxJ and (2) that s < 0 (from 
liquation 2.13). A similar set of arguments holds for negative overflow. ■ 




FiU m the following table in the style of Figure 2.25. Give the integer values of 
the 5-bit arguments, the values of both their integer and two’s-complement sums, 
toe bit-level representation of the two’s-complement sura, and the case from the 
derivation of Equation 2113. 


JT -f- y 


x+‘y 


Case 


[ 10100 ] [ 10001 ] 
















94 Chapter 2 Representing and Manipulating Information 


X y X + y ■*■'■ 5 )' Case 


[ 11000 ] [ 11000 ] 


[lom], [ 01000 ]' 


[ 00010 ] [ 00101 ] 


[ 01100 ] [ 00100 ] 


tlC€ 




Write a function with the following prototype; 


/* Determine whether 2 u:guments can be added without overflow */ ^ 

int tadd_ok(int x, int y); 

This function should return 1 if arguments x and y can be added without 
causing overflow. 




Your cowo'rker gets'impatient with yoiu analysis of the overflow* conditions for 
two’s-complement addition and presents you with the following implementatioii 


of tadd_ok: 


/* Determine whether arguments can be, added vithout overflow */ 
/* WARNING; This code is buggy. */ 
int tadd_ok(int x, int y) { ^ 

int sum = x+y| 

return (sura-x == y) && (sum-y == x); 

> 

You look at the code and laugh. Explain why. 


You are assigned the task of writing code for a function tsub_ok, with arguments 
X and y, that will return 1 if computing x-y does not cause overflow. Having just 
written the code for Problem 2.30, you write the following: 


/* Determine whether arguments can be subtracted without overflow */ 
/* WARNING: This code is buggy. */ ' 

int tsub_ok(int x, int y) ■( 























Section 2.3 Integer Arithmetic 95 


return tadd_ok(x, -y); 

> 




For what values of x and y will this function give incorrect results? Writing a 
correct version of this function is left as an exercise (Problem 2.74). 


2.3.3 Two's-Complement Negation 

We can see that every number x in the range TMin^ < x has an additive 

inverse under +'j^, which we denote x as follows: 


PRINCIPLE; Two’s-complementnegation 

For X in the range TMin^ 5 x < TMax^, its two’s-complement negation -{j, x is 
given by the formula 


I TMiriu,, X = TMitiu, 
I —X, X > TMiriy, 


(2.15) 

■ 


That is, for lu-bit two’s-compleraent addition, TMin^ is its own additive in- 
verse, while any other value jc has'—r as its additive inverse. 

DERIVATION:. TwoVcomplement negation 

Observe that TMin^ + TMin^ = -2“*“^ + —2“’“^ = —This would cause nega¬ 
tive overflow, and hence TMin^ +1^ TMin^ = —2^ + 2“" = 0. For values of x such 
that JC >'TMin^, the value ~x can also be represented as a uj-bit two’s-complement 
number, and their sum will be -x +x = 0. ■ 



We can represent a bit pattern of length w = A with a single hex digit. For a two’s- 
complement interpretation of these digits fill in the following table to determine 
the additive inverses of the digits shown: 


Hex Decimal Decimal Hex 

5__ _ 

D _ __ 

F ____ 

r 

What do yoli observe hbout the^bit patterns generated by two’s-complement 
I and unsigned (Problem 2.28) negation? 






96 Chapter 2 Representing and Manipulating Information 


VVe'fil'AsIde DATArT^EG gijalfevel Yepresentatiorf of two's-complement negation « ,, 

There arc’several clever ways to determine the two’s-cohiplenieiitl’negation^of a vaiueRepresented 
at the bit level. TTie fbliowi^ !wo’t^llni§ue^^th dse&,^ucl#a^ sjien one ejfec^fers AeVa^e ^ 
Oxf f f f f f f a yvhen debugging a progr|n^Vnd* lend insigh't into the natoe of the twb’s-compleiftenr 

representation. ». 

Qne' technique for perfonhing two’s-cpinplepient negation at the bjt level is to complenlent the bits ? 
and then in'crementRheResult. In C, we^ran state that for any integer-V^iu'e x?,*coffipYjtia|the expressidhs , 
-X and"~x +»1 will give identical result^ 

Hereure some^efamplel with ^ 4-bit wqrdjke;^ 


Jr-. . ... Jf 

“S' f* J 




Jc 


rx 



Jncr(-'x) 

-_i—^—a- 

JOlOl] 

, •*! "■ 

5 

[ipio] 

-"6 

[101*1]' -5 „ 

[Ollll 

7 

11606 ] 

-8 

[iboij -7 

[ 1100 ] 

-4 

^ iooil] 


[oioo] , 4 

[0000] 

0 

mill 

-^1 


OOOOp 0 ^ 

[1000] 


[bill]* 



[loop] , 


% p 14 % 

,.*« ’■ M’'4 i 

e* ■% tS- 

\ H 

Sv .4 


« *» •& » * 
V.3*' ** f 


For our earlier example, we know that the complemeripof Oxf is 0x0 anYl the complement of Oxa^^ 
is 0x5, andso Oxf flf iy/aj^KeYtwo’|;|opiBip|tehf|gpresentatiop o|4A|« ‘ ‘ « « - 

A seconydRvay lo perform twb’s-complenjentnegktion of 51 number «sisstja^ed on^sppttihg fhe pit ,| 
vector into two papts. let k be the position of the rightmost i, so the bitJevel representati6nt)f x has the I 
formlx^„_i, x^_2^. .., 0 . ■ • -Q]- (This is possible asJong»asx"s^O.)JhendgatiofLr^eri written I 

in binary fopn as 

bit position A. j, ^ , r, ,, # » I 

We illustrate this idea w}tl^sbin^%iinuinb^t|,,^}}ere %e highlight|ie rightmost pattepi0,^. .f ^J3 ^ 
ipdfalics: » 


WOO] 

(lOOO] 

[OlOi] 

[Dili] 


-4 

-8 

5 

7 




[0/W] 4 

d- t 

. 5 s t 

[ioo 4 -47 


4 t 1 . «*' 

iir * I’ ’ *’ 1 '! 


'■ s;'' 




H4 


2.3.4 Unsigned Multiplication 

Integers x and y in the range 0 < x, y < 2"' - 1 can be represented as w-bit un¬ 
signed numbers, but their product x ■ y can range between 0 and ( 2 “ - 1 )^ = 
22 w _ 2 UI +1 p_ xhis could require as many as 2w bits to represent. Instead, un¬ 
signed multiplication in C is defined to yield the tn-bit value given by the low-order 
w bits of the 2ui-bit4nteger product. Let us denote this jalue as x y. 

Truncating an unsigned number to w bits is equivalent to computing its value 
modulo 2“, giving the following: 














Section 2.3 Ihteger Arithmetic 97 


PRINCIRLE,* Unsigned multiplication 
For, a:, and y such thatp <x, y < UMax^-. 

^ 3^ = (•« ■ )>) mod 2“' (2.16) 


2.3.S Two's-Complement Multiplibation 

Integers x and y in the range —2“' ^ < ;r, j _ \ jjg represented as w;-bit 

two’s-complement numbers, bub their product,jc ■ y can range betwebn —2"'“^ • 
(2«'-i - 1 ,) = _ 22«"-2 -2“'-!^ = 2?'"-2. This-toiild require as 

many as 2iii bits to represent in two’s-complementtform! Instead, signed multi- 
phcation in C generally is performed by truncating the 2u;-bit product to w bits. 
We denote this yalue as .y y^^Tryncating a two’^-complement number to w bits 
is equivalent to first computing its value modulo 2“" and then converting from 
unsigned to two’s complement, giving the following: 

PRINCIPLE: Two’s-complembntmultiplication 
For and y such that TMin'^ <x,y< TMax^: 

^ 3' = U2T^{{x ■ y) mod,2“) (2.17) 


We claim that the bit-level fepf esentatioii of the product operation is identical 
for both unsigned and two’s-complement multiplicafion, as stated by the following 
principle: 


principle: Bit-level equivalence of unsigned and two’s-compleihent multipli¬ 
cation 

Let X and y be bit vectors'of length in. Define integers x and y as the Values repre¬ 
sented by these bits in two’s-complement form: x = B2T^{x) and y = B2Ty,{y). 
Define nonnegative integers x' and y' as the values represented by these bits in 
unsigned form: = B2U^(x) and y' = B2U^(y). Then 

T2B^(x*ly)'=U2B^(x' *1/) 

■ 

As illustrations, Figure 2.27 shows the results of multiplying different 3-bit 
nuinbers. For each pair of bit-level operands, we perform both unsigned and 
two’s-complement multiplication, yielding 6-bit products, and then truncate these 
to 3 bits. The unsigned truncated product always equals x ■ y mod 8. The bit- 
level representations df both truncated products are identical for both unsigned 
and two s-complement multiplication, even though the full 6-bit representations 
differ. ' 





98 Chapter 2 Representing and Manipulating information 


Mode 

X 

y 

1 E 

X' y 

Truncated x ■ y 

Unsigned 

5 [101] 

3 [011] 

15 [001111] 

7 

[111] 

Two’s complement 

-3 [101] 

3 [011] 

-9 [110111] 

-1 

[111] 

Unsigned 

4 [100] 

7 [111] 

28 [011100] 

4 

[100] 

Two’s complement 

-4 [100] 

-1 [111] 

4 [000100] 

-4 

[100] 

Unsigned 

3 [Oil] 

3 .[om] 

9 -[00100^]> 

1 

[001] 

Two’s complement 

3 [011] 

3 [011] 

9 [001001] 

1 

[001] 


Figure 2.27 Three-bit unsigned and two'j-complement multiplication examples; 
Although the bit-level represehtations of the full products may differ, those of the 
truncated products are identical. 

d’erivaTI’O N : Bit-level equivalence of unsigned and twb’s-complement multipli- 
ca'tion 

From Equation 2.6, we have x' — x + and y' = y + yio-i2“'. Computing the 

product of these values modulo 2“" gives the following; 

{x' ■ y') mod 2*" = [{x + x^_i2'^) ■ (y + y^-i2“)] mod 2“ (2.18) 

= [jc ■ y -h + yu)-i^)2“' + Xu,-\yw-i^^'"] 

— (x-y) mod 2"" 

The terms with weight 2’" and 2^“' drop out due to the modulus operator. By Equa¬ 
tion 2.17, we have x y = U2Tj(ix y) mod 2“'). We can apply the operation 
T2Uy^ to both sides to get 

T2U^{x y) = T2U^{mT^{{x • y) mod 2“)) = (x • y) mod 2“' 

Combining this result with Equations 2.16 and 2.18 shows that T2U^^(x y) 
(x',- yO mod 2“ = x' y'. We can then apply to both sides to get 

U2B^{T2U^{x*^^y}) = T2B^(x^ly) = U2B^ix'*ly’) 



•Fill in the following table showing the results of multiplying different 3-bit num¬ 
bers, in the style of Figure 2.27: 


Truncated x -.y 
_ _ 



Unsigned 

_ [100] — 

__ [101] 



Two’s complement 

[100] __ 

__ [101] 

■ -X 

t 

i 

Unsigned 

[010] _ 

— [Ill] 


Two’s complement 

[010] _ 

— [Ill] 
























Section 2.3 Integer Arithmetic 99 


Mode 

X 

y 

x y 

Truncated x ■ y 

Unsigned _ 

|B 

_riioi 



Two’s complement 


[110] 




You are given the assignment to develop code for a function tmult_ok that will 
determine whether two arguments can be multiplied without causing overflow. 
Here is your solution: 


/* Determine whether arguments can be multiplied without overflow */ 
int tmult_ok(int x, int y) f 
int p = x*y; 

/♦ Either x is zero, or dividing p by x gives y */ 
return !x II p/x == y; 

} 


You test this code for a number of values of x and y, and it seems to work 
properly. Your coworker challenges you, "saying, “If I can’t use subtraction to 
test whether addition has overflowed (see Problem 2.31), then how can you use 
division to test whether multiplication has overflowed?” 

Devise a mathematical justification of your approach, along the following 
lines. First, argue that the case r = 0 is handled correctly. Otherwise, consider 
w-bit numbers x (x ^ 0 ), y, p, and q, where p is the result of performing two’s- 
complement multiplication on x and y, and q is the result of dividing p by j:. 

1. Show that X ■ y, the integer product of x and y, can be written in the form 

X ■ y = p where t 7 ^ 0 if and only if the computation of p overflows. 

2. Show that p can be written in the form p = x • q + r, where |r [ < |x|. 

3. Show that q = y\i and only itr = t — Q. 



For the case where data type int has 32 bits, devise a version of tmult_ok (Prob¬ 
lem 2.35) that uses the 64-bit precision of data type int64_t, without using 
division. 



You are given the task of patching the vulnerability in the XDR code shown in 
the aside on page 100 for the case where both data types int and size_t are 32 
bits. You decide to eliminate the possibility of the multiplication overflowing by 
computing the number of bytes to allocate using data type uint64_t. You replace 












TOO Chapter 2 Representing and Manipulating Information 


Aside Security vulnerability in the XDR library 

In 2002, it was discovered that code supplied by Sun Microsystems to implement the XDR library, p. 
widely used facility for sharing data structures between programs, had a security vulnerability arising 
from the fact that multiplication can overflow without any notice being given to the program. 

Code similar to that containing the vulnerability is shown below: 

1 /* Illustration of code,,,vulnerability similar to that found in 

2 * Sun's XDR libreiry. 

3 */ 

4 void* copy_elements(void ♦ele_src[], int ele_cnt, size_t' ele_size) { 

5 /* 

6 * Allocate buffer for elG_cnt objects, each of ele_size bytes i 

^ i|l 

7 * and copy from locations designated by ele_src 

8 */ 

9 void *rehult = malloc(ele_cnt * ele_size); 

10 if (result == NULL) 

11 /* malloc failed */ 

12 return NULL; 

13 void *next = result; 

14 int i; j 

15 for (i = 0; i < el’e_cnt; i++) ■[ 

16 /* Copy object i t 9 destination ♦/ 

17 memcpyCnext, ele_sr,c[i], ele_size) ; 

18 /* Move pointer to next memory region ♦/ 

19 next +=■ ele_size; 

20 } 

21 return result; 

I- -4 i 

22 } 

The function copy_elements is designed to copy ele.cnt data structures, each consisting of ele_ 
size bytes into a buffer allobated by the function on line 9. The number of bytes required is pomputed 
as ele_cnt * ele_size. 

Imagine, however, that a malicious programmer calls this function with element being 1,048,577 
(2^° +1) and ele_size being 4,096 (2^^) with the program compiled for 32 bits. Then the multiplication { 
on line 9 will overflow, causing only 4,096 bytes to be allocated, rather than the 4,294,971*,392 bytes | 
required to hold that much data. The loop starting at line 15 will attempt to copy all of those bytes, j 
overrunning the end of the allocated buffer, and therefore corrupting other data structures. This could 1 
cause the program to crash or otherwise misbehave. 

The Sun code was used by almost every operating system and. in such widely used programs as ^ 
Internet Explorer and the Kerberos authentication system. The Computer Emergency Response Team , 
(CERT), an organization run by the Carnegie Mellon Software Engineering Institute to track security | 
vulnerabilities and breaches, issued advisory “CA-2002-25,” and many companies rushed to patch their i 
code. Fortunately, there were no reported security.breaches caused by this vulnerability. ^ 

A similar vulnerability existed in many implementations of the libraiy function calloc. These | 
have since been patched. Unfortunately, many^^programmers call allocation functions, such as malloc; 1 
using arithmetic expressions as arguments, without checking these expressions for overflow. Writing a J 
reliable version of calloc is left as an exercise (Problem 2.76). 



















Section 2.3 Integer Arithmetic 101 


the original call to malloc (line 9) as follows: 

uint64_t asize = 

ele_ciit * (uint64_t) ele_size; 
void ^result = mallo‘c(asize); 

Recall that'the argument to malloc has type'size_t. 

A. Does your code provide any improvement over the original? 

B; How would you change the code to eliminate the vulnerability? 

----- * ^ -» * 

r 

2.3.6 Multiplying by Constants 


Historically, the integer multiply instruction od many machines was fairly sldw, 
cequiring 10 or more clock cycles, whereas other integer operations—such, as 
.addition, subtraction, bit-level operations, and shifting—required only 1 clocjc 
cycle. Even on the Intel Core i7 Haswell we use as our reference machine, integer 
multiply requires 3 clock cycles. As a consequence, on^ important optimization 
used by compilers is to attempt to rbplace multiplications by constant factors with 
combinations of shift'and addition operations.* We will* first consider the case of 
multiplying by a power of 2, and then we will generalize this to arbitrary constants. 

PRINCIPLE: Multipliqation by a power of 2 

Let jc be the unsigned integer represented by bit'pattern x^_ 2 , jco]- 
Then for any > 0, the* w -l-.^-bit unsigned'representation of x2* is given by 
x^u_ 2 , ..., j:o. 0 , ..., 0], where k zeros have beeii added todhe right. ■ 

So, for example, 11 can be repif^se^ted for w = 4.^s,[1011]. Shifting this left 
by ^ = 2 yields the ,6-bit vector [101.100], which encodes the unsigned number 
11,- 4~44, 

DERIVATION: Multiplication by a power of 2 
This property can be derived using Equation 2.1; 


lU—1 

2» - ■ • > .Xq, 0, . . , , 0]) = X^'2} 

i=0 

■u;-l 


+k 




L /=0 
= x2* 


•2* 


^en shifting left by k for a fixed word size, the high-order k bits are discarded, 
yielding 


it—1* A:—2> ' * • » “^0» 0, . . . , 0] 


Chapter 2 Representing and Manipulating Information 

but this is also the case when performing multiplication on fixed-size words. We 
can therefore see that shifting a value left is equivalent to performing unsigned 
multiplication by a power of 2; 

PRINCIPLE; Unsigned multiplication by a power of 2 

For C variables x and k with unsigned values x and k, such that 0 < A: < w, the C 
expression x « k yields the value x 2*. ■ 

Since the bit-level operation of fixed-size two’s-complement arithmetic is 
equivalent to that for unsigned arithmetic, we can make a similar statement about 
the relationship between left shifts and multiplication by a power of 2 for two’s- 
complement arithmetic: 

PRINCIPLE; IVvo’s-complement multiplication by a power of 2 

For C variables x and k with two’s-complement value x and unsigned value k, such 
that 0 <k < w, the C expression x « k yields the value x 2*. ■ 

Note that multiplying by a power of 2 can cause overflow with either unsigned 
or two’s-complement arithmetic. Our result shows that even then we will get the 
same effect by shifting. Returning to our earher example, we shifted the 4-bit 
pattern [1011] (numeric value 11) left by two positions to get [101100] (numeric 
value 44). Truncating this to 4 bits gives [1100] (numeric value 12 = 44 mod 16). 

Given that integer multiplication is more costly than shifting and adding, many 
C compilers try to remove many cases where an integer is being multiplied by a 
constant with combinations of shifting, adding, and subtracting. For example, sup¬ 
pose a program contains the expression x*14. Recognizing that 14 = 2^ -I- 2^ -(- 2^, 
the compiler can rewrite the multiplication as (x«3) + (x«2) + (x«l), replac¬ 
ing ohe multiplication with three shifts and two additions. The two computations 
will yield the same result, regardless of whether x is unsigned br two’s comple¬ 
ment, and even if the multiplication would cause an overflow. Even bettet, the 
compiler can also use the property 14 = 2^* — 2^ to rewrite the multiplication as 
(x«4) - (x«l), requiring only two shifts and a subtraction. 



As we will see in Chapter 3, the lea instruction can perform computations of 
the form (a«k) + b, where k is either 0, 1, 2, or 3, and b is either 0 or some 
program value. The compiler often uses this instruction to perform multiplications 
by constant factors. For example, we can compute 3*a as (a«l) + a. 

Considering cases where b is either 0 or equal to a, and all possible values of k, 
what multiples of a can be computed with a single lea instruction? 


Generalizing from our example, consider the task of generating code for 
the expression x * K, for some constant K. The compiler can express the binary 
representation of K as an alternating sequence of zeros and ones: 























Section 2.3 Integer Arithmetic 103 


For example, 14 can be written as [(0 ... 0)(111)(0)]. Consider a run of ones from 
bit position n down to bit position 'm {n>m). (For the case of 14, we have n = 3 
and m = 1.) We can compute the effect of these bits on the product using either of 
two different forms: 

Form iA.: (x«n) + (x«(n — 1)) + • ■ - + Xx«/m) 

FormB: (x<<?(7j+l)) - (x«m) 


By adding together the results for each run, we are able to compute x * K with¬ 
out any multiplications. Of course, the trade-off between using combinations of 
shifting! adding, and subfractin^ versiis a single multiplication instruction depends 
on the relative speeds of these instructions, and these can be highly machine de¬ 
pendent. Most compilers only perform this optimization when a small number of 
shifts, adds, and subtractions suffice> >• 


How could we modify the expression for form B for the case where bit position n 
is the most significaiit bit? 



For each of the following values of AT, find ways to express x * K using only the 
specified number of operations, where we 'consider both addition^ and subtrac¬ 
tions to have compara|3le co§t. You may need to use some tricks beyond the simple 
form A'and B rules we have considered so far. 


Jf Shifts ,Add/Subs Expression 

6 2 1 _^ 

31 1 1 r ! 

~6 2 1 _ 

55 2 2 _ 


IMS; 




Fdr a run of ones starting at bit Jlosition n'dbwn fb"bit position m{n> m),-we saw 
that we call generatd 'two forms of cd^e, A and B. How should'the compiler decide 
wMchYonn'to-use? 


2.3.7 Dividing by Powers of 2 

Integer division on most machines is even slower than integer multiplication— 
requiring 30 or more clock cycles. Dividing by a power of 2 can also be performed 














104 Chapter Z Representing and Manipulating Information 


k 

» k (binary) 

Decimal 

152,340/2'' 

0 

0011000000110100. 

12,340 

12,340.0 

1 

0001100000011010 

6,170 

6,170.0 

4 

0000001100000041 

771 

771.25 

8 

0000000000110000 

48 

48.203125 


Figure 2.28 Dividing unsigned numbers by povJers of 2. The examples illustrate 
how performing a logical right shift by k has the same effect as-dividing by 2*^ and then 
rounding toward zero. 


using shift operations, but . we use a right shift rather than-a left shift. The two 
different right shifts—logical and arithmetic-^—serve this purpose for unsigned and 
two’s-complement numbers, respectively: ■' 

Integer division always rounds ’toward zero. Toxlefine this precisely, let.us 
introduce some notation. For any real number a, define iaj to be the unique 
integer a' such that a' <a <a' + 1. As examples, L3.14J = 3, [—3.14J = —4, and 
1.3J = 3. Similarly, define fa] to be the unique integer a' such that a' — 1 < a < a'. 
As examples, [3^41 = 4, f-3;l'4] =-3, and r3t = 3. For x > O'and y > 0, integer 
division should yield [x/yj, while for x < 0 and y > 0, it should yield fx/yl. That 
is, it should round down a positive result but round up a negative one. 

The case |or Using shift's with unsigned arithmetic is straightforward, in part 
because right shifting is guaranteed to be performe4 logically for unsigned values. 

f'v o 

PRIN CIP LE: Unsigned divisiorkjjy a power of 2 

For C variables x and'k with unsigifed values x and k, subh that 0'^ k‘< w, the C 
expression x » k yields the value Lx/2^j. ■ 

As examples. Figure 2.28 shows the effects of performing logical right shifts 
on a 16-bit representation of 12,340 to perform division by 1,2,16, and 256. The 
zeros shifted in from the left are shown in italics. We also show the result we would 
obtain if we did these divisions with real arithmetic. These examples show that the 
result of shifting consistently rounds toward zero, as is the convention for integer' 
division. 

DERIVATION: Unsigned division by a power of 2, 

Let X be tjie unsigned integer represented,*bX“t*it pattern [xu,_i, Xy,^2< ■ ./o]. and 
let k be in the rangg O <k < iq .^et x' be^he unsigned number with ly — fc-bit 
representation [xu,_i, x^_ 2 > • • •. X;^], and let x" be the unsigned nqpiber withfe-bit 
representation [x,t-i. • • •. ^o]- We can therefore see that x = 2*x' + x", and that 
0 < x" < 2*. It therefore follows that Lx/2*J = x'. 

Performing a logical right shift of bit vector [xu,_i,,Xju._ 2 ,.j..^^,,xo] by k yields 
the bit vector 


[0, . . . , 0, Xn,_]^, Xy,_2, . . . , Xj^] 






















Section 2.3 Integer Arithmetic 105 


k 

» k (binary) 

Decimal 

-12,340/2'' 

0 

liOOllllllOOilOO 

-12,340 

-12,340.0 

1 

tllOOllllllOOllO 

-6,170 

-6,170.0 

4 

trttiiooiiiiiloo 

-111 

-771.25 

8 

iiiiiiiiiiooiin 

-49 

-48.203125 


Figure'2.29, Applying arithmetic ri^ht shift. The examples illustrate That arithmetic 
right shift is similar to division by^a powepof 2? except that it rounds down rather than 
toward zero. 


This bit vector has numeric .value xV which we have seen is the value that would 
result by computing the expression x » k. I 


The case for dividing by a _^pwer of 2 withpwo’s-corriplement arithmetic is 
slightly more complex. First, the' shifting shpuld be performed using an arithmetic 
right shift, to eiisure that negative values remain negative. Let us investigate what 
value such a right shift would produce. 


PRINCI P.LE; T\vo’s-complement division by a power of 2, rounding down 

Let C variables x and k have two’s-complement* value x and unsigned value 
k, respectively, such.t.hat w. The C expression x » k, when the shift is 

performed arithmetically, yields the .yalue [x/2*J. ■ 

For X > 0, variable x has 0 as the most’significant bit, and so,the effect of an 
arithmetic shift is the same as for a lo^cal right shift. Thus, an arithmetic right shift 
by k is the same as division by 2* for a nonnegahve number. Xs ah example of a 
negative number. Figure 2.29 shows the effect of applying arithmetic right shift to 
a;^6^bit representation of -12:340 for different shift amounts. For the case when 
no rounding is required {k = 1), the resqlt will be x/2*. \\4ien rounding is required, 
shifting causes'the result to be rounded'downward. For example, the shifting right 
by four ha§ the effec.t of rounding —771.25 down to —772. We will need to adjust 
our strategy to handle division for negative values of x. 


DERIVATION: Two’s-complement division by a power of 2, roimding down 

Let X be the two’s-complement integer represented by bit pattern x,„_ 2 , 
• • • > ^nd let k be in the range i) <k < w. Let x' be the two’s-complement 
number represented by the lu - A: bits [x,„_i, x,„., 2 , ■ ■ • . a:*], and let x" be the 
unsigned number representecf by the low-order k bits [x*,_i,, xq]. By a s imilar 
analysis as the unsigned case, we have x = 2*x' + x" and 0 < x" < 2*, giving x' = 
L'x/ 2*J.-Furthermore, observe that shifting bit vector [x„,_i, x„,_ 2 , . , xq] right 

arithmetically by k yields the bit vector 


• 1 1 > ^w—2’ • ■ ■ > ^kl 

which is the sign extension from w-k bits to w bits of (x,„_i, x^„_ 2 ,.... x*]. Thus, 
this shifted bit vector is the two’s-complement representation of [x/2*^J. ■ 


106 Chapter 2 Representing and Manipulating Information 


k 

Bias 

—12,340 + bias (binary) 

>‘^ k (binary) 

Decimal 

-12,340/2^ 

0 

0 

1100111111001100 

1100111111001100 

-12,340 

-12,340.0 

1 

1 

1100111111001101 

1110011111100110 

-6,170 

-6,170.0 

4 

15 

1100111111011011 

1111110011111101 

-771 

-771.25 

8 

255 

1101000011001011 

1111111111010000 

-48 

-48.203125 


Figure 2.30 Dividing two's-complement numbers by powers of 2. By adding a bias 
before the right shift, the result is" rounded toward ^ero. 


We can correct for the improper rounding that occurs when a negative number 
is shifted right by “biasing” the value before shifting. 

PRI NCI PLE: Two’s-complement division by a power of 2, rounding up 

Let C variables x and k have two’s-cc)mplement value x'and'unsigned value k, 
respectiyely,’such thaf0 5 ^ < w. The texpression^t'x*+ (i «‘k'/ - 1) », k,when 
the shift’ is performed arithmetically, yields the value. fx /2*1. ■ 

Figure 2.30 demonstrates how adding the appropriate bias before performing 
the arithmetic right shift causes the result to be correctly rounded. In the third 
column, we show the result of adding.the bias value to —12,640, with the lower k 
bits (thos’e that will be shifted off to the tight) shown in italics. We can see that 
the bits to the left of these may or may not be incrementecl. For the case where no 
rounding is rec^uired (^ = 1), adding the bias only affects bits that are shifted off. 
For the cases where rounding is required, adding the bias causes the upper bits to 
be incremented, 39 that the result will be rounded toward zero. 

The biasing techhique exploils'the property that \xjy'\ = [(x + y - l)/yj for 
integers x and y such that y >; 0. As examples, whqn x = —30 and y = 4, we have 
X + y — 1 — —27 and f-^p/41 = -7 = [-27/4], When x = —32 and y = 4, we have 
x + y-\ = -29 and [-32/41 = -8 = L-2P/4J. 

DERIVATION: Two’s-complement division by a power of 2, rounding up 

To see that [x/yl = [(-^ + y — ll/yJ. suppose that x = qy + r, where 0 < r < y, 
giving (x -I- y - l)/.y ='q, 4- (r 4- y - l)/y, and so L(x 4-,y'- l)/yj = 9 4- K'’ 4r y - 
l)/yj. The latter term will equal 0 when r = 0 and 1 when r > 0. That is, by adding 
a bias of y — 1 to x and then rounding the division dbwhward, we will get q when 
y divides x and 5 4-1 otherwise. 

Returning to the case where y = 2*, the C expression x + (1 « k) - 1 yields 
the value x 4-,2* - 1. Shifting this right arithmetically b'y k therefore yields [x/2*1. 

I 

These analyses show that for a two’s-complement machine using' arithmetic 
right shifts, the C expression 

(x<b ? x+(l«k)-l : x) » k 

will compute the value x/T^-. 


1 























Section 2.3 Integer Arithmetic 107 



Write a function divl6 that returns the value x/16 for integer argument x. Your 
function should not use division, modulus, multiplication, any conditionals (if or 
?;), any comparison operators (e.g., <, >, or ==), or any loops. You may assume 
that data type int is 32 bits long and uses a two’s-complement representation, and 
that right shifts are performed arithmetically. 

We now see that division by a power of 2 can be implemented using logical qr 
arithmetic right shifts. This is precisely the reason the two types of right shiffs'are 
available o^ most macjiines. Unfortunately, this approach does not generalize to 
division by arbitrary constants. Unlike multiplication, we cannot express division 
by arbitrary constants K in terms of division by powers of 2. 



In the following codS, we have omitted the' definitions of constants M and N; 


#deflne M /* Mystery number 1 ♦/ 

#definG N /* Mystery number 2 */ 

int arithfint x, int y) { 
int result = 0; 

result = x*M + y/N; /♦ M and N are mystery numbers. ♦/ 
return result; 

> 

We compiled this code for particular values of M and N. The compiler opti¬ 
mized the multiplication and division using the jnethods we haye discussed. The 
following is a translation of the generated machine code back into C: 

/* Translation of assembly code for arith */ '< 

int optarithCint x, int y) ■( 
int t = x; 
x «= 5; 

X -= t; 

if (y < 0) y += 7; 
y »= 3; /* Arithmetic shift */ 

return x+y; 

} 

I 

What are the values of M and N? 


2.3.8 Final Thoughts on Integer Arithmetic 

As we have seen, the “integer” arithmetic performed by computers is really 
a form of modular arithmetic. The finite word size used to represent numbers 













108 Chapter 2 Representing and Manipulating Information 


limits the range of possible values, and the resulting operations can overflow. 
We have ^so seen that ^he two’s-complenfient repre,mentation provides a clever 
yvay to represent.both negative and posiljye values, whil,e using the same bit-level 
implementation? ^9 perform unsigned arithmetic—operations such as 

addition, subtraption, multiplication,, and even division ,have either i^lentical or 
very similar bit-level behaviors, whether the operands are in unsigned cir two’s- 
complement form. 

We have seen that some of the conventions in the C language can yield some 
sumrising results, and these can be sources of bugs that are hard to recognize or 
understand. We^have especially seen that the unsigned data type, while conceptu¬ 
ally straightforward, can lead to behavioh^ that eyen experienced pro^ammers'do 
not expect. We have also seen that thk data type can arise in unexpected ways—for 
example, when writing integer constants'and when invotinglibrary routines. 


Assume data type int is 32 bits long and uses a two.’s-complejnent representation 
for signed values. Right shifts are performed arithmetically for signed values and 
logically for unsigned values. The variables are declared and initialized as follows: 

int X = foot): /* Arbitrary value */ 
int y = barO; /* Arbitrary value */ 


i 

unsigned ux “ x; 
unsigned uy = y; 

For each of the following C expressi'dns, either (1) argue that it is true‘(evalu- 
ates to 1) for all values of x arid y,’of (2) give values of lx and y for which jt is false 
(evaluates to^): ’ ' 


A. (x > 0) II (x-1 < 0) 

B. (x & 7) != 7 I I (x«29 < 0) 

C. (x * x) >= 0 

D. X < 0 I I -X <= 0 

E. X > 0 11 -X >= 0 

F. x+y == uy+ux 

G. x*-y + uy*ux == -x 


2.4 Floating Point 

A floating-point representation encodes rational numbers of the form V = x x 2^. 
It'is useful for performing computations involving very large numbers V | 3> 0), 






























Section 2.4 Floatingpoint 109 




Th^'lEEE 


” 1 *”' 


• "’ If: • 








I Thg Institute of El&'tricaran^ Electronics Engineers (IEEE—pronounced “eye-triple-ee’*) is^a prol*^» 
I fes'sionalsociety. that encompasses alt of electronic an‘d'^mputer’techij.olbgy.^lf*ptiblishes'j'oiirria](s,'' 

I sponsors confereiipes, ant} sets a^xommittedstto ddfine^stan^ards^omtopics rangmg''froi&power limans- 
I mission to software engine‘ering. Another example of jaV IEEEf,stan'dard is the ^802.11 ^Iridard-for * 
wirelessmetworking. *■ * s. ^ >■ ^ i *■ =■* ^ ^ * 


1 . 


numbers very close to 0 ([ E | <$C 1), and more generally as an approximation to real 
arithmetic. 

Up until the 1980s, every computer manufacturer devised its own conventions 
for how floating-point numbers were represented and the details of the operations 
performed on them. In addition, they often did not worry too rnuch about the 
accuracy of the operations, viewing speed and ease of implementation as being 
more critical than numerical precision. 

All of this changed around 1985 with the advent of IEEE Standard 754, a 
carefullyxrafted standard for representing floating-point numbers and the oper¬ 
ations performed on them. This effort started in 1976 under Intel’s sponsorship 
with the design of the 8087, a chip that provided floating-point support for the 8086 
processor. Intel hired William Kahan, a professor at the University of California, 
Berkeley, as a consultant to help design a floating-point standard for its future 
processors. They allowed Kahan to join forces with a committee generating an 
industry-wide standard under the auspices of the Institute of Electrical and Elec¬ 
tronics Engineers (IEEE). The committee ultimately adopted a standard close to 
the one Kahan had devised for Intel. Nowadays, virtually all computers support 
what has become known as IEEE floating point. This has greatly improved the 
portability of scientific application programs across different machines. 

In this section, we will see how numbers are represented in the IEEE floating¬ 
point format. We will also explore issues of rounding, when a number cannot be 
represented exactly in the format and hence must be adjusted upward or down¬ 
ward. We will then explore the mathematical properties of addition, multiplica¬ 
tion, and relational operators. Many programmers consider floating point to be 
at best uninteresting and at worst arcane and incomprehensible. We will see that 
since the IEEE format is based on a small and consistent set of principles, it is 
really quite elegant and understandable. 

2.4.1 Fractional Binary Nurnbers 

A first step in understanding floating-point numbers is to consider binary numbers 
having fractional values. Let us first examine the more familiar decimal notation. 
Decimal notation uses a representation of the form 

dm dm-X ■ ■ ■ dxdfj . d_x d-2 ■ ■ ■ 













"110 Chapter 2 Representing and Manipulating Information 


Figure 2.31 
Fractional binary 
representation. Digits 
to the left of the binary 
point have weights of the 
form T, while those to the 
right have weights of the 
form 1/2'. 



where each decimal digit d, ranges between 0 and 9. This notation represents a 
value d defined ds' ’ 

d = Xl'.lO' X di ^ 

The weighting of the digits is defined relative to'the'decimal-point symbol («.'), 
irieaning that digits to the left are weighted by nonnegative powers of 10 , giving 
iiltegral values, while digits to the right are weighted by negative powers of- 10 , 
giving fractional values. For example, 12.34io represents the number 1 x 10 + 

2 X 10'’ -t- 3 X i0“^ -I- 4 k 10“^ = 12^. 

By analogy, consider a notation of the form 

^m-1 ■ ■ • • • • bLn+ib^n 

where each-binary digits or bit, bi ranges between 0 and 1 , is illustrated in 
Figure 2.31. This notation represents a number fc defined as 

m 

b=^2'iibl ‘ (2.19) 

i=—n 

The symbol now becomes a binary point, with'bits on the left being weighted 
by nonnegative powers of 2 , and those on the right being weighed by negative 
powers, of 2. For example, IOI.II 2 , represents the number 1x2 +0x2 + lx 
2 '’+ 1 X 2"^ +1 X 2"^ = 4 + 0 + 1 + 2+-3 = 5|. 

One can readily see from Equation 2.19 that shifting the bmary point one 
position to the left-has the^effect of dividing the number by 2. For example, while 
IOI.II 2 represents the number 5|, IO.III 2 represents the number 2 + 0 + 2 + 


I 


i 






















Section 2.4 Floating Point 


11 7 ’ 

4 + 8 =,2g.Similarly,’ shifting'the binary point one position to the right has the 
effect of multiplying the number by 2. For example, lOli. Ij represents the number 
8 + 0 + 2 + 1 + 1 = 111 . 

'Note that numbers of the form 0.11 ■ • •■I2 represent numbers just below 1. For 
example, O.IIIIII2 represents ||. We wilt use the shorthanchnotation 1.0 - e to 
represent "such values. 1 

Assuming we consider only finite-length encodings, decimal notation cannot 
represent numbers such as 5 and ^ exactly. Similarly„fractional binary notation 
can only represent numbers.that can be written x x 2>’».Other values can only be 
approximated. For example, the number 5 can be represented exactly as thfe frac¬ 
tional decimal number 0.20. As a fractional-binary number, however, we cannot 
represent it exactly and instead must approximate it with increasing accuracy by 
lengthening the binary representation: 


Representation 

Value 

Decimal 

O.O 2 

1- 

0 

0 

0 

0.01^ 

1 

4 

0.25io 

0,0102 

2 * 
8 

0 

0 

0.001 12 

3 

1? 

0.1875io 

0.001 IO 2 

6 

32 

0.1875io 

0.0011012 

13 

54 

0.203125io 

0.00110102 

26 

128 

0.203,125io 

0.00110011*2 

51 

255 

0.19921875; 


Fill in the iriissing" information'in the following table; ^ 

Fractional value Binary representation Decimal representation 


lO.lOll 

1.001 


5.875 

3.1875 


The imprecision’of floatihg-point'arithmetic can have disastrous effects! On Febru¬ 
ary 25, 1991, during the first Gulf War, an American Patriot Missile battery in 
Dharan, Saifdi Arabia, faUbd lo ‘intercept ah incoming' Iraqi* Scud missile. The 
Scud struck an American Array barracks and killed-28 •soldiers'; The US General 


112 Chapter 2 Representing and Manipulating Information 


Accounting Office (GAO) conducted a detailed analysis of the failure [76] and de¬ 
termined that the underlying cause.was an imprecision in A numeric calculation. 
In this exercise, you will reproduce part of the GAO’s analysis. 

The Patriot .system contains an internal clock, implemented ,as a counter 
that is incremented every,0.1 seconds. To determine the time in seconds,- t]ie 
program would multiply the value of this counter by a 24-bit quantity that was 
a fractional binary approximation to In particular,, the binary representation 
of ^ is the nonterminating sequence 0.000110011[0011] • ■ -2, where the portion in 
brackets is repe'ated indefinitely. The program approximated 0.1, as a value x,by 
considering just the first 23 bits .of .the sequence to the right*jof the binary.point; 
X = 0.00011001100110011001100.'(See Problem 2.51 for a discussion of howlhey 
could have approximated 0.1 more.precisely.) ^ 

A. What is the binary representation of 6.1 — x? 

B. What is the approximate decimal value of 0.1 — x? 

C. The clock starts at 0 when the system is first powered up and keeps counting 
up from there. In this case, the system had been running for around 100 hours. 
What was the difference between the actual time and the time computed by 
the software? 

D. The system predicts where an incoming missile will appear based on its 
velocity and the time of the last radar deteciibn. Given that a Scud travels 
at around 2,000 meters per second, how far off was its prediction? 

Normally, a slight error in the absolute time reported by a clock reading would 
not affect a tracking computation. Instead, it should depend on the relative timfe 
between two successive readings. The problem was that the Patriot software had 
been upgraded to use a more accurate function for reading time, .but not all of 
the function calls had been replaced by the new code. As a result, the tracking 
software used the accurate time for one reading ahd'the inaccurate time for the 
other [103]. , 


2.4.2 IEEE Floating-Point Representation 

Positional notation such as considered in the previous section would not be ef¬ 
ficient for representing very large numbers. For example, the representation of 
5 X 2^*® would consist of the bit pattern 101 followed by 100 zeros. Instead, we 
would like to represent niinibers in a form x x 2^ by giving the values of x and y. 

The IEEE floating-point-standard represents a number in a form V = (-1)'^ x 
M X 2^; 

• The sign s determines whether the number is negative (5 = 1) or positive 
(s = 0), where the interpretation of the si^n.bit for nui^eric value 0 is handled 
as* a special^:ase. 

• T\ie,significand, M is.a fractional binary number that ranges either between 1 
and 2 — e x)r between p-and 1 *- €. 

• The exponent E weights the value by a (possibly negative) power of 2. 




























Section 2.4 Floatingpoint 113 


Single precision 

31 30 23 22 " 0 



Figure 2.32 Standard floating-point formats. Floating-point numbers are represented 
by three fields. For the two most common formats, these are packed in 32-bit (single¬ 
precision) or 64-bit (double-precision) words. 

The bit representation of a floating-point number is divided into three fields to 
encode these values: 

• The single sign bit s directly encodes the sign s. 

•.TTie fc-bit exponent field exp = encodes the exponent E. 

• The n-bit fraction field frac = /„_i • • • /x/q encodes the significand M, but 

the value encoded also depends on whether or not the exponent field equals 

0. 

Figure 2.32 shows, the packing of these three, fields into words for the two 
most common formats. In the single-precision floating-point"format-(a float 
in C), fields s,- exp, and fjrac are 1, k,= 8, and n = 23 bits each, yielding a-32- 
bit representation.-In the double-precisigh floating-point fonnat (a double in C), 
fields's, exp, and frac are 1, k = ll,‘and n=:52 bits each, yielding a 64-bit 
representation. 

The value encoded by a given bit representation can be divided into three 
different cases (the latter having two variants), depending on the value of exp. 
These are illustrated in Figure 2.33 for the single-precision format. 

Cas^ 1: Normalised Values 

ft 

This is the most comnjon pa^e. It.occurs w^ipn the bit pattern of exp is neither 
all zeros (numeric value 0) nor all ones (numeric value 255 for single precision, 
2047 for double). In this case, the exponent field is interpreted as representing a 
signed integer in biased form. That is, the exponent value is E = e — Bias, where 
e is the unsigned number having bit representation ■ ■ • eie^ and Bias is a bias 

value equal to 2*“^ -1 (127 for single precision and 1023 for double). This yields 
exponent ranges from —126 to 4-127 for single precision and —1022 .to 4-1023 for 
double precision. 

The fraction field frac is interpreted as representing the fractional value /, 
where 0 < / < 1, having binary representation ■ ■ ■ /i/o, that is, with the 










114 Chapter 2 Representing and Manipulating Information 


a" • 


4 *- 


Aside Why set the bias t^is \^ay for dehQrmajized >faibes? 

Having the expon^ftt vMue be X^—JBiic^ Sjisfit s??®! Counterintuitive^ We will 

see shortly that it provides fo'r«smdbth transition«from deitormalizedlo'normalizea yalhesi 








1. Normalized 



Figure 2.33 Categories of singid-precision floating-point values. The value of the 
exponent determines whether the number is<l^ normalized, (2) denormdlized, 5r (3) a 
special value. i 


binary point to the left of the' ihost significant bit. The significand is defined to be 
M = 1 + /. Thisis'Sfometimes called an itnplied leading’l representation, because 
we cafl' view M to be.the number with binary representation ■ - M This 

representation is a trick f6r gettin'g an atiditiohalliit of precision for free, since we 
can always adjust the exponent E so that significand M is in the rangel E'M <2 
(assuming there is no overflow). We therefore do not need to explicitlyTepresent 
the leading bit, since it always equals 1. 

Case 2: Denorffialized Values ' 

When the exponent field is all zeros, the represented.number is in ^deporma^ized 
form. In this case, the exponent value is £ = 1 — Bias, and the significand value is 
M — f, that'is, the value of the fraction field'hvitVdut an implied leadirfg'T. 

Denorihalized numbers serve 'two purfloses. First, they provide a way ‘to 
represent numeric vali/dD, Snce with a normalized number we must always have 
M > 1, and hence we cannot represent 0. in fact, the floating-point representation 
of -t-0.0 has a bit pattern of all zeros; the sign'bifis 0, the exponent field is'all 
zeros (indicating a denormalizfed value), and the fraction field is all zeros,'giving 
Afi= / = 0. Curiously, when the sign bit is 1, but the other fields are all zerbs, we 
get the value -0.0. With IEEE floating-point format, the values -0.0’and -t-0.0 
are considered different in sofne'ways and the salne in others. 



























Section 2.4 Floating Point 


115 


A second function of denormalized numbers is to represent numbers that are 
very close to 0.0. They provide a property known as gradual underflow in which 
possible numeric values are spaced evenly near 0.0. 

Case 3; Special Values 

A final category of values occurs when the exponent field is all ones. When the 
fraction field is all zeros, the resulting values represent infinity, either +oo when 
5 = 0 or —oo when 5 = 1. Infinity can represent results that overflow, as when we 
multiply two very large numbers, or when we divide by zero. When the fraction 
field is nonzero, the resulting value is called a NaN, short for “not a number.” Such 
values are returned as the result of an operation where the result cannot be given 
as a real number or as infinity, as when computing or oo — oo. They can also 
be useful in some applications for repiresenting uninitialized data. 

2.4.3 Example Numbers 

Figure 2.34 shows the set of values that can be represented in a hypothetical 6-bit 
format having A = 3 exponent bits and n — 2 fraction bits. The bias is 2^“^ — 1 = 3. 
Part (a) of the figure shows all representable values (other than NaN). The two 
infinities are at the extreme ends. The normalized numbers with maximum mag¬ 
nitude are ±14. The denormalized numbers are clustered around 0. These can be 
seen more clearly in part (b) of the figure, where we show just the numbers be¬ 
tween —1.0 and +1.0. The two zeros are special cases of denormalized numbers. 
Observe that the representable numbers are not uniformly distributed—they are 
denser nearer the origin. 

Figure 2.35 shows some examples for a hypothetical 8-bit floating-point for¬ 
mat having A = 4 exponent bits and n = 3 fraction bits. The bias is 2“*“^ — 1 = 7. 
The figure is divided into three regions representing the three classes of numbers. 
The different columns show how the exponent field encodes the exponent E, 
while the fraction field encodes the significand M, and together they form the 


D h -ft-ft-ft—ft—ft—ft— ft - ftftftft fttftftfWMttm ftftftftft —ft—ft—ft- 

-00 -10 -5 0 -1-5 

I» Denormalized a Normalized ninfinityl 

(a) Complete range 


—ft- * -ft n 

-1-10 H-w 


-0 H -0 
\/ 

ft-ft r—ft-ftn- ft ft I ft ft ft i«—♦—ft O «—•—ft ft ft ft i ft ft -r*-ft—, ft-ft 

-1 -0.8 -0.6 -0.4 -0.2 0 -1-0.2 H-0.4 +0.6 +0.6 +1 

I ft Denormalized a Normalized n Infinity! 

(b) Values between -1.0 and +1.0 

Figure 2.34 Representable values for 6-bit floating-point format. There are k — 3 
exponent bits and n = 2 fraction bits. The bias is 3. 












136 Chapter 2 Representing and Manipulating Information 




Exponent 

Fraction 


Value 

Description 

Bit representation 

e 

E 

2^ 

7 

M 

2^ xM 

V 

Pecimal 

Zero 

0 0000 000 

0 

—6 

1 

U 

0 

S 

0 

8 

0 

ST2 

0 

0.0 

Smallest positive 

0 0000 001 

0 

-6 

1 

u 

1 

g 

1 

8 

1 

512 

1 

512 

0.001953' 


0 0000 010 

0 

-,6 

1 

5? 

2 

g 

2 

8 

2 

512 

1 

255 

0.003906 


0 0000 oil 

0 

-6 

1 

U 

3 t 
' g ' 

3 , 
8 

3. 

512 

3 

512 

0.005859 

Largest denormalized 

0 0090 111 

0 

-6 

1 

54 

7 

>1 8 

} 

7 

512 

7 

512 

0.013672 

Smallest normalized 

0 0001*000 

1 

-6 

1 

54 

0 

g 

8 

8 

8 

5|2 

1 

64 

0.015625 


0 0001 001 

1 


1 

5? 

1 

g 

9 

8 

9 

512 

9 

512 

0.01757g' 


0 0110 110 

6 

-1 

1 

2 

6 

8 

14 

T 

14 

T5 

7 

g 

0.875 


0 0,110 111 

6 


1 

2 ' 

7 

■g 

15 

‘.8 

15 

15 

15 

15 1 

0.9375 3 

One 

0 0111 000 

7 


1 

0 

g 

8 

8 

8 , 

'8 

1 

.1.0' 


0 0111 001 

7 

0 

1 

1 

8 

9 

8 

9 

8 

9 

g 

1.125 


0 0111 010 

7 

0 

1 

2 

8 

10 

T 

10 

T 

5 

% 

1.25 


O’l'llO 110 

14 

7 

128 

6 

8 

14 

T 

1792 

-T 

224 

224.0' 

Largest normalized 

01110 111 

14 

'T 

128 

7 

g 

15 

8 

■ '1920 

8 

240 

240.0 

Infinity 

0 1111*'000 

— 

— 

— 

— 


— 

00 

— 


I I { 


Figure 2.35 Example nonnegative^values for 8-bit floating-point format. There are fe 4 exponent bits 
and R = 3 fraction bits. The bias is 7. i r 

1 '< 

represented value V = 2^ x M. Closest to 0 are the denormalized numbers, start¬ 
ing with 0 itself. Denormalized numbers jn this format have E = d — 7 = —6, giv¬ 
ing a weight 2^ = ^. The fractions / and significants Af range over the values 
0, g,..., g, giving numbers V in the range 0 to ^ x g = 

The smallest normalized numbers in this format also have £ = 1 — 7 = —6, 
and the fractions also range over the values 0, g, ... g. However, the significands 
then range from l-l-0 = ltol-l-g = ^, giving numbers V in the range 3x5 = p 

512- 

Observe the smooth transition between the largest denormalized number 3^ 
and the smallest normalized number 5^. This smoothne^sis due to our definition 
of E for denormalized values. By making it 1 — Bias rather than —Bias, we com- 
ifensatefor the fact that the significand of a denormalized number does not have 
an implied leading 1. 


































Section 2.4 Floating.^oint 117 


As we increase the exponent, we get successively larger normalized values, 
passing through 1.0 and then to the largest normalized number, ^is number has 
exponent E = 7, giving a weight 2^ = 128. The fraction equals g, giving a signifi- 
cand M = y. Thus, the numeric value is V = 240. Going beyond this overflows to 
+ 00 . 

One interesting property of this representation is that if we interpret the bit 
representations of the values in Figure 2.35 as unsigned integers, they occur in 
ascending order, as do the values they represent as floating-point numbers. This is 
no accident—the IEEE format was designed so th^it floating-point numbers could 
be sorted using an integer sorting routine. A minor difficulty occurs when dealing 
with negative numbers, since they have a leading 1 and occur in descending order, 
but this can be overcome without requiring floating-point operations to perform 
comparisons (see Problem 2.84). 



Consider a 5-bit floating-point representation l^sed on the IEEE floating-point 
format; with one sign bit, two exponent bits {k = 2), ^nd two fraction bits {n = 2). 
The exponent bias is 2,^7^ - 1,= ,1. 

The table that follows enumerates the entire nonnegative r^nge for this 5-bit 
floating-point representation. Fill in the blank table entries using the following 
directions: 

e\ The value represented by considering the exponent field to bean unsigned 
integer 

E: The value of the exponent after biasing 
2^: The numeric weight of the exponent 
/: The value of the fraction 

.M [ 

M : The value of the significand 

,2f X M: The (unreduced) fractional value of the number 
V: The reduced fractional value of the number 
Decimal: The decimal representation of the number 

Express the 'values of 2®, /, M, 2^ x M, and V either as integers (when 
possible)'or as fractions of the form where y is a power of 2. You need not 
fill in entries marked —. 

Bits e E 2® / M 2^ y. M V 

0 00 00 _ _ _ ____ 

0 00 01 _ _ _ _ _ _ _ 

0 00 10 _ ___ _ _ _ 

0 00 11 _ _ _ _ _ _ _ 


Decimal 







118 Chaf5ter2 Representing and Manipulating Information 


Bits 


0 0101 1 0 

£ %25 . / 


1 

M 



2?:x M V 


5, 

■4 


5 

4 


Decimal 


0 01 10 
0 01 ll 
0 10 00 
0 10 01 
0 10 10 
0 10 11 
0 11 00 
0 11 01 
0 11 10 
0 11 11 


Figure 2.36 shows the representations and hurtieric values of some importanf 
single-’and'd&uble-precision floating-point numbers. As with the 8-bit'format 
shown in Figure 2.35, we can see some general properties for a floating-point 

representation with a /:-bit exponent and an n-bit fraction: ‘ * 

• */ 1 

• The value -1-0.0 always has a bit representation of all zeros. 

, • The smallest positive denormalized value has a bit representation consisting o f 
a 1 in the least significant bit position and otherwise all zeros. It has a fraction 
(and significand) value M = f = 2“" and an exponent value E = —2*“^ -I- 2. 
The numeric value is therefore V = 2“"“^*'’^^. 

• The largest denormalized value has a-‘bit feprese'htatioil consisting of an 
exponent field of all zeros and a fraction field of all ones, It has a'fraction 
(and significand) value M = f = 1 — 2~" (which we have written 1 — e) and 
an exponent value E = —2*^“^ 4- 2. The numeric value is therefore v = (1 — 
2“") X 2“^* which is just slightly smaller: than the smallest normalized 
value. 


I 


Description 

exp 

frac. 

Single precision 

Double precision 

Value 

Decimal 

Value 

Decimal 

Zero 




B3 

0 

0.0 



Smallest denormalized 

00- 



••01 

2-23 ^ 2-126 

1.4 X 10-^5 

2-52 X 2-1022 

4.9 X 10-22“ 

Largest denormalized 

00- 


1- 

• 11 

(1 - e) X 2-126 

1.2 X 10-2S 

(1 - e) X 2-1022 

2.2 X 10-208 

Smallest normalized 

00' 

BbI 

0- 

•00 

1 X 2-12® 

1.2 X 10-2® 

1 X 2-1022 

2.2 X 10-208 

One 

01- 

• 11 

El 

••00 

1x2° 

1.0 

1x2° 

1.0 

Largest normalized 

11- 

•10 

1- 

• 11 

(2-e) x2i22 

3.4 X 10®® 

(2 - e) X 21022 

1.8 X 10208 


Figure 2.36 Examples of nonnegative floating-point numbers. 




























Section 2.4 Floatingpoint 119 


• The smallest positive normalized value has a bit representation with a 1 in 

the least significant bit of the exponent field and otherwise'all zeros. It has a 
significand value M = \ and ah,exponent value E ~ + 2. The numeric 

value is therefore V ^ 

• The value 1.0 has a bit representation with all but the most significant bit of 
the exponent field equal to 1 and all other bits equal to 0. Its significand value 
is Af = 1 and its exponent value is £ = 0. 

• The largest normalized value has a bit representation with a sign bit of 0, the 
least significant bit of the exponent equal to 0, and all other bits equal to 1. It 

t has a fraction value of / = 1 — 2~”, giving a significand Af = 2 — 2~" (which we 
have written 2 — e.) It has an exponent value E = 2*~^ — 1, giving a numeric 
value V = {2 ~ 2-«) x 22*^’“^ = (1 - 2-"-^) x 2^*“'. 

One useful exercisdfor understanding floating-point representations is to con¬ 
vert sample integer values into flo'ating-point form. For example, we saw in Figure 
2.15 tharl2,345 has binhi^y reffresentation [11000000111001]. We create a'normal¬ 
ized representation of this by shiftin'g' 13 positions to the right of a binary point, 
giving 12,345 = 1,10000001110012 x 2^^. To encode this in IEEE single-precision 
format, we>construct the fraction field by dropping'the leading 1 and adding 10 
zeros to the end, giving binary representation [lOOOOOOlTlOOlOOOOOOOOOO]. Tb 
construct the'exponent field, we add bias 127 to 13, giving 140, which has bi¬ 
nary representation [10001100]. We dombine this with a sign bit of 0 to get the 
floating-point representation'in bihAry of [01000110010000001110010000000000]. 
Recall from Sectidii 2.1.3 that we "observed the following correlation in the bit- 
level representations of the integer value 12345 (0x3039) and the single-precision 
floating-point value 12345.0 (0x4640E4O0): 

.0 00 ‘'03039 

OOOOOOOOOOOOOOOOOO11000000111001 
+ ♦*=*! + *****♦*♦ 

4640E400 
01000110010000001110010000000000 

We, can now see that the region of correlation corresponds to the low-order 
bits of the integer, stopping just before the most significant bit equal to 1 (this bit 
forms the implied leading 1), matching the high-order bits in the fraction part of 
the floating-point representation. 




As mentioned in Problem 2.6, the integer 3,510,593 hh's hexadecimal represen¬ 
tation 0x00359141, while the single-precision floating-point numbfer 3(510,593.0 
has hexadecimal representation 0x4A564504. Derive thi§ floating-poiht represen¬ 
tation and explain the correlation between the bits of the integer and floating-point 
representations. 








Chapter 2 Representing ^nd Manipulating Information 



A. a floating-point format with an «-bit fraction,,give,a formula for the 
smallest' positive integer that'^c^nof*be represented exactly (because it 
would require an (n + l)-bit fraction to be exact). Assume the exponent 
field-size k is large enough that the range of representable exponents does 
not>provide a limitationior this problem. 

B. What is the numeric value of this integer for single-pretision format (n = 
23)? 


2.4.4 Rounding 

Floating-point arithmetic can only approximate real arithmetic, since the repre¬ 
sentation has limited range-and precision."Thus, for a value;x,<,wg generally want 
a systematic ijiethod of findingithe. “closest” piatching v^Jue x' that can be rep¬ 
resented in the desired floating-point format. This is the [task of the. rounding 
operation. One key problenj is to define the direction to round a value that is 
halfway between two possibilities. For exampfle, if I have $1.50 and want to round 
}t tq the nearest doll^, should the result be $1 or $2? An alternative, approacli is 
to maintain a lowen and an upper bound on,the actuql number. For example, we 
could determine representable values x~ and such that tl 3 ,e value x is guaran¬ 
teed to lie between tbepi: x~ < x < x'*'. Thp J^EE figatingrpoint, format defines 
four different rounding'nrodes.‘The jdefaujt; method finds a closest match, whil^ 
the other three can be use,d.for computing upper and lowen bounds. 

Figure 2.37 illusti;at,esr,tbe four rovmding modes applied, to the problem of 
rounding a monetary amount to the nearest whole dollar. Round-toreven (also 
called round-to-nearest) is the default mode. It attempts to find a closest match. 
Thus, it rounds $1.40 to $1 and $1.60 to $2; since these are the closest whole dollar 
values. -The only design decision is to determine the effect-of rounding values 
that are halfway between two possible results. Round-to-even mode adopts the 
convention that it rounds the number either upward or downward such that the 
least significant digit of the result is even.i Thus, it rounds both $1.50 and $2.50 
to $ 2 . 

The other three modes’produce guaranteed bounds on the actual value. These 
can be useful’in some numetical applications'.-Round-t'oward-zero mode roundi 
positive numbers downward 'and negative numbers upward, giving a value Jc such 

f 


Mode 

$1.40 

$1.60 

$1.50 

$2.50 

$-1.50 

Round-to-eyen 


$2 

$2 

$2 

$-2 

Round-tqward-zero 

$1 

$1 

$1 

$2 

$-1 

Roundidpwij 

-i!$l 

$1 

$1 

$2 t 

$-2 

Round-up 

$2 

$2 

J$2 

$3 

$-1 


Figure 2.37 Illustration of rounding modes for dollar rounding. The first rounds to 
a nearest value, while the other three bound the result above or below. 





















Section2.4 Floatingpoint 121 


that |i| < Ijt]. Round-down mode rounds both positive and negative numbers 
doyvnward, giving a value x~ such that x~ < x. Round-up mode rounds both 
positive and negative numbers upward, giving a value such that x < 

,Round-to-even at first seems like it h^ a rather arbitrary goal—^why is there 
any reason to prefer even numbers? Why not consistently round values halfway 
between two representable values upward? The problem with such a convention 
is that one can easily imagine scenarios in which rounding a set of data values 
would then introduce a statistical bias into the computation of an average of the 
values. The average of a set of numbers that we rounded by this means would 
be slightly higher than the average of the numbers themselves. Conversely, if we 
always rounded numbers halfway between downward, the average of a set of 
rounded numbers-would be slightly lower than the average of the numbers them¬ 
selves. Rounding toward even numbers avoids this statistical bias in most real-life 
situations. It will round upward about 50% of the time and round downward about 
50% of the time. 

Round-to-even rounding can be applied even when we are not rounding to 
a whole number. We simply consider whether, the least significant digit is even 
or odd. For example, suppose we want to round decimal Tiumbers to the nearest 
hundredth. We would round 1.2349999 to 1.23 and 1.2350001 to 1.24, regardless 
of rounding mode, since they are not halfway between 1.23 and 1.24. On the other 
hand, we would round both 1.2350000 and 1.2450000 to 1.24, since 4 is even. 

Similarly, round-to-even rounding can be applied to binary fractional num¬ 
bers. We consider least significant bit value O td’be even and 1 to be odd. In 
general, the rounding mode is only significant when we have a bit pattern of the 
form XX ■ ■ ■ X.YY ■ ■ ■ FlOO • ■ where X and Y denote arbitrary bit values with 
the rightmost Y being the position to which we wish to round. Only bit patterns 
of this form denote values that are halfway between two possible results. As ex¬ 
amples, consider the problem of rounding values to the nearest quarter (i.e., 2 bits 
to the right of the binary point.) We would round IO.OOOII 2 (2^) down to IO.OO 2 
(2), and IO.OOIIO 2 (2^) up to IO.OI 2 (2^), because these values are not halfway 
between two possible values. We would round IO.IIIOO 2 (2|) up to II.OO 2 (3) and 
10.101002 (2|) down to IO.IO 2 {2^), since these values are halfway between two 
possible results, and we prefer to have the least significant bit equal to zero. 


Show how the following binary fractional values would be rounded to the nearest 
half (1 bit to the right of the binary point), according to the round-to-even rule. 
In each case, show the numeric values, both before and after rounding. 

A. IO.OIO 2 

B. IO.OII 2 

C. IO.IIO 2 

D. II.OOI 2 










122 Chapter 2 Representing and Manipulating Information 


We saw in Problem 2'.46 that the Patriot missile software approximated 0.1 as 
O.OOOIIOOIIOOIIOOIIOOIIOO 2 . Suppose instead that they had used IEEE rqun^- 
td-even motle to detennine aif approximation x' to O.l'with 23 bits to the'right of 
the binary point!* 


A. What is the binary tepresentation of x"? 

B. What is the approximate decimal value of x' — 0.1? 

C. How far off would the computed clock have been after 100 hours of opera¬ 
tion? 

D. How fan off would the program’s prediction of the position of the Scud 
missile have been? 



Consider the following two 7-bit floating-point representations'based on the IEEE 
floating-point f(3nnat.'Neither has a sign bit—they can onlyTepresent nonnegative 
numbers. 


1. Format A 

• There are k = 3 exponent bits. The exponent bias is 3. » 

■ There are « = 4 fraction bits. 

2. Format B 

■ There are k = 4 exponent bits. The exponent bias is 7. ^ 

• There are n = 3 fraction bits. 

t.! 

Below, you are given some bit patterns in format A, and your task is to convert 
them to the closest value^in format B. If necessary, you should apply the roupd^to- 
even rounding rule. In ad(^tion, give the values of numbers given by the format A 
and format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions 
(e.g., 17/64). 

Format A Format B 

Bits Value Bits Value 

011 0000 1 0111000 1 

101 1110 _ __ 

010 1001 ___ 

116 1111 ___ 

000 0001 _ _ _ 


2.4.5 Floating-Point Operations 

The IEEE standard specifies a simple rule for determining the result of an arith¬ 
metic operation such as addition or piultiplication. Viewing floating-point values x 































Section 2.4 Floating Point 123 


and y as real numbers, and some operation O defined over real numbers, the com¬ 
putation should yield Round{x O y), the result of applying rounding to the exact 
result of the real operation. In practice, there are clever tricks floating-point uni t 
designers use to avoid performing this exact computation, since the computation 
need only be sufficiently precise to guarantee a correctly rounded result. When 
one of the arguments is a special value, such as — 0, oo, or NiiN, the standard,spec- 
ifies conventions that attempt to be reasonable. For example, 1/-0 is defined to 
yield -oo, while l/-i -0 is defined to yield +oo. 

One strength of the IEEE standard’s method of specifying the behavior of 
floating-point operations is that it is independent of any particular hardware or 
software realization. Thus, we can examine its abstract mathematical properties 
without considering how it is actually implemented. 

We saw earlienthat integer'addition, both unsigned and two’s complement, 
forms an abelian group. Addition over real numbers also forms an abelian group, 
but we must consider what effect rounding has on these properties. Let us define 
x y to be Round(x + y). This operation is defined for all values of x and y, 
although it may yield infinity even when both x and y are real numbers due to 
overflow. The operation is commutative, with jc y = y +' x for all values of x and 
y. On the other hand, the operation is not associative. For example, with single¬ 
precision floating point the expression (3.14+lelO)-lelO evaluates to 0.0—the 
value 3.14 is lost due to rounding. On the other hand, the expression 3 .14+ (le 10- 
lelO) evaluates to 3.14. As with an abelian group, most values have inverses 
under floating-point 'addition, that is, x —x = 0. Hie exceptions are infinities 
(since -j-oo — oo = NaN), and NaNs, since NaN +' x = NaN for any x. 

The lack of associativity in floating-point addition is the most important group 
property that is lacking. It has important implications for scientific programmers 
and compiler writers. For example, suppose a compiler is given the following code 
fragment: 

X = a + b + c; 
y = b + c + d; 

The compiler might be tempted to save one floating-point addition by generating 
the following code: 

t = b + c; 

X = a + t; 
y •= t + d; 

However, this computation might yield a different value for x than would the 
original, since it uses a different association of the addition operations. In most 
applications, the difference would be so small as to be inconsequential. Unfor¬ 
tunately, compilers have no way of knowing what trade-offs the user is willing to 
make between efficiency and faithfulness to the exact behavior of the original pro¬ 
gram. As a result, they tend to be very conservative, avoiding any optimizations 
that could have even the slightest effect on functionality. 






124 Chapter 2 Representing and Manipulating Information 


On the other hand, floating-point addition satisfies the following monotonicity 
property: ifa>bi then x +'*0 >x+^b for any values 6ia,b, and x other than NaN , 
This property of real (and integer) addition is notTobeyed.by unsigned or'two’s- 
compleipent addition. '3 ' ^ 

FloJiting-point multiplication also obeys many of the properties one normally 
associates with multipUcation’. Let us define .r ** y to be Roundix'x y). This oper¬ 
ation is closed under multiplication '(although possibly yielding infinity or NalV), 
it is commutative, and it has 1.0 as, a multiplicative identity. On the other hand, 
it is not associative, due to the ’possibility tof overflow or' the loss of precision 
due to rounding. For'example, with-single-precision floating-point; the expression 
(le20*le20)*l'e-20 evaluates to -foo; .while -le20*(le20*ie-20) -evaluatesHo 
le20. In addition, floating-point multiplicatipn does not distribute over-addition. 
For-example, with single-precision floating pbint,>the-expression Ie20*(le20- 
le20) evaluates,to 0.0, while Ie20*le20-le20*le26 eval 4 ates to NaM. 

On-the other handf-floating-point multiplication satisfies the following mono¬ 
tonicity properties for any values of a, b, and c other than NaN: 

'U 

* a>b, and r a'*‘ c >b c ' 

i* ^ f r 

a->b and c< 0 =;>a*c<£i*c 

In addition, we are also guaranteed that a a > 0,- as long as ^ NaN. As we 
sgw earlier, none ofjthfse monotonicity,properties hold fpr unsigne 4 or two’s- 
complement multiplication. -i - » 

This lack of associativity and distributivity is of serious concern to scientific 
programmers and to compile^ writers. Even such a,seemingly simple t^k as writing 
code tg determine whether two line^untersect in (hree-dimensiop^l-space,can,be 
a major challenge. 


2.4.6 Floating Point in C 

All versions of C proyicj^ two different floating-point data type?; f loat,and dour 
ble. On machines that support IEEE floating point, these data types correspond 
to single- and double-precision floating point. In addition, the machines use the 
round-to-even rounding mode. Unfortunately, since the C standards do not re¬ 
quire the machine to use IEEE floating point, there are no standard methods to 
change the rounding mode or to get special values such as —0, -|-oo, — 00 , or NaN. 
Most systems provide a combination of include (. h) files and procedure libraries 
to provide access to thesp features, but the, details vary from one system to an¬ 
other. For example, the GNU compiler occ defines program constants INFINITY 
(for -hoo) apd NAN (for NaN) wheij the following sequence p^urs in the prograjn 
file: - 5 , ,,ij 

#define _GNU_S0URCE 1 ‘ 

#include <math.h> 























Section 2.4 Floatingpoint 125 



Fill in the following macro definitions to generate the double-precision values -i-oo, 
-oo, and -0: 


#define P0S_INFINITY 
#define NEG_INFINITY 
#defiiie NEG.ZERO 

You cannot use any include files (such as math. h), buf you can make use of the 
fact that the largest finite number that can be represented with double precision 
is around 1.8.x ■10^’*^. 

: nil 


When casting values between int, float, and double formats, the program 
changes the numeric values and the bit representations as follows (assuming data 
type int is 32 bits): 

• From int to f i^oat, the number cannoj overflow, but it may,be rounded. 

• From int or f loafto double, the exatt numeric value can be preserved be¬ 
cause double has both greater range (i:e.,'the range of representable values), 
as well as greaterprecision (i.e.,' the number of significant bits). 

• From double to^loat, the value can overflow to -l-oo or —oo, since the range 
is smallef. Otherwise, it may be rounded, because the' precision is smaller. 

• From float or double to int, 'the'value will be rounded'toward zero. For 

example, 1.999 will be converted to 1, while -1.999 will fee converted to 
-1. Furthermore, the value may overflow. The C standards'do not specify 
a fixed result for this case. Intel-compatible microprocessors designate the 
bit pattern [10 • 00] {TMin^ for word size w) as an integer indefinite value. 

Any conversion from floating point to iniegejr thht cannot assign a reasonable 
integer approximation yields this value. Thus, the expression (int) +lelO 
yields -21483648, generating a negative,value from a positive one. 



Assume variables X, f, and d are of type int, float, and double, respectively. 
Their values are arbitrary'-^except that neither f nor dequals -foo, —oo, 'or NaN. 
For each of the following C expressions, either argue that it will always be true 
(i.e.,"evaluate to 1) or give a value for the variables such that it is not true (i.e., 
evaluates to 0). 

'■ff 

A. X == (int)(double) x 

B. X ==,<(int) (float) x 

C. d == (double)(float) d 

D. f == (float) (double) f 

E. f==-(-f) 











126 Chapter 2 Representing and Manipulating Information 

F. 1 . 0/2 == 1 / 2.0 

G. d*d>= 0.0 

H. (f+d)-f==d 

2.5 Summary 

Computers encode information as bits, generally organized as sequences of bytes. 
Different encodings are used for representing integers, real numbers, and charac¬ 
ter strings. Different models of computers use different conventions for encoding 
numbers and for ordering the bytes within multi-byte data. 

The C language is designed to accommodate a wide range of different imple¬ 
mentations inderms of word sizes and'numeric encodings. Machines with>64-bit 
word sizes have become increasingly.common, replacing the 32-bit machines that 
dominated the market for around 30 years. Because 64-bit machines can also run 
programs compiled for 32-bit machines, we have focused on the distinction be¬ 
tween 32- and 64-bit programs, rather than machine's. The advantage of 64-bit pro¬ 
grams is that they can go beyond .the 4 GB address limitation'of ‘32-bit programs. 

Most machines encode signed numbers using a two’s-complement representa¬ 
tion and encode floating-point numbers using IEEE Standard 754. Understanding 
these encodings at the bit level, as well as understanding the mathematical char¬ 
acteristics of the arithupietic operations, is important for writing programs that 
operate correctly over the full range of nunieric values. 

When casting between signed and unsigned integers of the same size, most 
C implementations follow the convention that the underlying bit pattern does 
not change. On a two’s-complement machine, this behavior is characterized by 
functions T2 and U2T^, for a uj-bit value. The implicit casting o'f C gives results 
that many programmers do not^anticipate, often leading to program bugs. 

bue to tlie finite lengtlis of the encodings, computer arithmetic has properties 
quite different from conventional integer andVeal arithmetic. The finite length can 
cause numbers to overflow, when they exceed the range of the representation. 
Floating-point values can also underflow, when they are so close to 0.0 that they 
are changed to zero. 

The finite integer arithmetic imple_mented by C, as well as most other pro¬ 
gramming languages, has some peculiar properties compared to true integer arith¬ 
metic. For example, the expression x*x can evaluate to a negative number due 
to overflovv. Noneth,eless, both unsigned and two’s-complement arithmetic satisfy 
many of the other properties of integer arithmetic, including associativity, com¬ 
mutativity, and distributivity. This allows compilers to do many optimizations. For 
example, in replacing the expression 7*x by (x«3)-x, we make use of the as¬ 
sociative, commutative, and distributive properties, along with the relationship 
between shifting and multiplying by powers of 2. 

We have seen several clever ways to exploit combinations of bit-level opera¬ 
tions and arithmetic operations. For example, we saw that with two’s-complement 
arithmetic, ~x+l is equivalent to -x. As another example, suppose we want a bit 

















Bibliographic Notes 127 


Aside Ariane 5: The higfycost of floating-ppint overf)qvy 

Converting large floating-point numbers to integers is a common source of programming errors. Such 
an error hacl disastrous consequence's for the maiden voyage of the Ariane 5 rocket, on June 4,1996. Just 
37 seconds after liffofh’ the rocket veered off its’flight path, broke up, and exploded. Communication 
satellites valued’at $500 million-were oii board the rocket, ^ 

A later investigation [73,33] showed that the'computer controlling the*inertial navigation system 
had sent invalid data to the computer controlling the engine nozzles. Instead of sending flight control 
information, it had sent a diagnostic bit pattern indicating that'an?>^ferflow hadbccurred during the 
conversion of a 64-bit floating-point number to a 16-bit signed integer. 

'The value that overflowed measured the horizontal velocity of the rocket, which could be more 
than five times higher than that achieved by the earlier Ariane 4 rocket. In the design of the Ariane 4 
softwale, they had carefully analyzed the numeric values ancf determined that thd horizontal velocity 
would never overflow a 16-bit number. Unfortunately, they simply reused this part of the software in 
the Ariane 5 without checking the assumptions on which it had been based. 


pattern of the form [0 ,..., 0,1,..., 1], consisting ol w ~ k zeros followed by k 
ones. Such bit patterns are useful for masking operations. This pattern can be gen¬ 
erated by the C expression (l«k)-l, exploiting the property that the desired 
bit pattern has numeric value 2* — 1. For example, the expression (1«8)"1 will 
generate the bit pattern OxFF. 

Floating-point representations approximate real numbers by encoding num¬ 
bers of the form x x 2^. IEEE Standard 754 provides for several different preci¬ 
sions, with the most common being single (32 bits) and double (64 bits). IEEE 
floating point also has representations for special values representing plus and 
minus infinity, as well as not-a-number. 

Floating-point arithmetic must be used very carefully, because it has only 
limited range and precision, and because it does not obey common mathematical 
properties such as associativity. 

Bibliographic Notes 

Reference books on C [45, 61] discuss properties of the different data types and 
operations. Of these two, only Steele and Harbison [45] cover the newer features 
found in ISO C99. There do not yet seem to be any books that cover the features 
found in ISO Cl 1. The C standards do not specify details such as precise word sizes 
or numeric encodings. Such details are intentionally omitted to make it possible 
to implement C on a wide range of different machines. Several books have been 
written giving advice to C programmers [59, 74] that warn about problems with 
overflow, implicit casting to unsigned, and some of the other pitfalls we have 
covered in this chapter. These books also provide helpful advice on variable 
naming, coding styles, and code testing. Seacord’s book on security issues in C 
and C++ programs [97] combines information about C programs, how they are 
compiled and executed, and how vulnerabilities may arise. Books on Java (we 














Chapter 2 Representing and Manipulating Information 

recommend the one coauthored by James Gosling, the creator of the language [5]) 
describe the data formats and arithmetic operations supported by Java. 

Most books on logic design [58,116] have a section op encodings and arith¬ 
metic operations. Such books describe different ways of implementing arithmetic 
circuits. Overton’s book on IEEE floating point [82] provides a detailed descrip¬ 
tion of the format as well as the properties from Jhe perspective of a numerical 
applications programmer. 

I, 

Homework Problems 

2.55 ♦ 

Compile and run the sample code that uses show_bytes (file show-bytes. c) on 
different machines to which you have access. Determine the byte orderings used 
by these machines. < 

2.56 ♦ '■ 

Try running the code for sh.ow„bytes for different sample values. 

2.57 ♦ • u 

Write procedures show_short, show_long, and show_doubie that print the byte 

representations of C objetts of types short, long, and double, respectively. Try 
thes'^out on‘several machines. 

2.58 ♦♦ 

Write a procedure is^little.endian thatAyiH return 1 when compiled and run 
on a little-endian machine, and will return 0 when compiled and run on a big- 
endian machine. This program should run on any machine, regardless of its word 
size. 

2.59 ♦♦ ....... 

Write a C expression that will yield a word consisting of the least significant byte of 
X and thetemaining'bytes of y. For dperands x = 0x89ABCDEF and y = 0x76543210, 
this would give 0x765432EF. 

2.60 ♦♦ . jt • 

Suppose we number the bytes in a w-bit word from 0 (least significant) to w/8 - 1 
(most significant). Write codQ.Jor the following C function, which wilUetujm 
unsigned value in which byte i ot argument x has been replaced by byte b: 

unsigned replace_byte' (unsigned x, ‘int i, un^^igneti char b) ^ 

•Here are some examples showing how the function should work: i 

replace_byte(0x12345678, 2, OxAB) > 0xl2AB5678^ 
replace_byte(0x12345678^, 0, OxAB) > 0xl23466AB ^ 

^it-LeVel Integer Coding Rules 

In several of the following problems, we will artificially restrict what pro’gramming 
constructs you can use to help you gain a better understanding of the bit-level, 


I 




















Homework' Problems 129 


logic, and arithmetic operations.of C. In 
must follow these rules: 


answering'these problems,’your code 


• Assumptions ' 

■ Integers are represented in two’s-complement form'. 

■ Right shifts of signed data are performed arithmetically. 

• Data type int is w bits long. For some of the problems, you will be giveii a 
specific value for w‘, but otlie'rwise your code should work as^ong as ly'is a 
multiple of 8 . You can use the expression sizeof (int)«3'to compute w. 

• Forbidden 

■ Conditionals (if or ?:), loops, switch statements, function calls, and macro 
invocations. 

■ Division, modulus, and niultiplication. 

■ Relative comparison operatQr?,(<, >, <=, and >;=). 

• Allowed operations 

■ All bit-level and logic operations. 

• 'Left and right shifts, but only-with shift amoilnts between 0 and u; 1 •( 

■ Addition and subtraction. 

- Equality (==) and inequality (! =) tests. (Some of the problems do not allow 

■ Integer constants INT_MIN and INT_MAX. 

■ Casting between data types int and unsigned, either explicitly or im¬ 
plicitly. ^ ^ 


Even with these rules, you should try to make your code readable by choosing 
descriptive vanable names and using comments to describe the logic behind-your 
solutions. As an example, the following code extracts the most significant byte 
from mteger argument x; 


/* Get most significant byte from x */ 
int get_msb(int x) -( 

/* Shift by w-8 +/ 

int shift_val = (sizeof(int)-l)«3; 

/* Arithmetic shift */ 

int xright = X » shift_val; 

/+ Zero all but LSB */ 
return xright & OxFF; 


2.61 ♦♦ 

Write C expressions that evaluate to 1 when the following conditions are true and 
to 0 when they are false. Assume x is of type int. 

A. Any .bit of X equals 1. 

B. Any bit of x equals 0. 






130 Chapter 2 Representing and Manipulating Information 


C. Any bit in the-least significant byte of x equals 1. 

D. Any bit in the most significant byte of x equals 0. 

Your code should follow the bit-level integer coding rules (page 128), with the 
additional restriction that you may not use equality (==) o^r inequality (! =) tests. 

2.62 ♦♦♦ , . . ! . 

Write a function int_shifts_are_arithmetic() that yields 1 when run on a 
machine that uses arithmetic right shifts‘for da\a type int and yields 0 otherwise. 
Your code sliould work on a machine with any word size. Test your code on several 
machines. 

2.63 ♦♦♦ ' . . u 

Fill in code for the following C functions. Function srl performs a logical nght 
shift using an arithmetic right shift (given by value xsra), followed 5y other oper¬ 
ations not including right shifts or division. Function sra^performs an arithmetic 
right shift using a logical right shift (given by value xsl-1), followed by other 
operations not including right shifts br division. You may use the computation 
8 *sizeof (int) to determine w, the number of bits in data.type int. The shift 
ambunt k can range from 0 to u; - 1. 

unsigned srl(unsigned x, int k) f * 

/* Perfoji^ shift arithmetically -*/ 

unsigned xsra = (int) x >> k; /. 


int sra(int x, int k) { 

/* Perform shift logically */ 
int xsrl = (unsigned) x » k; 


> 

2.64 ♦ 

Write code to implement the following function; 

/* Return 1 when any odd bit of x equals 1; 0 otherwise. lo 

. Assume ‘W=32 *V > 

int any_odd_one(unsigned x); 

Your function should follow the bit-level integer coding rules '(page 128), 
except that you may assume that data type int has w = 32 bits. 

























I Homework Problems 131 


2.65 

Write code to implement the following function; 

/+ Return 1 when x contains an odd number of Is; 0 otherwise. 

Assume w=32 ♦/ 
int odd_ones(unsigned x); 

Your function should follow the bit-level integer coding rules (page 128) 
except that you may assume that data, type int,has w = 32'bits., t 

Your code should contain a total of at most 12 arithmetic, bitwise, and logical 
operations. 

2.66 ♦♦♦ 

Write code to_ implement the following function: 

/* 

* Generate mask indicating leftmost 1 in x. Ass'ume w=32. 

For,example, OxFF.OO -> OxSOqO, _and 0x6600 —> 0x4000. 

* If X = 0, then return 0. 

*/ 

int leftmost^one(unsigned x); 

^ tt 

Your function should follow the bit-level integer coding rules (page 128) 
except that you may assume that data type int has u; = 32 bits. 

Your code should contain a total of at most 15 arithmetic, bitwise, and logical 
operations. 

Hint: First transform x into.a bit vector of the form [0 • • • 011 ■ • • 1]. 

2.67 ♦♦ 

You are given the task of writing a procedure int_size_is_32() that yields 1 
when run on a machine for which an int? is 32F.its, ahd yields 0 otherwise. You are 
not allowed to use the sizeof operator. Here is a first attempt: 

1 /* The following code does not run properly on some machines */ 

2 int bad_int_size_tis_32() f 

3 /* Set most significant bit (msb) of 32-bit machine */ 

4 int set_msb = 1 « 31; 

5 /* Shift past msb of 32-bit word */ 

6 int beyond_msb = 1 « 32; 

7 

8 /♦ set_msb is nonzero when word size >= 32 

9 beyond_msb is zero when word size <=32 ♦/ 

10 return set_msb && !beyond_msb; 

11 } < ■ 

When com^led and run on a 32-bit SUN SPARC, however, this procedure 
returns 0. The fdllowing compiler message gives us an'indication of the p'roblem- 

Ji o’ 

warning: left shift count >= widths of type 


132 Chapter 2 Representing and Manipulating Information 

A. In what way does our code fail to comply with the C standard? 

B. Modify the code to run properly on any machine for'which data type int is 
at least 32 bits. 

C. Modify the code to run properly on any machine for which data type int is 
at least 16 bits. 

2.68 * 

Write code for a function with thej following prototypes 
/* 

* Mask with least signfleant n bits set to 1 

* Examples: n = 6 —> 0x3F, n = 17 > OxlFFFF 

* Assume 1 <= n <= w 
*/ 

int lower_one_inask(int n) ; 

Your function should'follow the bit-level'integer coding rules (pag’e 128). Be 
careful of the case n = u;. 

2.69 ♦♦♦ 

Write code for a function with the following prototype: 

* / M 

/* 

* Do rotating left shift. Assume 0 <= n < w 

* Examples when x = 0x12346678 and w = 32; 

* n=4’-> 0x23456781', n=20 -> 0x67812345 i 

*1 

unsigned rotate_left(unsigned x, int n); 

t I 

Your function should follow.tKd bit-level integer coding rules (page 128). Be 
careful of the case n = 0. 

2.70 ♦♦ 

Write code for the function with the following prototype: 

/* 

* Return 1 when x can be represented as an n-bit, 2's-complement 
+ number; 0 otherwise 

* Assume 1 <= n <= w 
*/ 

int fits_bits(int x, int n); 

Your function should follow the bit-level integer coding rules (page 128). 

2.71 ♦ ' ^ . 

You jusj started working for a company that is implementing a s^t of procedures 
to opera'te on a data structure where 4 signed bytes are packed into a 32-bit 
unsigned. Bytes within the word .are numbered from 0 (least significant) to 3 




















Homework Problems 133 


i 


(most significant). You have been assigned the task of implementing a function 
for a machine using tWo’s-complement arithmetic and arithmetic right shifts with 
the following prototype: 

/* Declaration of data t 3 ^e where 4 bytes are packed 
into an unsigned */ 
typedef unsigned packed_t; 

/* Extract byte from word. Return as signed integer */ 
int xby,te (packed_t word, int bytenum),; 

That is, the function will extract the designated byte and sign extend it to be 
a 32-bit int. 

Your predecessor (who was fired for incompetence) wrote the following code; 

/* Failed attempt at xbyte */ 

int xbyte(packed_t word, int bytenum) 

{ 

return (word » (bytenum « 3)) & OxFF- 

} 

A. What is wrong with this code? 

B. Give a correct implementation of the function that uses only left and right 
shifts, along with one subtraction. 

2.72 ♦♦ 

You are given the task of writing a function that will copy an'integer val into a 
buffer buf , but it should do so only if enough space is avafiable in the buffer. 

Here is the code you write; 

/* Copy integer into buffer if spacV is available */ 

/* WARNING: The 'following code is buggy */ 
void copy_int(int val/ void *buf, int maxbytes) ■( 
if (maxb 3 rtes-sizeof (val) >= 0) 

meracpy(buf, (void *) &val, sizeof(val)); 


This code makes use of the library function memcpy. Although its use is a bit 
artificial here, where we simply want to copy an int, it illustrates an approach 
commonly used to copy larger data structures. 

You carefully test tfie code and discover that it always copies the value to the 
buffer, even when maxbytes is too small. 

A. Explain why the conditional test in the code always succeeds. Hint: The 
sizeof operator returns a value of type size_t. 

B. Show how you can rewriter-the conditional test to make it work properly. 










134 Chaf)ter2 Representing and Manipulating Information 


2.73 

Write Code-for a function with the following prototype: 

/♦ Addition that saturates to TMin or TMax */ 
int saturating_add(iiit x, int yj; 

Instead of overflowing the way normal two’s-complement addition does, sat¬ 
urating addition returns TMax when there would be positive overflow, and TMm 
when there would be negative overflow. Saturating arithmetic is commonly used 

in programs that perform digital signal processing. 

Your function should follow the bit-level integer coding rules (page 12b). 

2.74 ♦♦ 

Write a function with the following prototype: 

/* Determine whether arguments can he sub^tracted without overflow */ 
int tsub_oV;(int x, int y) ; 

This function should return 1 if the computation x-y does not overflow. 

2.75 ♦♦♦ , - • . u 

Suppose we want to compute the complete 2iu-bit representation of r - y, where 

both X and y are unsigned, on a machine for which data type unsigned is w bits. 

The low-order w bits of the product can be computed with the expression x*y, so 
we only recjuire a proceduije with prototype 

unsigned unsigned_high_prod(unsigned x, unsigned y); 

that computes the high-order w bits of x-y for unsigned variables. ^ ^ 

We have access to a library function with prototype 

iEt sign6d_high_prod.(iiit x, int y); 

that computes the high-order w bits of x ■ y for the case where x and y are in two’s- 
complement form. Write code calling this procedure to implement the function 
for unsigned arguments. Justify the correctness of your solution. 

Hint: Look at the relationship between the signed product x ■ y and the un¬ 
signed product x' • y' in the derivation of Equation 2.18, 

2.76 ♦ 

The library function calloc has the following declaration: 

void *calloc(size_t nmemb, sizeit size); 

According to the library documentation, “The calloc function allocates memo^ 
for an array of nmemb elements of siz'e Bytes each. The memory is set to zero. If ^ 
nmemb or size is z'ko,'theri calloc returns NUllL.” 

Write an im plementation of calloc that performs the allocation by a call to | 
malloc 'ahd sets the memory to zero via memset. Your code shoiJld not have dny ; 
vulnerabilities due to arithmetic overflow, and it should work correctly regardless j 

of the number of bits used to represent data of type s,ize_t. i 

As a reference, functions malloc and memset have the following declarations. | 

















v&id *malloc('size_t size); 

void *raeinset(void *s, int c, size_t n); 


Homework Problems 135 


2.77 ♦♦ 

Suppose we are given the task of generating code to multiply integer variable x 
by various different constant factors it. To be efficient, we want to use only the 
operations +, and «. For the following values of K, write C expressions to 
perform the multiplication using at most three operations per expression. 


A. 

K = 

17 

b: 

K = 

-7 

c., 

K = 

60 

D. 

K = 

-112 

2.78 

♦♦ 



Write code for a function with the following prototype; 

/* Divide by power of 2. Assume 0 <='k < w-1 */ 
iut divide_poH€r2(int x, int k); 

The function should compute x/2* with correct rounding, and it should follow 
the bit-level integer coding rules (page 128). 

2.79 ♦♦ 

Write code for a function mul3div4 that, for integer argument x, computes 3 * 
x/4 but follows the bit-level integer coding rules (page 128). Your‘code should 
replicate the fact that the computation 3*x can cause overflow. 

2.80 

Write code for a function threef ourths that, for integer argument x, computes 
the value of | x, rounded toward zero. It should not overflow. Your function should 
follow the bit-level integer coding rules (page 128). 

2.81 ♦♦ 

Write C expressions to generate the bit patterns that follow, where represents 
k repetitions of symbol a. Assume a w-bit data type. Your code may contain 
references to parameters j and k, representing the values of j and k, but not a 
parameter representing w. 

A. 

B. 0’"-*--'l*0-' 

2.82 ♦ 

We are running programs where values of type int are 32 bits. They are repre¬ 
sented in two’s complement, and they are right shifted arithmetically. Values of 
type unsigned are also 32 bits. 








136 Chapter 2» Representing and Manipulating Information 

We generate arbitrary values x and y, and convert them to unsigned values as 
follows: * 

/♦ Create some arbitrary values */ 
int X = randoijiO; 
int y-= random(); 

/* Convert to unsigned */ 

unsigned ux = (unsigned) x; ' 

unsigned uy = (unsigned) y; 

For each of the following C expressions, you are to indicate whether .or 
not the expression always yields 1. If it always yields 1, describe thp underlying 
mathematical principles. Otherwise, give an example of arguments'that make' it 
yield 0. 

A. (x<y) == (-x>-y) 

B. ((x+y)«4) + y-x =f 17:*y+15*x 
C ~x+“y+l == “(x+y) 

D. (ux-uy) == -(unsigned)(y-x) 

E. ((x » 2) « 2) <= X 

2.83 ♦♦ 

Consider numbers having a binary representation consisting of an infinite string 
of the form O.yyyyyy---, where y is a fc-bit sequence. For example, the binary 
representation of ^ is- 0.01010101 ■ • ■ (y ^ 01 ),~ whUe the representation of | is 
O.OOJlOOlKMlt. ■ - l^y = 0011). 

A. Let Y = BlUkly), that is, the number having binary represehtati<^n y. (jiVe 
a formula in terms of Y and k for the value-represented by the infinite string. 
Hint: Consider the effect of shifting the binary point k positions to the right. 

B, What is the numeric value of the string for the following values of y? 

' (a) 101 

(b) 0110 

(c) 010011 

2.84 ♦ 

Fill in the return value for the following procedure, which tests whether its first 
argument is less than or equdl to its second. Assume the function f 2u returns an 
unsigned 32-bit number having the same bit representation as its floating-point 
argument. You can assume that neither argument is NaN. The two flavors of zero, 
- 1-0 and - 0 , are considered equal. 

int float.le(float x, float y) i 
unsigned ux = f2u(x); 
unsigned uy = f2u(y); 




















j,Homework Problems 137 

/* Get the sign bits */ 
unsigned sx = ux » 31; 
unsigned sy = uy » 31; 

/* Give cin expression using only ux, uy, sx, and sy */ 
return ; 

> 

2.85 ♦ 

Given a floating-point fonnat with a ifc-bit exponent and an «-bit fraction, write 
formulas for the exponent E, the significand M, the fraction /, and the value V 
for the quantities that follow. In addition, describe the bit representation. 

A. The number 7.0 

B. The largest odd integer that can^be represented exactly 

C. The reciprocal of the smallest positive normalized value 

2.86 ♦ 

Intel-compatible processors also support an “extended-precision” floating-point 
format with an 80-bit word divided into a sign bit, I: = 15 exponent bits, a single 
integer bit, and n = 63 fraction bits. The integer bit is an explicit copy of the 
implied bit in the IEEE floating-point representation. That is, it equals 1 for 
normalized values and 0 for denormalized values. Fill in the following table giving 
the approximate values of some “interesting” numbers in this format: 

Description 

Smallest positive denormalized 
Smallest positive normalized 
Largest normalized 

This format can be used in C programs compiled for Intel-compatible ma¬ 
chines by declaring the data to be of type long double. However, it forces the 
compiler to generate code based on the legacy 8087 floating-poiiit instructions. 

The resulting program will most likely run much slower than would be the case 
for data type float or double. * 

2.87 ♦ 

The 2008 version of .the IEEE floatiitg-point standard, named IEEE ,754-2008, 
includes a 16-bit “half-precision” floating-point format^ It was originally devised 
by computer graphics companies for storing data in which a higher dynamic range 
is required than can be achieved with 16-bit integers. This format has ■! sign 
bit, 5 exponent bits {k = 5), and 10 fraction bits (n = 10), The exponent bias .is 
25-1 _ 1 ^ 15. 

Fill in the table that follows for each of the numbers given, with the following 
instructions for each column: 


Extended precision 
Value Decimal 



138 Chapter 2 i Representing and Manipulating Information 

Hex: The four hexadecimal digits describing the encoded form. 

M\ The value of the significand. This should be a number of the form x or 
where x is an integer and y is an integral power of 2. Examples include 0, 
^,and 

E'. The integer value of the exponent. 

V. The numeric value represented. Use the notation or x x V, where x and 
z are integers. 

Z): The (possibly approximate) numerical value, as is' printed uSing the' 
formatting specification of printf. 

As an example, to represent the number g, we would have s = 0, Af = 5, 
and E = -1. Our number would therefore have an exponent field of OIIIO 2 
(decimal value 15 - 1 = 14) and a sigriificaiid field of IIOOOOOOOO 2 , giving a hex 
representation 3B00. The numerical value is 0.875. > 

You need not fill in entries marked —. 

Description 

-i-^ 

-0 

Smallest value^> 2 
512 

Largest denormalized 
—00 

Number with hex 
representation 3BB0 

2.88 ♦♦ ' if 

Consider the following two 9-bit floating-point representations based on the IEEE 
floating-point format. 

1. Format A ^ 

■ T^ere is 1 sign bit. , ^ 

There are jfc = 5 exponent bits. The exponent bias is 15. 

■ There are n = 3 fraction bits. 

2. Format B 

■ There is 1 sign bit. 

■ There are jfc-= 4 exponent bits. The exponent bias is 7. 

■ 'There are n'= 4 fraction'-bits. 

I 

In the following table, you are given some bit patterns in format A, and your 
task is to conyert them-to the;:losest value in format B. If rounding is necessary 
you should round toward -l-oo. In addition, give the values of numbers given by 
the format A and format B bit patterns. Give these as whole numbers (e.g., 17) or 
as fractions (e.g., 17/64 or 17/2®). 



















Homework Problems 


139 


Format A , .Format B 

Bits _ Value _ Bits Value >■£ u 

t 01111 001 5 101110010 ^ 

0 10110 011 ___ 

100111010 _ __^ 

0 00000 111 _ _ _ 

1 11100 000 _ _ _ 

0 10111 100__ 

2.89 ♦ 

We are running programs on a machine where values of type int have a 32- 
bit two’s-complement representation. Values of type float use the 32-bit IEEE 
format, and values of type double use the 64-bit IEEE format. 

We generate arbitrary integer values x, y, and z, and convert them to values 
of type double as follows: 

/* Create some eirbitrary values */ 

Int X = random(); 
int y = randomO; 
int z = randomO; 

/* Convert to double */ 
double dx = (double) x; 

double dy = (double) y; 

double dz = (double) z; 

For each of the following C expressions, you are to indicate whether or 
not the expression always yields 1. If it always yields 1, describe the underlying 
mathematical principles. Otherwise, give an example of arguments that make 
it yield 0. Note that you cannot use an IA32 machine running gcc to test your 
answers, since it would use the 80-bit extended-precision representation for both 
float and double. 

A. (float) X == (float) dx 

B. dx “ dy == (double) (x-y) 

C. (dx + dy) -t dz =»= dx + (dy + dz) 

D. (dx"* dy) * dz == dx * (dy * dz) 

E. dx / dx == dz /,dz 

2.90 ♦ 

You have been assigned the task of writing a C function to compute.a floating- 
pomt representation,of 2-'. decide that the best^way to do this is to directly 

construct the IEEE single-precision representation of the result.' When x is too 
small, ybiir routine-will return 0.0. Wherix'is'too large, it will return'-foo. Fill in the 
blank portions of the code that follows to compute the correct result.'Asslime the 






140 Chapter 2 Representing and Manipulating Information 

function u2f returns a floating-point value having an identical bit representation 
as its unsigned argument. 

float fpwr2(int x) 

{ 

/* Result exponent and fraction */ 
unsigned exp, frac; 
unsigned u; 

if (x < -) f 

/* Too small. Return 0.0 */ 

exp =_; 

frac = _ 

> else if (x <-) f 

/* Denormdlized result */ 

exp = ; 

frac = _; 

> else if (x < -) f 

/* Normalized result. */ 

exp = : 

frac = -: 

]• else { 

/* Too big. Return +oo */ 

exp = _ 

frac = -; 

> i - 
> ! 

/♦ Pack exp and frac into .32 bits ♦/,,{ 
u = exp « 23 I frac; 

/* Rectum as float */ 
return u2f(u); 


I ▼ TH ^ 22 

Around250B.C.,theGreekmatheraaticianArchimedesprovedthat ^ 

Had he had access to a computer and the standard library <math. h>, he would have 
been able to determine that the single-precision floating-point appfoximatioq of 
n has the hexadecimal representation Ox40490FDB. Of course, all of these are jiist 
approximations, since n is not rational. 

A. What is the fractional binary number denoted by this floating-point value? 
B What is the fractional binary representation Hint: See Problem 2.83. 

' ‘ I 

C. At what bit position.(relative to the binary point) do these two approxima¬ 
tions to Tc diverge? i ! • 
















Homework Problems 141 


Bit-Level Floating-Point Coding Rules 

In the following problems, you will write code t6 implement floating-point'func¬ 
tions, operating directly on bit-level representations of floating-point numbers. 
Your code should exactly replicate the conventions for IEEE floating-point oper¬ 
ations, including using round-to-even mode when rounding is required. 

To this end, we define data type float_bits to be equivalent to unsigned: 

* ' i' f. 

/* Access bit—level representation floating—poi^t number */ 
typedef unsigned float_ljits; 


Rather than using data typfe fio'at in yoUr code, you’will use float_bits. 
You may use both int.and unsigned data types, including unsigned and integeV 
constants and operations. You may not use any unions, structs, or arrays. Most 
significantly, you^may not use any floating-point data types, operations, or con¬ 
stants. Instead, your code should perform the bit manipulations that implement 
the specified floating-point operations. 


The following function illustrates the use of these coding rules. For argument 
/, it returns ±0 if / is denormalized (preserving the sign of /), and returns / 
otherwise. 


/* If f is denorm, return 0. Otherwise, return f */ 
float_bits flo^^t_denorm_zero(float^bits f) { 

/*,Decoppose bit representation into parts */ 
unsigned sign = f»31; 
unsigned exp'“ f»23 & OxFF; 
unsigned frac = f & OxTFFFFF; 
if' (exp == 0) { I 

/* Denormalized. Set fraction to 0 */ 
frac = 0; 

> 

/* Reassemble bits */ 

return (sign « 31) I (exp « 23) I frac; 


2.92 ♦♦ 

Following the bit-level floating-point* coding, rules, implement the-function with 
the following prototype: 

/* Compute -f. If f is NaN, then return f. */ 
float_bits float_negafe(float_bits f); 

For floating-point number /, this function computes -/. If / is NaN, your 
function should simply return /. 

Test your function by evaluating it for all 2^2 values of argument f and-com- 
paring the result to what would be obtained using your machine’s floating-point 
operations. 



m 


142 Chapter 2 Representing and Manipulating Information 


2.93 ♦♦ ■ 1, 

i;;ollowing the bit-level floating-point coding rules, implement the function wiUi 

the following prototype: ,, 

/♦ Compute ifl’. If f iWaN, 'thfen return f. */ 
float_bits float_absval(float.bits f); 

For floating-point number /, this function computes |/1. If / is NaN, yom 
function should simply return /. 

Test your function by evaluating it for all 2^^ valufes of argument f and com¬ 
paring the result to what would be obtained usiri^^your. machine’s floating-point 
operations. ^ 

2.94 ♦♦♦ I . . . 

Following the bit-level floating-point coding rule^ implement the funttion with 

the following prototype: 

/* Compute 2*f. If i is NaN, then return f. */ '' 

float^bits !float_twice'(float_bits-f); 

For floating-point number /, this function computes 2.0 • /. If / is NaN, your 
function should simply return /. 

Test your function by evaluating it for aU 2^2 values'^Sf argument f and com¬ 
paring the result to what would be obtained using your machine’s floating-point 
operations. 

2.95 ♦♦♦ . 

Following the bit-level floating-point coding rules, implement the function with 

the following prototype: ’ 

/* Compute 0.5*f. If f is NaW, then return f. */ 
float^bits float.half(float^bits f); 

For floating-point number /, this function computes 0.5 • /.tf / is NaN, your 
function should simply return /. 

Test your function by evaluating it for all 2^2 values of argument f and com¬ 
paring the. result to what would be obtained using your machine’s floating-pdint 
operations. 

2.96 ♦♦♦♦ .• 

Following the bit-level floating-point coding rules, implement the function with 

the following prototype: 


* Compute JOint) 

* If conversion causes overflow or,f is NaN, return 0x80000000 
*/ 

int float_f2i(float_bits f); 






















Solutions to Practice Problems 143 


For floating-point number /, this function computes ‘(int) /. Your function 
should round toward zero. If / cannot be represented as an integer (e.g., it is out 
of range, or it is NaN), then the function should return 0x80000000. 

Test your function by evaluating it for all 2^2 values of argument f and com¬ 
paring the result to what would be obtained using your machine’s floating-point 
operations. 

2.97 ♦♦♦♦ 

Following the bit-level floating-point coding rules, implement the function with 
the following prototype: 

/* Compute (float) i */ 
float_bits float_i2f(int i); 

For argument i, this function computes the bit-level representation of 
(float) i. 

Test your function by evaluating it for all Ip- values of argument f and com¬ 
paring the result to what would be obtained using your machine’s floating-point 
operations. 

Solutions to Practice Problems 

Solution to Problem 2.1 (page 37) 

Understanding the relation between hexadecimal and binary formats will be im¬ 
portant once we start looking at machine-level programs. The method for doing 
these conversions is in the text, but it takes a little practice to becoirie familiar. 

A. 0x39A7F8 to binary: 

Hexadecimal 3 9 a 7 p 8 

Binary 0011 1001 1010 0111 1111 1000 

B: Binary 1100100101111011 to hexadecimal; 

Binary- 1100 1001. 0111 1011 

Hexadecimal C 9 7 b 

C. 0xD6E4C to binary: 

Hexadecinial D 5 e 4 c 

Binary 1101 0101 1110 010() 1100 

D. Binary 1001101110011110110101 to hexadecimal: 

Binary 10 0110 1110 0111 1011 0101 

Hexadecimal 2 6 E 7 B 5 

Solution to Problem 2.2 (page 37) 

This problem gives you a chance to think about powers of 2 and their hexadecimal 
representations, 








144 Chapter 2 Representing and Manipulating Information 


n 

2" (decimal) 

2" (hexadecimal) 

9 

512- 

0x200 

19 

524,288 

0x80000 

14 

16,384 

6 x 4000 

16 

65,536 

0x10000 

17 

131,072 

0x20000 

5 

32 

0x20 

7 

T28 

0x80 


Solution to Problem 2.3 (page 38) . j • , j 

This problem gives you a chance to try out conversions between hexadecimal and 
deciinal representations for some smaller numbers. For larger ones, it becomes 
much.more ppnvenient and reliable to uspa calculatonpr conversion program. 

Decimal Binar y Hexadecimal , 

0 0000 0000 0x00 

167 = 10'16+ 7 1010 0111 0xA7 

62 = 3-16 + 14 0011 1110 0x3E 

188 = 11-16 + 12 1011 1100 OxBC ‘ 

3-16 + 7 = 55 00110111 0x37 

8-16 + 8 = 136 10001000 0x88 ^ 

15-16 + 3 = 243 1111 0011 0xF3 

5-16+ 2 = ^2 
10 -16 + 12 = 172 
14.16 + 7 = 231 


Binary 

0000 0000 
1010 0111 
00111110 
10111100 
0011 0111 
10001000 
nil 0011 

Offil 0010 
‘ 10101100 
1110 0111 


Solution to Problem 2.4 (page 39) 

When you begin debugging Inachine-level programs, you will find many cases 
where some simple hexadecimal,arithmetic would be useful. You can .always 
convert numbers to decimal, perform the arithmetic, and convert them back, but 
being able to work directly in hekadecimdl is more efficient and informative. 

A. 0x503c + 0x8 = 0x5044. Adding 8 to hex c gives 4 with a carry of 1. 

B 0x503c - 0x40 = 0x4f f c. Subtracting 4 from 3 in the second digit position 
■ requires a borrow from the third. Since this digit is 0, we inust also borrow 
from the fourth position. 

C 0x503c + 64 = 0x507c. Decimal 64 (2^) equals hexadecimal 0x40. 

D 0x50ea - 0x503c = Oxae. To subtract hex c '(decimal 12) from hex a (d^mal 
io) we borrow 16 from the second digit, giving hex e (decimal 14). In 
the’second digit, we now subtract 3 from hex d (decimal 13), giving hex a 
(decimal 10). 

Solution to Problem 2.5- (page 48). w. * J 

This problem tests your understanding of the byte representation of data- and the 
two different byte orderings. 





















Solutions to Practice Problems 145 

A. Little endian: 21 Big endian: 87 

B. Little endian: 21’^43 , Big endian: 87 65 

C. Little endian: 21 43 65 Big endian: 87 65 43 

Recall that show.bytes enumerates a series of bytes starting from the one with 
lowest address aricJ working toward the one with fiigheSt address. On a little- 
endian machine, it will list the bytes from least significant to most. On a big-endian 
machine, it will list bytes from the most significant byte to the least. 

Solution to Problem 2.6 (page 49) 

This problem is another chance to practice hexadecijnal to binary conversion. It 
also gets you thinking about integer and floating-point representations. We will 
explore these representations in more detail later in this chapter. 

A. Using the notation bf the example in the text, we write the two strings as 
follows: 

00359141 

OOOOOOOOOOllOlOllOOlOOOlOlOOOOOl 

4A564504 

01001010010101100100010100000100 

B. With the second word shifted two positions to the right relative to the first, 
we find a sequence with 21 matching bits. 

C. We find all bits of the integer embedded in the floatmg-point number, except 
for the most significant bit having value 1. Such is the case for the example 
in the text as wejj. In addition, the floating-point number has some noijzero 
high-order bits that do not match those of the integer. 

Solution to Problem 2.7 (page 49) 

It prints 61 62 63 64 65 66. Recall also that the library routine strlen does not 
count the terminating null character, and so show_bytes printed only through the 
character ‘f’. 

Solution to Problem 2.8 (page 51) 

This problem is a drill to help you become more'familiar iwith Boolean operations. 


Operation 

Result 

a 

[01101001] 

b 

[01010101] 

~a 

[10010110] 

~b 

[10101010] 

akb 

[01000001] 

a 1 b 

[01111101] 

a~ b 

[00111100] 




146 Chapter 2 Representing and Manipulating Information 

Solution to Problem 2.9 {page 53) ‘ 

This problem illustrates how Boolean algebra'can be used to describe and reason 
about real-world systems. We can see that this color algebra,is identical to the 
Boolean algebra over bit vectors of length 3. , 

A. Colors a^ complemented by.t;omplementing th? .values of R,.G, and B. 
From this, we pan see that white is the complement of black, yellojv is the 
complement of blue, magenta is the complpment of green, and cyan is the 
complement of red. 

B. We perform Boolean operations based on a bit-vector representation of t^ 
colors. From this w'e get the following: 

I 

Blue (001) 1 Green (010) = Cyan (Oil) 

Yellow/110) a Cyan (Oil) =i, Creep (010) , 

Red (100) ^ Magenta (101) = Blue (001) 

Solution to Problem 2.10 (page 54) 

This procedure relies on the fact that exclusive-or is comifiutative and associative, 
and that a “ a = 0 for any o. 


Step 


*x 


t' 


Imtially q 

Step 1 a 

Step 2 a~iq~b) = (a ■'q)~i> = b 

Step 3 b / 


1 

a ^ b 

b^~ (a " b) ' b)' a = a 


See Problem 2!ll for a case where thi^function will fail. 


0 

'Ui J i 


Solution to Problem 2.11 (page 55) 

Jhis problem illustrates a subtle„and interesting feature of our inplace swap 
routjne. > 

A. Both first and last have value /c, so we are attempting to swap the middle 
element with itself. _ ^ 

B In this case, arguipents.x apd y .to inplace.swap both point to the sa^e 
location. When we compute *x '■ *y, we get 0. We then store 0 as the middle 
element of the array, and the subsequent steps keep setting this element to 
0. We can see that our reasoning in Problem 2.10 implicitly assumed that x 
and y denote different locations. ^ . 

C. Simply replace the test in line 4 of reverse.array to be first < last, since 
there is no need to swap the middle element with itself. 


Solution to Problem 2.12 (page 55) 
Here are the expressions: 



























A. X ft OxFF 

B. X - ~OxFF 

C. X I OxFF 


Solutions to Practice Problems 147 


These expressions are typical of the kind commonly found in performing low-level 
bit operations. The expression -OxFF creates a mask" where the 8 least-significant 
bits equal 0 and the rest equal 1. Observe that such a mask will be generated 
regardless of the word size. By contrast, the expression OxF^fFFF'oO would only 
work when data type int is 32 bits, 

i '/ 

Soludon to Problem 2.13 (page 56) 

These problems^help you think about the relation between Boolean operations 
and typical ways that programmers apply masking operations. Here is the code: 

/* Declarations of functions implementing operations, bis and bic */ 
int bisCint x, int m); 
int bicCint x, int m); 

/* Compute xly using only calls to functions bis and bic */ 
int bool_or(int x, int y) ■( 

int result = bis(x,y); ’ 

return result;' 

> 

/* Compute x*y using only calls' to functions bis and bic */ 
int bool_xor(int x, Ipt y) < 

Int result = bis(bic(x,y), "bic(y,x)); 
return result; 

} 


The bis operation is equivalent to Boolean on—a bit is set in if either this 
bit is set in x or it is set in m. On the other hand, bic (x, m) is equivalent to x ft ~m; 
we want the result to equal 1 only when the corresponding bit of x is 1 and of m is 

O' , 

Given that, we can implement I with a single call to bis. To implement *, we 
take advantage of the property 

X " y = (x ft ~y) I (~x ft y) 

h 

Solution to Problem 2.|4 (page 57) 

This problem highlights the relation between bit-level Boolean operations and 
logical operations in C. A common programming error is to use a bit-level oper¬ 
ation when a logical one is intended, or vice versa. 





148 Chapter 2 -Representing and Manipulating Information 


Expression 

Value 

Expression 

Value 

x& y 

0x20 

X y 

0x01 

X 1 y 

0x7F 

X 11 y 

0x01 

-X 1 ~y 

OxDF 

lx 1 I !y 

0x00 

x''& ly 

OxOp 

X &a -y 

0x01 


Solution to Problem.2.15 (page 57) 

The expression is ! (x “ y). 

That is, x~y will be zero if and only if every bit of x matches the corresponding 
bit of y. We then exploit the ability of ! to determine whether a word contains any 
nonzero bit. 

Therfe is no re4l reason to use this expression cather'than simply writing x == 
yj but it demonstrates someTof the nuances'bf bit-level and logical operations. 

Solution to Problem'2.16 (page 58) 

This problem is a drill to help you understand the different shift operations. 

Logical Arithmetic 



X 

X « 3 


X » 2 


X » 2 


Hex 

Binary 

Binary 

Hex 

Binary 

Hex 

Binary 

Hex 

0xC3 

[11000011] 

[00011000] 

0x18 

[00110000] 

0x30 

[11110000] 

OxFO 

0x75 

[01110101] 

[10101000] 

0xA8 

[00011101] 

OxlD 

[00011101] 

OxlD 

0x87 

[10000111] 

[00111000] 

0x38 

[00100001] 

0x21 

[11100001] 

OxEl 

0x66 

[01100110] 

[00110000] 

0x30 

[00011001] 

0x19 

[pOOllOOl] 

0x19 


Solution to Problem 2.17 (page 65) 

In general, working through examples for very small word sizes is a very good way 
to understand computer arithmetic. 

The unsigned values correspond to those in Figure 2.2. For the'two’s- 
complement valued, hex digits 0 through 7 have a most significant bit of 0, yielding 
honnegative values, while hex digits 8 through F have a most significant bit of 1, 
yielding a negative valite. 

I ' 


Hexadecimal 

X 

Binary 

B2U4(x) 

B2T4(x) 

OxE 

[1110] 

2^+ 2^ + 2^ = 14 

-2? + 2^ -1- 2' = -2 

0x0 

[0000] 

0 

0 

0x5 

[0101]^ 

2^+ 2° = 5 
* 1 

‘22 + 2° T= 5 

* - J ' 

OxS' 

[iooo]' 

2^ = 8 ,, 

-2? 1= -8, 

OxD 

[1101] 

2^-1-2^-f 2*’= 13 

-2^-1-22 + 20 =-3 

OxF 

[1111] 

2^ + 2^ + 2^ -H 2® = 15 

_23 + 22 + 2^ + 2° = -1 

















Solutions to Practice Problems 149 


Solution taProblem 2.18 (page 69) y 

For a 32-bit word, any value consisting of 8 hexadecimal digits beginning with one 
of the digits 8 through f represents a negative number. It is quite common to see 
numbers beginning with a string of f’s, since the leading bits of a negative number 
are all ones. You must look carefully, though. For example, the number 0x8048337 
has only 7 digits. Filling this out witfra leading zero gives 0x08048337, a positive 
number. 

4004d0: 48 81 ec eO 02 00 00 sub, $0x2e0,i£)rsp A- 736 

4004d7: 48 Sb 44 24 a8 mov f-0x58(%rsp).Xrax B. -88 

4004dc; 48 03 47 28 add 0x28(y.rdi) .‘/.rax C. 40 

4004e0: 48 89 44 24 dO mov %rax,-0x30(%rsp) D. -48 

4004e5: 48 8b 44 24 78 mov 0x78(7,rsp) £. 120 

4004ea: 48 89 87 88 00 00 00 mov '/(rax,0x88(5£rdi) F. 136 

4004fl: 48 8b 84 24 f8 01 00 mov ■0xlf8(%rsp) ,'Xrax a. 504 

4004f8: 00 

4004f9: 48 03 44 24 08 add 0x8(%rsp) ,'4rax 

4004fe; 48 89 84 24 cO 00 00 mov '/,rax,0xc0(%r3p) H. 192 

400505; 00 

400506: 48 8b 44 d4 b8 mov -0x48C%rsp,Xrdx,8),%rax I. -72 

Solution to Problem 2.19 (page 71) 

The functions T2U and U2T are very peculiar from a mathematical perspective. 
It is important to understand how they behave. 

We ^olve this problem by reordering the rows in the solution of Problem 2.17 
according to the two’sTCOnmlement value ^nd then listing the unsigned value as 
the result, of the function application. We show the hexadecimal values to make 
this process more concrete. 


X (hex) 

X 

T2U^{,x) 

0x8 

-8 

8 

OxD 

-3 

13 

OxE 

-2 

14 

OxF 

-1 

15 

0x0 

0 

0 

0x5 

5 

5 


Solution to ^*robiem 2.i?0 .(page 73) 

This exercise tests your understandingjOf Equation 2.5, 

For the first four entries, the valfies of x are negative and TZU^ix} = x -f 2'*. 
For the remaining two entries, the values of x are nonnegative and TZU^ix) =x. 

I 

I Solution tp Problem ^•■^l (page 76) 

This problem .reinforces youx understanding.of the relation between two’s- 
complement and unsigned representations, as well as the effects of the C promo- 
[ tion rules. Recall that TMiri’^i is —2,147,483,648, and that when cast to unsigned it 





150 Chapter'2 Representing and Manipulating Information 


becomes 2,147,483,648. In addition, if either operand is unsigned, then the other 
operand will be cast to unsigned before comparing. 


E xpression _ _ 

-2147483647-1,=“ 2147483648,U 
-2147483647-1 < 2147483647 
-2147483647-lU < 2147483647 
-2147483647-1<-2147483647 
-2147483647-lU <-2147483647 


lype Evaluation 

Unsigned) j 1 

Signed 1 

Unsigned 0 

Signed 1 

Unsigned 1 


Solution to Problem 2.22 (page 79) 

This exercise provides p concrete-demonstration of how sign extension preserves 
the numeric value ofea two’s-complement representation. 


A. [1011] 

B. [11011] 
C [111011] 


^ 2 ^ + 2 ‘ + 2 ° = 

-2^'+2^-f2i + 2“ = 

-2^-|-2'‘-f 2^-l-2^-|-2“ = 


-S + T+l = -5 

-16-f 8-(-2-l-l = 

-32 -f 16 + 8 -i- 2 -f 1 = -5 


Solution to Problem 2:23 (page 80) r 

The expressions in'th&e functions are common program “idioms 

values from a word in which multiple bit fields have been packed Ihey exploit 

Ihe zmb fimng .nd sign-extending properties of the different shift operation. 

Note carefully the ordering otthe cast and shift operations. In fml the sh fts 

are'performed on unsigned'variable word and hence are logical. In fun2,-shifts 

are performed after casting word to int and hence are arithmetic. 


funl (w) 


fuii2(w) 


0x00000076 

0x87654321 

0x000000C9 

0XEDCBA987 


0x00000076 0x00000076 
0x00000021 0x00000021 
0x000000C9 OxFFFFFFCg 
0x00000087 0xFFFFFF87 


B Function funl extracts a value from the low-order 8 bits of the argument, 
' giving an integer ranging between 0 and 255. Function f un2. extracts a value 
from the low-order 8 bits of the argument, but it also performs sign extension. 
The result will be a number betweeh- -128 and 127. 


Solution to Problem 2.24 (page 82) , f 

The effect of truncation is fairly intuitive for unsigned numbers, but not for two s- 

complementnumbers.lliisexerciseletsyouexploreits properties usmgwerysmaU 

word sizes. 


























Solutions to Practice Problems 151 


Hex 

Unsigned 

Two’s complement 

Original 

Truncated 

Original 

Truncated 

Original 

■ Truncated 

0 

0 

0 

b 

0 

0 

2 

2 

2 

2 

2 

2 

9 

1 

9 

1 

-7 

1 

B 

3 

11 

3 

-5 

3 

F 

7 

15 

7 

-1 

’-1 


As Equation 2,9 states, the effect of this truncation on unsigned values is to 
simply find their residue, modulo 8. The effect of the truncation on signed values 
is a bit more complex. According to Equation!,2.10, we first compute the modulo 8 
residue of the argument. This will give values 0 through 7 for arguments 0 through 
7, and also for arguments.—8 thro,ugh' —1. Then we apply "function to these 

residues, giving two repetitions of the sequences 0 through 3 and —4 through — 1. 

Solution to Problem 2.25 (page 83) 

This problem is designed to demonstrate how easily bugs can arise due to the 
implicit casting from signed to unsigned. It seems quite natural to pass parameter 
length as an unsigned, since one would never want to use a negative length. The 
stopping criterion i <= length-1 also seems quite natural. But combining these 
two yields an unexpected outcome! 

Since parameter 1 ength is unsigned, the cornpufation 0 — 1 is performed using 
unsigned arithmetic, which is equivalent to modular addition. The result is then 
UMax. The < comparison is also performed using an unsigned comparison, and 
since any number is less than or equal to UMax, the comparison always holds! 
Thus, the code attempts to access invalid elements of array a. 

The code can be fixed either by declaring length to be an int or by changing 
the test of the for loop to Be i < length. 

Solution to Problem 2.26 (page 83) 

This example demonstrates a subtle feature of unsigned arithmetic, and also the 
property that we sometimes perform unsigned arithmetic without realizing it. This 
can lead to very tricky bugs. 

A. For what cases will this function produce an incorrect result? The function 
will incorrectly return 1 when s is shorter than t. 

B. Explain how this incorrect result comes about. Since strlen is defined to 
yield an unsigned result, the difference and the comparison are both com¬ 
puted using unsigned arithmetic. When,s is shorter than t, the difference 
strlen(s) - strlen(t) should be negative, but instead becomes a large, 
unsigned number, which is greater than 0. 

C. Show how to fix the code so that it will work reliably. Replace the test with 

the following: ,, 


return strlen(s) > strlen(t); 












152 Chapter'2 Representing and Manipulating Information 
Solution'to Problem 2.27 (page 89) 

This function is a direct implementation of the rules given to determine whether 
or not an unsigned addition overflows. 

/* Determine whether arguments can be added without overflow */ 
int uadd_ok(unsigned x, unsigned y) { 
unsigned sum = x+y; 
return sum >= x; 

> 

I 

Solution'to Problem 2.28 (page 89) J 

TTii^problem is a'^imple demonstration Of arithmetic modulo 16. The easiest way 
to solve it is to convertthe hex pattern into its unsigned debimal value. For nonzero 


valufes of a?, we must have (-^ r) + r = 16. Then we convert the complemented 

value back to hex. 





X 

-U r 

r J 


Hex 

Decimal 

Decimal 

Hex 

- 

0 

0 

0 

0 ■' 

^ Ul 

5 

5 

11 

B 

• 

8 

8 " 

8 

8 

r 

D 

13 

15 

3 

a' 


F 

1 

1 



Solution to'Problem 2.29-(page 93) 

This problem is an exercise to make sure you understand twd’s-complement 
addition. 


X 

y 

jc + y 


Case 

-12 

-15 

-27 

5 

1 

[10100] 

[10001] 

[100101] 

[00101] 


-8 

-8 

-16 

-16 

2 

[11000] 

[iiopo] 

[110000] 

[10000] 


_9 

'8" 

-1 

-1 

2‘ 

[10111] 

[01000] 

[111111]' 

[11111] 

'' i 


2 

5 

7 

'7 

3 

[00010] 

[00101] 

[000111], 

[00111.] 

\ 

12 

4 

16 

-16 

4 

[01100] 

[00100] 

[010000] 

[10000] ,, 



4 





























Solutions to Practice'Problems 753 


Solution to Problem 2.30 (page 94) 

^is function is a direct implementation of the rules given to determine whether 
or not a two s-compleme^t addition overflows. 

/* Determine whether, arguments can be added without overflow */ 
int tadd_ok(int x, int y) f 
int sum = x+y; 

int neg_over = x < 0 && y < 0 && sura >= 0; 
int pos.over = x >= 0 && y >= o && sura < 0; 
return !neg_over && !pos_over; 

Solution to Problem 2.31 (page 94) 

Your coworker could'^have learned, by studying Section 2 . 3 . 2 , that two’s- 
complement addition forms an abelian group, and so the expression (x+y)-x 
wiU evaluate to y regardless of whethef or not the addition overflows, and that 
(x+y) -y wiU always evaluate to x. 

Solution to Problem 2.32 (page 94) 

y tliis case, we 

Tveiflow whet” to function tadd.ok will indicate 

overflow when x is negative and no overflow when x is nonnegative. In fact the 

« ^ learned from this exercise is that TMin should be included 

as one .of the cases mrany test procedure for a function. 

Solution to Problem 2.33 (page 95) 

™r/Se.'“‘»o'»-“n>Plsnient negation naing a very small 
For w = 4, we have 7’Mm4 = -g.'so -8 is its own additive inverse while other 

values are negated by integer negation. verse, wniie other 


Decimal 


Decimal 


The bit patterns are the same as for unsigned negation. 


Solution to Problem 2.34 (page 98) 

SinEdo” “ ” twoVcomplemen, 








Chapter 2 Representing and Manipulating Information 


Mode 


X 




X y 

Truncated x • y 

Unsigned 

4 

[100] 

5 

[101] 

20 

fOlOlOO] 

4 

[100] 

Two’s complement 

-4 

[100] 

-3 

[lOT] 

12 

I'OOllOO] 

-4 

[100] 

Unsigned 

’ 2 

[010] 

7 

[111] 

14 

[001110] 

6 

[110] 

Two’s complement 

2 

[010] 

-1 

[111] 

-2 

[111110] 

-2 

[110] 

Unsigned 

6 

[110] 

6 

[110] 

36 

[100100] 

4 

[100] 

TVvo’s complement 

-2 

[110] 

-2 

[110] 

4 

[000100] 

-4 

[100] 


Solution to Problem 2.3S (page 99) 

It is not realistic to test this function for all possible values of x and y. Even if 
you could run lO^billion tesjs per second, it wopld jequire over 58 years to test all 
combinations when (Jata type int is 32 bits.,On the other hand, it is feasijjle to test 
your code by writing the function with data type short or char and then testing 
it exhaustively. 

Here’s a more principled approach, following the proposed set of arguments: 

1. We know that x yican be written as a 2u;-bit two’s-complement number. Let 
u denote the unsigned number represented by the lower w bits, and u denote 
the two’s-complement number represented by the upper w bits: Then, based 
on Equation 2.3, we can see that x ■ y = n2“’ + u. 

'We also know that'W = T2U^{p), since they are unsigned and two’s- 
complement numbers arising from the same bit pattern, and so by. Equation 
2.6, we can write u — p-\- where p^-i is the most significant bit of p. 

Letting i = v + p^_i, we have x • y — p + tT". 

When f = 0, we have x ■y = p; the multiplication does not overfloXv. When 
t ^0,vie have x • y p; the multiplication does overflow. 

2. By 'definition of integer division, dividing p by nonzero x gives a quotient 

q and a remainder r such that p = x ■ + r, and |r| < |x|. (We use absolute 

values here, because the signs of jc and r may differ. For example, dividing —7 
by 2 gives quotient —3 and remainder —1.) 

3. Suppose q = y. Then we have x • y = x • y -f r + ;2“’. From this, we can see 

that r -I- = 0. But lr| < |x| < 2“, and so this identity can hold only if f = 0, 

in which case r = 0. 

Suppose r = f = 0, Then we will have x ■ y ~x • q, implying that y = q. 

When X equals 0, multiplication does not overflow, and so we see that our code 
provides a reliable way to test whether or not two’s-complement multiplication 
causes overflow. 

Solution to Problem;2.36 (pagei99) 

With 64 bits, we can perform the multiplication without overflowing. We then test 
whether casting the product to 32 bits changes the value: 






















Solutions to Practice Problems 155 

1 /* Determine whether the arguments’canibe'multiplied 

2 without overflow */ 

3 int tmult_ok(int x, int y) { 

4 /* Compute product without overflow ♦/ 

5 int64_t pll = (int64_t) x*y; 

6 /* See if casting to int preserves value */ 

7 return pll == (int) pll; 

8 } 

Note that the casting on the right-hand side of line 5 is critical. If we instead 
wrote the line as 


int64_t pll = x*y; 

the product would be computed as a 32-bit value (possibly overflowing) and then 
sign extended to 64 bits. 

Solution to Problem 2.37 (page 99) 

A. This change does not help at all. Even though the computation of asize will 
be accurate, the call to malloc will cause this value to be converted to a 32-bit 
unsigned number, and so the same overflow conditions will occur. 

B. . With malloc having a 32-bit unsigned member as its argument, it cannot 

possibly allocate a block of more than iP- bytes, and.so‘there is no point 
attempting to allocate or copy this much memory. Instead, the function 
should abort and return NULL, as illustrated by the following replacement 
to the original call to malloc (line 9): 

uint64_t required_siz6 = ele_cnl; * (uint64_t) ele^size; 
size_t request_size = (size_t) required_size; 
if (required_size != request_size) 

/* Overflow must have occurred. Abort operation */ 
return NULL; 

void ^result = malloc(requ6St_size); 
if (result == NULL) 

/* malloc failed */ 
return NULL; 

Solution to Problem 2.38 (page 102) 

In Chapter 3, we will see many examples of the lea instruction in action. The 
instruction is provided to support pointer arithmetic, but the C compiler often 
uses it as a way to perform multiplication by small constants. 

For each value of k, we can compute two multiples: 2* (when b is 0) and 2* -f-1 
(when b is a). Thus, we can compute multiples 1,2,3,4,5,8, and 9. 




















'156 Chapter 2 Representing'and Manipulating Information 


Solution to Problem 2.39” (page'103) 

The expression simply becomes -(x«m). To see this, let the word size be w so that 
n = w- l. Form B states that we should compute (x«u;) - , but shifting 

X to the left by w will yield the value' 0. '' 


Solution to Problem 2.40 (page 103) 

This problem requires you to try out the optimizations already described and also 
to supply a bit of your own ingenuity. 


K Shifts Add/Subs 


6 2 1 

31 1 1 

-6 2 1 

55. 2 2 


Expression 

(x«2) + (x«l) 
(x«5) - X 
(x«l) - (x«3) 
(x«6) - (x«3) - X 


Observe that the fourth case uses a modified version of form B. We can view 
the bit pattern [110111] as having a run of 6 ones with a zero in the middle, and so 
we apply the rule for form B, but then we subtract the term corresponding to the 
middle zero bit. i 


Solution to Problem 2.41 (page 103) 

Asshming that additidn and subtraction-have the same performance, thd rul'e is 
to 'choose form A when n=^m, eitherlforifi when n = m +.1, and form B, when 

n > W + 1. ’ 

The‘justification for this rule is asTollows. Assume first that m > 0. When 
n = m, form A requires only a single shift, whileriorm B requires two shifts 
and a subtraction. When n = m +1, both forms require two shifts and either an 
addition-or a subtraction--When n ■> m +1, form ^ requires only two shifts and one 
subtraction, while form A requires n - m + 1 > 2 shiftg..^4<?* -ni>X additions. 
For the case of m = 0, we get one fewer shift for both forms A and B, and so the 
same rules apply for choosing between the twp., 


Solution to Problem 2.42 (page 107) 

The only challenge here is to compute the bias without any testing or conditional 
operations. We use the trick that the expression x »'31 generates a word with all 
ones if x is negative, and all zeros otherwise. By masking off the appropriate bits, 
we get the desired bias value. 


int divl6(int.x). { 

/* Compute bias to .be «i1;her 0 (x >= ’0) or 15 l(x < 0) *7 or 
int bias = (x 31) '& OxF; 
return (x + ‘bias) » 4j 




















Solutions to Practice Problems 

Solution to Problem 2.43 (page 107) 

We havd found that people have difficulty with this exercise -when working di¬ 
rectly with assembly code. It becomes more clear when put in the form shown in 
optarith.. 

We can see that M is 31; x*M is computed as (x«5) -x. 

We can see that N is 8; a bias value of 7 is added when y is negative and the 
right shift is by 3. 

Solut;ion to Problem 2.44 (page 108) 

These ‘C puzzle” problems provide a clear demonstration that programmers must 
understand the properties of computer arithmetic: 

A. (x > 0) II (x-1 < 0) 

False. Let x be —2,147|,483,648 {TMiny 2 ). We will then have x-1 equal to 
2,147,483,647 {TMaX'i2). 

B. (x&7) !=7|| (x«29 < 0) 

True. If (x & 7) ! = 7 evaluates to 0, then we must have bit X 2 equal to 1. 

When shifted left by 29, this will become the sign bit. 

C. (x + x) >= 0 

False. When x is 65,535 (OxFFFF), x*x is -131,071 (OxFFFEOOOl). 

D. X < 0 I I ,-;x ,0. 

True. If X is norinegative, then -x is nonpositive. 

E. X > 0 11 -X >= 0 

False. Let x be —2,147,483,648 (TMinj 2 ). Then both x and -x are negative. 

F. x+y == uy+ux 

True. Two’s-complement and unsigned addition have the same bit-level be¬ 
havior, and they are commutative. 

G. x*~y + Tiy+ux == -X 

True. ~y equals -y-1. uy*ux equals x*y. Thus, the left-hand side is equivalent 
to x*-y-x+x*y. 

Solution to Problem 2.45 (page 111) 

Understanding fractional Binary representations is an important step to under¬ 
standing floating-point encodings. This exercise lets you try out some simple ex¬ 
amples. 

g 0.001 0.125 

I 0,11 0.75 

I 1.1001 1.5625 

^ 10.1011 2.6875 

I 1.001 1.125 

j 101.111 5.875 

51 
IS 


157 


11,0011 3.1875 







158 


Chapter 2 Representing and Manipulating Information 

One simple way to think about fractionatbinary representations is to repre¬ 
sent a-number as a fraction of the form We can w^rite this in. binary using the 
binary representation of x, with the binary point inserted k positions from the 
right. As an example; for’Ig, we have 25^0 = 110012- We then put the binary point 
four positions from the right to get I.IOOI 2 . 

Solution to Problem 2.46 (page 111) 

In most cases, the limited precision of floating-point numbers is not a major 
problem, because the relative error of the computation is still fairly low.'In this 
example, however, the system was sensitive to the absolute error. 

A. We can see that 0.1 - has the binary representation 

0.000000000000000000000001100[1100]- • • 2 

B. Comparing this to the binary representation of ^, we can see that it is simply 

X which is around 9.54 x 10“*. ^ 

C. 9.54 X 10-® X 100 X 60 X 60 X 10 0.343 seconds. 

D. 0.343 X 2,000 f=» 687 meters. 

Solution to i*robiem 2.47 (page 117) 

Working through floating-point representations for very sniall word sizes helps 
clarify how TF , F- F floating point works. Note especially the transition between 
denormahzed and normalized values. ' 

-t' ) 


Bits 

e 

E 

2 ^ 

/ 

M 

2^ X M 

V 

Decimal 

b qo 00 

0 

0 

1 

0 

4 

0 

4 V 

0 

i 3 

0 

0.0 

0 00 01 

0 

0 

1 

1 

4 

1 

3 

1 

4 

1 

4 

0.25 

0 00 10 

0 

0 

1 

2 

3 

2 

3 

2 

4 

1 

2 

0.5 

0 00 11 

0 

0 

1 

3 

4 

3 

4 

3 

3 

3 

3 

'0.75 

0 0100 

1 

0 

1 

0 

4 

4 

4 

4. 

3 

1 

1.0 

0 01 01 

1 

0 

1 

1 

3 

5 

4 

5 

3 

5 

3 

1.25 

0 011 , 0 , 

1 

q 

1 

2 

4 

6 

3 

6 

4 

3 

5 

1.5 

0 0111 

1 

0 

1 

3 

4 

7 

3 

7 

4 

7 

3 

1.75 

0 10 00 

2 

1 

2 

0 

3 

4 

4 

8 

5 

2 

2.0 

0 10 Oi 

2 

1 

2 

1 

3 

5 

4 

10 

4 

5 

2 

2.5 

0 10 10 

2 

1 

2 

2 

4 

6 

4 

12 

T 

3 

3.0 

0 10 11 

2 

1 

2 

3 

3 

7 

3 

14 

4 

7 

2 

3.5 

0 11 00 

— 

— 

— 

— 

— 

— 

00 

— 

0 11 01 

— 

— 

— 

— 

— 

— 

NaN 

' 1 

0 1110 

— 

— 

— 

— 

— 

— 

Nal^ 

— 

0 1111 

— 

— 

— 

— 

— 

— 

NaN 

— 

















’ Solutions to Practice Problems 159 


Solution to Problem 2.48 (page 119) 

Hexadecimal 0x359141 is equivalent to binary [1101011001000101000001] Shift¬ 
ing this right 21 places gives I.IOIOIIOOIOOOIOIOOOOOI 2 x 2?^. We form the frac¬ 
tion field by dropping the leading 1 and adding two zeros, giving 

[ 10101100100010100000100 ] 

^e exponent is formed by adding bias 127 to 21, giving 148 (binary [10010100]). 
e combine this with a sign field of 0 to give a binqry representation 

[01001010010101100100010100000100] 

We see that the matching bits in the two representations correspond to the low- 
order bits of the integer, up to the most significant bit equal to 1 matching the 
high-order 21 bits of the fractiop: 

00359141 

00000000001101011001000101000001 

*******************^:* 

4A664504 

01001010010101100100010100000100 

Solution to Problem 2.49 (page 120) 

Hiis exercise helps you think about what numbers cannot be represented exactly 
m floating point. ^ 

A. The number has binary representation 1, followed by n zeros, followed bv 1 

giving value -f 1. ’ 

B. When n = 23, the value is 2^^* -t-1 = 16,777,217. 

Solution to Problem 2.50 (page 121) 

Performing rounding by hand helps reinforce the idea of round-to-even with 
binary numbers. 

■Original Rounded 

10.0102 2 } loo 2 

IO.OII 2 2l 10.1 2| 

IO.IIO 2 2| 11.0 3 

11.0012 3^ 11.0 3 

Solution to Problem 2.51 (page 122) 

A. Looking at the nonterminating sequence for we see that the 2 bits to the 
right of the rounding position are 1, so a better approximation to ,4 would be 
obtained by incrementing x to get x' = O.OOOflOOllOOllOOllOOllOl, which 
is larger than 0.1. 

B. We can see th^at x' — 0.1 has binary representation 

0.0000000000000000000000000[1100] 







160 Chapter 2 Representing and Manipulating Information 

Comparing this to the binary representation .of we can see that it is 
X A, which'is around 2.38 x lO"®-. 

C. 2.38 X 10“® X 100 X 60 X 60 X lb *=5^ 0.086 seconds, a factor of 4 less than the 
error in the Patriot system. 

D. 0.086 X 2,000 171 meters. 

Solution to Problem 2.52 (page 122) , . , j- 

This problem tests a left of coiicepts about floating-point representations, including 
the encoding of normalized and denormalized >;^u^s, as well as rounding. 


Format A Format B 


Bits 

Value 

Bits 

Value 

Comments 

oil 0000 

1 

0111 000 

1 


101 1110 

15 

2 

1001 111 

15 

2 


010 1001 

25 

32 

0110 100 

3 

3 

Round down 

110 1111 

31 

2 

1011 000 

16. 

Round up 

000 0001 

1 

54 

0001 000 

1 

54 

Denorm -> norm •* 


Solution to Problem 2.53 (page 1,25) 

In general, it is better to use a library macro rather than inventing your own code. 
This cbde seems to work on a variety of machines, however. 

We assume that the valufe le400 overflows to infinity. 

#define POS^INFINITY le400 

Sdefine NEG_INFINITY (-P0S_INFINITY) ^ ^ 

#define NEG^ZERO (- 1 . 0 /P 0 S_INFINITY) 

Solution to Problem 2.54 (page 125) 

Exercises such as this one help you develop your ability to reason about floating¬ 
point operations from a programmer’s perspective. Make sure you understand 
each of the answers. 

A. X == (int)(double) x 

Yes, since double has greater precision and range than int. 

B. X == (int) (float) x 

No. For example, when x is TMax. 

C. d == (double) (f Ip^t) d , ( 

No. For example, wheh'd is^le40, we will get -|-oo on the right. 

D. f == (float)(double)-f 

Yes, since double has greater precision and range than float. 

Yes, since a floating-point number is negated by simply inverting its sign bit. 






















Solutions to Practice Problems 


F. 1.0/2 ==1/2.0 

Yes, the numerators and denominators will both be converted to floating¬ 
point representations before the division is performed. 

G. d*d >=0.0 

Yes, although it may overflow to -i-oo. 

H. Cf+d) -f == d 

No. For example, when f is 1.0e20 and d is 1.0, the expression f+d will be 
rounded to 1.0e20, and so the expression on the left-hand side will evaluate 
to 0.0, while the right-hand side will be 1.0. 




























i 





Machine-Level Representation 
. of Programs 

3.1 A Historical Perspective 166 

3.2 Program Encodings 169 

3.3 ciata F.ormats 177 

3.4 Accessing Information 179 

^.5 Arithmetic and Logical Operations 191 

3.6 Control 200 

3.7 Procedures 238 

3.8 Array Allocation and Access 255 

3.9 Heterogeneous Data Structures 265 

3.10 Control and Data in Machine-Level Programs 276 

3.11 ^ Floating-I^oint Code 293 

3.12 Summary 309 

Bibliographic Notes 310 
Homework Problems 311 

Solutions to,Practice Problems 325' 


163 




164 Chapters Machine-Level Representation of Programs 


C omputers execute machine code, sequences of bytes encoding the low-level 
operations that manipulate data, manage memory, read and write data on 
storage devices, and communicate over networks. A compiler generates machine 
code through a series of stages, based on the rules of the programming language, 
the instruction set of the target machine, and the conventions followed by the op¬ 
erating system. The gcc C compiler generates its output in the form of assembly 
code, a texmal representation of the machine code giving the individual instruc¬ 
tions in the program. Gcc then invokes both an assembler and a linker to generate 
the executable machine code from the assembly code. In this chapter, we will take 
a close look at machine code and its human-readable representation as assem¬ 
bly code. 

When programming in a high-level language such as C, and even more so 
in Java, we are shielded from the detailed machine-level implementation of our 
program. In contrast, when writing programs in assembly code (as was done in the 
early days of computing) a programmer must specify the low-level instructions 
the program uses to carry out a computation. Most of the time, it is much more 
productive and reliable to work at the higher level of abstraction provided by a 
high-level language. The type checking provided by a compiler helps detect many 
program errors and makes sure we reference and manipulate data in consistent 
ways. With modern op ti mizin g compilers, the generated code is usually at least as 
efficient as what a skilled assembly-language programmer would write by hand. 
Best of all, a program written in a high-level language can be compiled and 
executed on a number of different machines, whereas assembly code is highly 
machine specific. 

So why should we spend our time learning machine code? Even though com¬ 
pilers do most of the work in generating assembly code, being able to read and 
understand it is an important skill for serious programmers. By invoking the com¬ 
piler with appropriate command-line parameters, the compiler will generate a file 
showing its output in assembly-code form. By reading this code, we can under¬ 
stand the optimization capabilities of the compiler and analyze the underlying 
inefficiencies in the code. As we will experience in Chapter 5, programmers seek¬ 
ing to maximize the performance of a critical section of code often try different 
variations of the source code, each time compiling and examining the generated 
assembly code to get a sense of how efficiently the {program will run. Furthermore, 
there are times when the layer of abstraction provided by a high-level language 
hides information about the run-time behavior of a program that we need to under¬ 
stand. For example, when writing concurrent programs using a thread package, as 
covered in Chapter 12, it is important to understand how program data are shared 
or kept private by the different threads and precisely Jiow and where shared data 
are accessed. Such information is visible at the machine-code level. As another 
example, many of the ways programs can be attacked, allowing malware to in¬ 
fest a system, involve nuances of the way programs store their run-time control 
information. Many attacks involve exploiting weaknesses in system programs to 
overwrite information and thereby take control of the system. Understanding how 
these vulnerabilities arise and how to guard against them requires a knowledge of 
the machine-level representation of programs. The need for programmers to learn 



















Chapter 3 Machine-Level Representation of Programs 165 

machine code has shifted over the years from one of being able to write programs 
directly in assembly code to one of being able to read and understand the code 
generated by compilers. 

In this chapter, we will learn the details of one particular assembly language 
and see how C programs get compiled into this form of machine code. Reading 
the assembly code generated by a compiler involves a different set of skills than 
writing assembly code by hand. We must understand the transformations typical 
compilers make in converting the constructs of C into machine code. Relative to 
the computations expressed in the C code, optimizing compilers can rearrange 
execution order, eliminate unneeded computations, replace slow operations with 
faster ones, and even change recursive computations into iterative ones. Under¬ 
standing the relation between source code and the generated assembly can often 
be a challenge—it's much like putting together a puzzle having a slightly differ¬ 
ent design than the picture on the box. It is a form of reverse engineering —trying 
to understand the process by which a system was created by studying the system 
and working backward. In this case, the system is a machine-generated assembly- 
language program, rather than something designed by a human. 'Ibis simplifies 
the task of reverse engineering because the generated code follows fairly regu¬ 
lar patterns and we can run experiments, having the compiler generate code for 
many different programs. In our presentation, we give many examples and pro¬ 
vide a number of exercises illustrating different aspects of assembly language and 
compilers. This is a subject where mastering the details is a prerequisite to under¬ 
standing the deeper and more fundamental concepts. Those who say “I understand 
the general principles, 1 don’t want to bother learning the details” are deluding 
themselves. It is critical for you to spend time studying the examples, working 
through the exercises, and checking your solutions with those provided. 

Our presentation is based on x86-64, the machine language for most of the 
processors found in today’s laptop and desktop machines, as well as those that 
power very large data centers and supercomputers. This language has evolved 
over a long history, starting witli Intel Corporation’s first 16-bit processor in 1978, 
through to the expansion to 32 bits, and most recently to 64 bits. Along the way, 
features have been added to make better use of the available semiconductor tech¬ 
nology, and to satisfy the demands of the marketplace. Much of the development 
has been driven by Intel, but its rival Advanced Micro Devices (AMD) has also 
made important contribution;!. The result is a rather peculiar design with features 
that make sense only when viewed from a historical perspective. It is also laden 
with features providing backward compatibility that are not used by modern com¬ 
pilers and operating systems. We will focus on the subset of the features used by 
Gcc and Linux. This allows us to avoid much of the complexity and many of the 
arcane features of x86-64. 

Our technical presentation starts with a quick tour to show the relation be¬ 
tween C, assembly code, and machine code. We then proceed to the details of 
x86-64, starting with the representation and manipulation of data and the imple¬ 
mentation of control. We see how control constructs in C, such as if. while, and 
switch statements, are implemented. We then cover the implementation of pro¬ 
cedures. including how the program maintains a run-time stack to support the 












166 Chapters Machine-Level Representation of Programs 


,4' i- I s»,fe « «■* "* “s 4 *<■ ( 

Web Aside ASM;IA32 1^32 programming^ ^ 

1A32, the 32-bit'predecessor to ^86-^4, jvis‘introduced by.Irfte! 'in„1985. It.sef^e“d asjthe machmfc ’ 
language of choice tot "Several decades? Most x86’micrdprocessors. sold today, "and ihofet operating, 
systems installed on these m£(chihes,.are desired'to'rup x8ff-64.‘Iioy,e’^er, the^' Can'also execute IA32, 
programsnn a baclcward compatibility mode. As a-fesult, mimy application prd^ams are'ltill bas’& on 
IA32. In addition, many existing systems canno?ditecute*x86-64, due *t6iimlfations’of theii^ hardware 
or system software. IA32 coritinues to b’d'an‘itAportdnt*macHine languagd Y&u will find that havihg a ' 
background in x86-64 willdnable you’to^earn the IA32 machine language c^uiteTfeadil^^ * 


passing of data and control between procedures, as well as storage for local vari¬ 
ables. Next, we consider how data structures such as arrays, structures, and xmions 
are implemented at the machine level. With this backgroimd in machine-level pro¬ 
gramming, we can examine the problems of out-of-bounds memory references and 
the vulnerability of systems to buffer overflow attacks. We finish this part of the 
presentation with some tips on using the gdb debugger for examining the run-time 
behavior of a machine-level program. The chapter concludes with a presentation 
on machine-program representations of code involving floating-point data and 
operations. 

The computer industry has recently made the transition from 32-bit to 64- 
bit machines A 32-bit machine can only make use of around 4 gigabytes (2^^ 
bytes) of random access memory, With memory prices dropping at dramatic 
rates, and our computational demands and data sizes increasing, it has become 
both economically feasible and technically desirable to go beyond this limitation. 
Current 64-bit machines can use up to 256 terabytes (2'*® bytes) of memory, and 
could readily be extended to use up to 16 exabytes (2^ bytes). Although it is 
hard to imagine having a machine with that much memory, keep in mind that 
4 gigabytes seemed like an extreme amount of memory when 32-bit machines 
became commonplace in the 1970s and 1980s. 

Our presentation focuses on the types of machine-level programs generated 
when compiling C and similar programming languages targeting modern oper¬ 
ating systems. As a cbnsequence, we make no attempt to describe many of the 
features of x86-64 that arise out of its legacy support for the styles of programs 
written in the early days of microprocessors, when much of the code was writ¬ 
ten manually and where programmers had to struggle with the limited range of 
addresses allowed by 16-bit machines. 


3.1 A Historical Perspective 

The Intel processor line, colloquially referred to as x86, has followed a long evo¬ 
lutionary development. It started with one of the first single-chip 16-bit micropro¬ 
cessors, where many compromises had to be made due to the limited capabilities 
of integrated circuit technology at the time. Since then, it has grown to take ad- 

























Section 3,1 A Historical Perspective 167 


vantage of technology improvements as well as to satisfy the demands for higher 
performance and for supporting more advanced operating systems. 

The list that follows shows some models of Intel processors and some of their 
key features, especially those affecting machine-level programming. We use the 
number of transistors required to implement the processors as an indication of 
how they have evolved in complexity. In this table, “K” denotes 1,000 (10^), “M” 
denotes 1,000,000 (10®), and “G” denotes 1,000,000,000 (10’). 


8086 (1978, 29 K transistors). One of the first single-chip, 16-bit microproces¬ 
sors. The 8088, a variant of the 8086 with an 8-bit external bus, formed 
the heart of the original IBM personal computers. IBM contracted with 
then-tiny Microsoft to develop the MS-DOS operating system. The orig¬ 
inal models came with 32,768 bytes of memory and two floppy drives (no 
hard drive). Architecturally, the machines were limited to a 655,360-byte 
address space—addresses were only 20 bits long (1,048,576 bytes address¬ 
able), and the operating system reserved 393,216 bytes for its own use. 
In 1980, Intel introduced the 8087 floating-point coprocessor (45 K tran¬ 
sistors) to operate alongside an 8086 or 8088 processor, executing the 
floating-point,instructions. The 8087 established the floating-point model 
for the x86 line, often referred to as “x87.” 

80286 (1982, 134 K transistors). Added more (and now obsolete) addressing 
modes. Formed the basis of the IBM PC-AT personal computer, the 
original platform for MS Windows. 

i386 (1985,275 K transistors). Expanded the architecture to 32 bits. Added the 
flat addressing model used by Linux and recent versions of the Windows 
operating system. This was the first machine in the series that could fully 
support a Unix operating system. 

i486 (1989,1.2 M transistors). Improved performance and integrated the float¬ 
ing-point unit onto the processor chip but did not significantly change the 
instruction set. 

Pentium (1993, 3.1 M transistors). Improved performance but only added mi¬ 
nor extensions to the instruction set. 

PentiumPrOj (1995,-5.5 M transistors). Introduced a radically new processor 
design, internally knpwn as the P6 microarchitecture. Added a class of 
“conditional move” instructions to the instruction set. 

Pentium/MMX (1997,4.5 M transistors). Added new class of instructions to the 
Pentium processor for manipulating vectors of integers. Each datum can 
be 1,2, or 4 bytes long. Each vector totals 64 bits. 

Pentium II (1997,7 M transistors). Continuation of the P6 microarchitecture. 

Pentium III (1999,8.2 M transistors). Introduced SSE, a class of instructions for 
manipulating vectors of integer or floating-point data. Each datum can be 
1,2, or 4 bytes, packed into vectors of 128 bits. Later versions of this chip 









168 Chapter 3 Machine-Level Representation of Programs 


went up to 24 M transistors, due to the incorporation of the level-2 cache 
on chip. 

Pentium 4 (2000, 42 M transistors). Extended SSE to SSE2, adding new data 
types (including double-precision floating point), along with 144 new in¬ 
structions for these formats. With these extensions, compilers can use SSE 
instructions, rather than x87 instructions, to compile floating-point code. 

Pentium 4E (2004,125 M transistors). Added hyperthreading, a method to run 
two programs simultaneously on a single processor, as well as EM64T, 
Intel’s implementation of a 64-bit extension to IA32 developed by Ad¬ 
vanced Micro Devices (AMD),-which-we"refer to as x86-64. 

Core 2 (2006, 291M transistors). Returned to a microarchitecture similar to 
P6. First multi-core Intel microprocessor, where multiple processors are 
implemented on a single chip. Did not support hyperthreading. 

Core i7, Nehalem (2008,781M transistors). Incorporated both hyperthreading 
and multi-core, with the initial version supporting two executing pro¬ 
grams on each core and up to four cores on each chip. 

Core i7, Sandy Bridge (2011,1.17 G transistors). Introduced AVX, an exten¬ 
sion of the SSE to support data packed into 256-bit ve’ctors. 

Core i7, Haswell (2013, 1.4 G transistors). Extended AVX to AVX2, adding 
more instructions and instruction formats. 

t 

i Each successive processor has been designed to be backward compatible— 

^ able to run code compiled for any earlier version. As we will see, there are many 

strange artifacts in the instruction set due to this evolutionary heritage. Intel has 
had several names for their processor line, including IA32, for “Intel Architecture 
32-bit” and most recently Intel64, the 64-bit extension to IA32, which we will refer 
to as x86-64. We will refer to the overall line by the commonly used coUoqdial 
name “x86,” reflecting the processor naming conventions up through the i486. 

Over the years, several companies have produced processors that are com¬ 
patible with Intel processors, capable of rurming the exact same machine-level 
programs. Chief among these is Advanced Micro Devices (AMD). For years, 
AMD lagged just behind Intel in technology, forcing a marketing strategy where 
they produfced processors that wbre less expensive although somewhat lower in 
performance. They became more competitive around 2002, being the first to break 
the 1-gigahertz clock-speed barrier for a commercially available microprocessor, 
and introducing x86-64, the widely adopted 64-bit extension to Intel’s IA32. Al¬ 
though we will talk about Intel processors, our presentation holds just as well for 
the compatible processors produced by Intel’s rivals. 

Much of the complexity of x86 is not of concern to those interested m programs 
for the Linux operating system as generated by the gcc compiler. The memory 

I model provided in the original 8086 and its extensions in the 80286 became ob- 

I solete with the i386. The original x87 floating-point instructions became obsolete 

I 

I 

t 

I 

















Section 3.2 Program Encodings 169 


'V. ^ 

Aside Moore's Law 


A 


sf 


« ^ 

Intel microprocessor domplexity 



If we? plot the number of transistors in the different Intel processors versus the year of introduction, and 
* use a logarithmic scale for the y-axis, we can see that the growth has been phenomenal. Fitting a line 
through the data, we see that the number of transistorsincreases at an aimual rate of approximately 
37%, meaning .that the.number‘of transistors doubles about every 26 months. This growth has been 
sustained over the multiple-decade history of x86 microprocessors. 

In 1965, Gordon Moore, a^oiindenof Intel Corporation, extrapolated from the chip technology of 
the day (by which they could fabricate circuits with around 64 transistors on a single chip) to predict 
that the number of transistors per chip would double ey^ry year fer, thq next 10*y;ears. This prediction 
became known as A/oorei Law. As it turns out, his prediction was just a little bit optimistic, but also too 
short-sighted. Qver more^thari'50 years, the semiconductor industry has Ijeen able to double transistor 
cbifnts on average eVery 18 m 9 ntfis. 

Similar expqriential growth rates have occurred for other aspects of computer technology, including 
the storage capacities of magneticdisks and semiconductor mempriea Thes^ remarkable growth rates 
have been the major driving forces of the computer fevolution.* ' 


with the introduction of SSE2. Although we see vestiges of the historical evolu¬ 
tion of x86 in x86-64 programs, many of the most arcane features of x86 do not 
appear. 


3.2 Program Encodings 

Suppose we write a C program as two files pi. c and p2. c. We can then compile 
this code using a Unix command line: 










170 Chapter 3 Machine-Level Representation of Programs 
liniix> gcc -Og -o p pl.c p2.c 

The command gcc indicates the gcc C compiler. Since this is the default compiler 
on Linux, we could also invoke it as simply cc. The command-line option -Og^ 
instructs the compiler to apply a level of optimization that yields machine code 
that follows the overall structure of the original C code. Invoking higher levels of 
optimization can generate code that is so heavily transformed that the relationship 
between the generated machine code and the original source code is difficult to 
understand. We will therefore use -Og optimization as a learning tool and then see 
what happens as we increase the level of optimization. In practice, higher levels 
of optimization (e.g., specified with the option -01 or -02) are considered a better 
choice in terms of the resulting program performance. 

The gcc command invokes an entire sequence of programs to turn the source 
code into executable code. First, the C preprocessor expands the source code to 
include any files specified with #include commands and to expand any macros, 
specified with #define declarations. Second, the compiler generates assembly- 
code versions of the two source files having names pl.s and p2.s. Next, the 
assembler converts the assembly code into binary object-code files pi. o and p2. o. 
Object code is one form of machine code—it contains binary representations of all 
of the instructions, but the addresses of global values are not yet filled in. Finally, 
the linker merges these two object-code files along with code implementing library 
functions (e.g., printf) and generates the final executable code file p (as specified 
by the command-line directive -o p). Executable code is the second form of 
machine code we will consider—it is the exact form of code that is executed by 
the processor. The relation between these different forms of machine code and 
the linking process is described in more detail in Chapter 7. 

3.2.1 Machine-Level Code 

As described in Section 1.9.3, computer systems employ several different forms 
of abstraction, hiding details of an implementation through the use of a simpler 
abstract model. Two of these are especially important for machine-level program- 
.ming. First, the format and behavior of a machine-level program is defined by the 
instruction set architecture, or ISA, defining the processor state, the format of the 
instructions, and the effect each of these instructions will have on the state. Most 
ISAs, including x86-64, describe the behavior of a program as if each instruction is 
executed in sequence, with one instruction completing before the next one begins. 
The processor Hardware is far more elaborate, executing many instructions con¬ 
currently, but it employs safeguards to ensure that the overall behavior matches 
the sequential operation dictated by the ISA. Second, the memory addresses used 
by a machine-level program are virtual addresses, providing a memory model that 


1. This optimization level was introduced in occ version 4.8. Earlier versions of occ, as well as non- 
GNU compilers, will not recognize this option. For these, using optimization level one (specified with 
the command-line flag -01) is probably the best choice for generating code that follows the original 
program structure. 























Section 3.2 Program Encodings 171 


appears to be a very large byte array. The actual implementation of the mem¬ 
ory system involves a combination of multiple hardware memories and operating 
system software, as described in Chapter 9. 

The compiler does most of the work in the overall compilation sequence, 
transforming programs expressed in the relatively abstract execution model pro¬ 
vided by C into the very elementary instructions that the processor executes. The 
assembly-code representation is very close to machine code. Its main feature is 
that it is in a more readable textual format, as compared to the binary format of 
machine code. Being able to understand assembly code and how it relates to the 
original C code is a key step in understanding how computers execute programs. 

The machine code for x86-64 differs greatly from the original C code. Parts of 
the processor state are visible that normally are hidden from the C programmer: 

• The program counter (commonly referred to as the PC, and called %r ip in x86- 
64) indicates the address in memory of the next instruction to be executed. 

• The integer register file contains 16 name^ locations storing 64-bit values. 
These registers can hold addresses (corresponding to C pointers) or integer 
data. Some registers are used to keep track of critical parts of the program 
state, while others are used to hold temporary data, such as the arguments 
and local variables of a procedure, as well as the value to be returned by a 
function. 

• The condition code registers hold status information about the most recently 
executed arithmetic or logical instruction. These are used to implement con¬ 
ditional changes in the control or data flow, such as is required-to implement 
if and while statements. 

• A set of vector registers can each hold one or more integer or floating-point 
values. 

Whereas C provides a model in which objects of different data types can be 
declared and allocated in memory, machine code views the memory as simply 
a large byte-addressable array. Aggregate data types in C such as arrays and 
structures are represented in machine code as contiguous collections of bytes. 
Even’for scalar data types, assembly code makes no distinctions between signed or 
unsigned integers, between different types of pointers, or even between pointers 
and integers. 

The program memory contains the executable machine code for the program, 
some information required by the operating system, a rim-time stack for managing 
procedure calls and returns, and blocks of memory allocated by the user (e.g., by 
using the malloc library function). As mentioned earlier, the program memory 
is addressed using virtual addresses. At any given time, only limited subranges of 
virtual addresses are considered valid. For example, x86-64 virtual addresses are 
represented by 64-bit words. In current implementations of these machines, the 
upper 16 bits must be set to zero, and so an address can potentially specify a byte 
over a range of 2"*®, or 64 terabytes. More typical programs will only have access 
to a few megabytes, or perhaps several gigabytes. The operating system manages 










172 Chapters Machine-Level Representation of Programs 






Aside The evec-^hanging form§^of generated cqd§ „ ^ ^ ^ ^ 

*In our presentation, we'^wiU'show the code gener^ited by a“ particular versiorrfo^Gcc with plrticulaf 
settings of the cqjnmand-line optionk If you coftipile fiode^on your own machine, qh'ances arS you \vill be 
using a different compiler of a different version of 6cc and h&nCe will’ generate different cdd^Thedpen- 
source community supporting dec RSejfe changing the code gerierator,» attempting to%ehSrate‘ more 
efficient code accordingdo’chahging code guidelines providSd by,the microprocessormanufacturers. 

Our goal in studyingdhe’examples sho^’ffi our^resetitationdsdo demoriStrate how,, to examine* 
assembly code and map’ft back“^o the constructs found in high-level prograihm|p| Ihnguhges. Yod will | 
need to adapt these’tecjini^ues to the style of, code geftdfate'd by ;^our pafticulaf bompiler. * I 


this virtual address space, translating virtual addresses into the physical addresses 
of values in the actual processor memory. 

A single machine instruction performs only a very elementary operation. For 
example, it might add two numbers stored in registers, transfer data between 
memory and a register, or conditionally branch to a new instruction address. The 
compiler must generate sequences of such instructions to implement program 
constructs such as arithmetic expression evaluation, loops, or ptocedure calls and 
returns. 

3.2.2 Code Examples 

Suppose we write a C code file mstore. c containing the following function defi¬ 
nition: 

long mult2(long, long); 

void multstoreClong x, long y, long *dest) { 
long t = niult2(x, y) ; 

*dest = t; 

> 

To see the assembly code generated by the C compiler, we can use the, -S 
option on the command line: 

llnux> gcc -Og -S instore, c 

This will cause gcc to run the compiler, generating an assembly file mstore. s, 
and go no further. (Normally it would then invoke the assembler to generate an 
object-code file.) 

The assembly-code file contains various declarations, including the following 
set of lines: ' 

multstore: 

pushq 7.rhx > 



























% ^ 


lyw* 


Section 3.2 Program Encodings 173 


' Aside Ho^ do I display the byte nepoesentatiop^of a program’ ’, ’ 

Tq display the binary object code'fon'^ program (say, mstore),-we use a disassembler (described below) 
j to determine that the code for the,procedure is 14 bytes long. Theh^we rumthe-GNU debugging tool 
j GDB on hie mstore. o and give it the cOnunand ' »■ 

I (gdb) x/14'xb muJtstofe 

^ filing it to display (abbreviated ‘x^) I 4 hex-formattedJaIso!‘x’) bytes starting at the address where 
1 Action multstorq js located. Yop ^yill find that ops hp many useful features fqr.analyzing machine- 

level programs, a? wll be discussed in Section 3.10.2. 


movq 

%rdx. 

%rbx 

call 

mult2 


movq 

%rax. 

(%rbx) 

popq 

%rbx 


ret 




Each mdented line in the code corresponds to a single machine instruction. For 
example, the pushq instruction indicates that the contents of register %rbx should 
be pushed onto the program stack. All information about local variable names or 
data types has been stripped away. 

If we use the -c command-line option, 'gcc will both compile and assemble 
the code 


linux> gcc -Og -c mstore.c 

This will generate an object-code file.mstore. o that is in binary format and hence 
cannot be viewed direcdy. Embedded within4he;l,368 bytes of the file mstore. o 
is a 14-byte sequence with the hexadecimal representation 

53 48 89 d3 e8 00 00 00 00 48 89 03 5b c3 

This is the object code corresponding to the assembly instructions listed previously. 
A key lesson to learn from this is that the program executed by the machine is 
simply a sequence of bytes encoding a sefies of instructions. The machine has 
very little information about the source code from which these instructions were 
generated. 

To inspect the contents of machine-code files, a class of programs known as 
disassemblers can be invaluable. These programs generate a format similar to 
assembly code from the machine code. With Linux systems, the program objdump 
(for “object dump”) can serve this role given the -d command-line flag: 

linux> objdump -d mstore .0 

The result (where we have added line numbers on the left and annotations in 
italicized text) is as follows; 











174 Chapter 3 Machine-Level Representation of Programs 

Disassembly of function sum in binary fi-Ia mstore.o 


1 

0000000000000000 

<multstore>: 




Offset Bytes 


Equivalent assembly language 

2 

0; 53 


push 

%rbx 

3 

1: 48 89 d3 


mov 

7,rdx,%rbx 

A 

4: e8 00 00 

00 00 

callq 

9 <multstore+0x9> 

5 

9; 48 89 03 


mov 

7,rax, (%rbx) 

6 

c: 5b 


pop 

Xrbx 

7 

d: c3 


retq 



On the left we see the 14 hexadecimal byte values, listed in the byte sequence 
shown earlier, partitioned into groups of 1 to 5 bytes each. Each of these groups 
is a single instruction, with the assembly-language equivalent shown on the right. 

Several features about machine code and its disassembled representation are 
worth noting: 

• x86-64 instructions can range in length from 1 to 15 bytes. The instruction 
encoding is designed so that commonly used instructions and those with fewer 
operands requii^e a smaller number j^f bytes than do less common ones or ones 
with more operands. 

• The instruction format is designed in such a way that froip a given starting 
position, there is a unique decoding of the bytes into machine instructions. 
For example, only thcjinstruction pushq 7orbx can start with byte value 53. 

• The disassembler determines the assembly code based purely on the byte 
sequences in the machine-code file. It does not require access to the source or 
assembly-code versions of the program. 

• -The disassembler uses a slightly different‘naming convention for the instruc¬ 

tions than does the assembly code generated by Gcc. In our example^ it has 
omitted the suffix ‘q’ from many of the instructions. These suffixes are size 
designators and can be omitted in most cases. Conversely, the disassembler 
adds the suffix ‘q’ to the call and ret instructions. Again, these suffixes can 
safely be omitted. i 

Generating -the actual executable code requires running a Hnker on the set 
of object-code files, one of which must contain a function main. Suppose in file 
main. c we had the following function; 

#include <stdio.h> 

void multstoreClong, long, Tong *); 

int mainO { 
long d; 

multstore(2, 3, &d); 
printf("2 * 3 --> ‘/.IdNn", d) ; 
return 0; 


> 





















Section 3.2 Program Encodings 175 


long mult2(long a, long b) { 
long s = a * b; 
return s; 

} 

Then we could generate an executable program prog as follows: 
linux> gcc ~0g -o prog main.c mstore.c 

The file prog has grown to 8,655 bytes, since it contains not just the machine 
code for the procedures we provided but also code used to start and terminate 
the program as well as to interact with the operating system. 

We can disassemble the file prog; 

linux> objdump —d prog 

The disassembler will extract various code sequences, including the following: 

Disassembly of function sum in binary file prog 

1 0000000000400540 <niultstore>; 

2 400640: 53 

3 400541: 48 89 d3 

4 400544: e8 42 00 00 00 

5 400549: 48 89 03 

6 40054c: 5b 

7 40054d: c3 

8 400546: 90 

9 40054f; 90 

This code is almost identical to that generated by the disassembly of mstore. c. 
One important difference is that the addresses listed along the left are different— 
the linker has shifted the location of this code to a different range of addresses. A 
second difference is that the linker has filled in the address that the callq instruc¬ 
tion should use in calling the function mult 2 (line 4 of the disassembly). One task 
for the linker is to match function calls with the locations of the executable code for 
those functions. A final difference is that we see two additional lines of code (lines 
8-9). These instructions wiU have no effect on the program, since they occur after 
the return instruction (line 7). They have been inserted to grow the code for the 
function to 16 bytes, enabling a better placement of the next block of code in terras 
of memory system performance. 

3.2.3 Notes on Formatting 

The assembly code generated by gcc is difficult for a human to read. On one hand, 
it contains information with which we need not be concerned, while on the other 
hand, it -does not provide any description of the program or ho<v it works. For 
example, suppose we give the command 


push %rbx 

mov Xrdx,%rbx 

callq 40058b <mult2> 

mov Xrax,(Xrbx) 

pop %rbx 

retq 

nop 

nop 


linux> gcc -Og ~S mstore.c 








176 Chapter 3 Machine-Level Representation of Programs 

to generate the file mstore. s. The full content of the file is as follows; 

.file "010-mstore.c" 

• text 

.globl multstore 
.type multstore, ©function 
multstore: 

pushq %rbx 

movq 7.rdx, Xrbx 

call mult2 

movq Xrax, (%rbx) 

popq %rbx 

ret 

.size multstore, .-multstore 

• ident "GCC: (Ubuntu 4.8. l-2ubuiitul-12.04) 4.8.1" 

. section .note .GNU-stack, ,eprogbits 

All of the lines beginning with ‘. ’ are directives to guide the assembler and 
hnker. We can generally ignore these. On the other hand, there are no explanatory 
remarks about what the instructions do or how they relate to the source code. 

To provide a clearer presentafion of assembly code, we will show it in a form 
that omits most of the directives, while including line numbers and explanatory 
annotations. For our fexample, an annotated version would appear as follows; 

void aultstoreClong x, Jong y, long *dest) 

X in Xrdi, y in %rsi, dost in Xrdx 


1 

multstore 



2 

pusbq 

Xrbx 

Save Zrbx 

3 

movq 

%rdx, 7,rbx 

Copy desp to %rbx 

4 

call 

mult 2 

Call mult2(f, y) 

5 

movq 

,%yax, (7.rbx) 

.Store result at *dast 

6 

popq 

%rbx 

;Restore %rbx 

7 

ret 


Return 


We typically show only the lines of code relevapt to the point being discussed. 
Each line is numbered on the left for reference and annotated on the right by a 
brief description of the effect of the instruction and how it relates to the computa¬ 
tions of the original C code. This is a stylized version of the way assembly-language 
programmers format their code. 

We also p rovide Web asides to cover material intended for dedicated machine- 
language enthusiasts. One Web aside describes IA32 machine code. Having’a 
background in x86-64 maizes ^learning IA32 fairly simple. Another Web aside gives 
a brief presentation of ways to incorporate assembly code into.C.programs. For 
some applications, the programmer must drop down-to assembly code to access 
low-level features of the machine. One approach is to write entire fimctions in 
assembly code and combine them with C functions during the linking stage. A 



























Section 3.3 Data Formats 177 


Aside ATT versus Iptel assem^ly-cofie for[nats 

In our pres*entatipn, jve show assembly code, in ATr,{orjnat (named after AT&T^ the coippany.that 
operated Bell Laboratories for many years), the default format for Gcc, oatpUMP, and the other tools y<e 
will consider. Other programming'tQols.Jncludipg those from Microsoft as well,^s the documentation 
fromTntel, shoty assembly code' in Intel format. -The two formats differ in a nupiber o| ways. As an 
example, ocp pan generate cdde in Intel format for the sum function rising the following command line; 

linuX> gcc -S” -masm=intel mstore^.c 


This giv^s,the foUojving assembly cpde: 


multstore: 


push 

rbx 

mov 

rbx, rdk 

'call 

mult2 

•.-v 

mov 

QWORD PTR' [rbx] 

pop 

rbx 

ret 







ft 




We see ^aWhe'Intel and ATT foi^ats differ in the following ways: 


• The Intel cpdepiruts the size designation suffixes, Wp §ee instruction push and,mov instead of pushq 
and movq., 

• The Intel code omits character in frjjnt pf r^gistey,names, using rb» instead of Xrbx. 

•'TheJntel code has a^’different way'qCopscribihg locations ^in-'memory—for example, QWQRD PTR 
[rbx] rather |han (Xrbx). fe ’ ■» 

• Instructiohs vnth multiple operands list them in the reverse order. This can t>e very confusing when 

s^tching between the two formats. • ” 

/^though we-will not be“usingTntel format in our presentation, ydu will encountfep jf in documentation 
from Intel aqd Microsoft 


second is to use Gcc’s support for embedding assembly code directly within C 
programs. 

33 Data Formats 

Due to its origins as a 16-bit architecture that expanded into a 32-bit one, Intel 
uses the term “word” to refer to a 16-bit data type, Based on this, they refer to 32- 
bit quantities as “double words,” and 64-bit quantities as “quad words.” Figure 3.1 
shows the x86-64 representations used for the primitive data types of C. Standard 
int values are stored as double words (32 bits). Pointers (shovra here as char *) 
are stored as 8-byte quad words, as would be expected in a 64-bit machine. With 
x86-64, data type long is implemented with 64 bits, allowing a very wide range 
of values. Most of our code examples in this chapter use pointers and long data 




178 Chapter 3 Machine-Level Representation of Programs 


Web Aside ASM:EASM Combining assembly cddeWth Cprograms * 

Although a C compiler does’a good job of converting’ the comiiutations expressed in a program into | 
machine code, there are some features of a machine that cannot be accessed'by'a.C program. For « 
exanlple, every time an x86-64 processor executes an-afithmetic oV logical operation, it sets a 1-bit | 
condition code flag, named PF (for “parity flag”), to 1 when the lower 8 bits in the resulting computation | 
have an even number of ones and to 0 otherwise.' Computing this information in C requires at least j 
seven shifting, masking, and exclusive-or operations (spe Problem 2.65), Eveif though the hardware 
performs this computation as part of’every arithmetic or logical operation, there is no way for a C | 
.program to determine the value of the PF condition code flag. This task can readily be performed by 1 
incorporating a small number of assembly-code instructions into the program. ^ 

There are two ways to incorporate assembly code into C programs. First, we can.write an»entire * 
function as a separate’assembly-code file and let the assembler and linker combine’this, with code we 
have written in C. Second, we can use the inline assemply feature of gcc, where brief sechpns of as|embly j 
code can be incorporated into d C prograhi using the asm directive. This approach has the advantage ^ 
that it minimizes the amount of machine-specific code. I 

Of course, including assembly code in a C program makes the code specific to a particular class of | 
machines (such as x86-64), and so it should only be used when the de^ed feature can only be accessed f 
in this way. ^ * I 


C declaration 

Intel data type 

Assembly-code suffix 

Size (bytes) 

ch2ir 

Byte 

b 

1 

short 

Word 

w 

2 

int 

Double word 

1 

4 

long 

Quad word 

q 

8 

char * 

Quad word 

q 

8 . 

float 

Single precision 

S 

4 

double 

Double precision 

1 

8 


Figure 3.1 Sizes of C data types in x86-64. With a 64-bit machine, pointers are 8 bytes 
long. 


types, and so they will operate on quad words. The x86-64 instruction set includes 
a full complement of instructions for bytes, words, and double words as well. 

Floating-point numbers come in two principal formats; single-precision (4- 
byte) values, corresponding to C data type float, and double-precision (8-byte) 
values, corresponding to C data type double. Microprocessors in the x86‘family 
historically implemented all floating-point operations with a special 80-bit (10- 
byte) floating-point format (see Problem 2.86). This format can be specified in C 
programs using the declaration long double. We recommend against using this 
format, however. It is not portable to other classes of machines, and it is typically 


















Section 3.4 Accessing Information 179 


not implemented with the same high-performance hardware as is the case for 
single- and double-precision arithmetic. 

As the table of Figure 3.1 indicates, most assembly-code instructions gener¬ 
ated by Gcc have a single-character suffix denoting the size of the operand. For 
example, the data movement instruction has four variants: movb (move byte), 
raovw (move word), movl (move double word), and movq (move quad word). The 
suffix ‘1’ is used for double words, since 32-bit quantities are considered to be 
“long words.” The assembly code uses the suffix ‘1’ to denote a 4-byte integer as 
well as an 8-byte double-precision floating-point number. This causes no ambigu¬ 
ity, since floating-point code involves an entirely different set of instructions and 
registers. 


3.4 Accessing Information 

An x86-64 central processing unit (CPU) contains a set of 16 general-purpose 
registers storing 64-bit values. These registers are used to store integer data as well 
as pointers. Figure 3.2 diagrams the 16 registers. Their names all begin with %r, but 
otherwise follow multiple different naming conventions, owing to the historical 
evolution of the instruction set. The original 8086 had eight 16-bit registers, shown 
in Figure 3.2 as registers '/iax through %bp. Each had a specific purpose, and hence 
they were given names that reflected how they were to be used. With the extension 
to IA32, these registers were expanded to 32-bit registers, labeled %eax through 
%ebp. In the extension to x86-64, the original eight registers were expanded to 64 
bits, labeled %rax through °/.rbp. In addition, eight new registers were added, and 
these were given labels according to a new naming convention: %r8 through 7.rl5. 

As the nested boxes in Figure 3.2 indicate, instructions can operate on data 
of different sizes stored in the low-order bytes of the 16 registers. Byte-level 
operations can access the least significant byte, 16-bit operations can access the 
least significant 2 bytes, 32-bit operations can access the least significant 4 bytes, 
and 64-bit operations can access entire registers. 

In later sections, we will present a number of instructions for copying and 
generating 1-, 2-, 4-, and 8-byte values. When these instructions have registers as 
destinations, two conventions arise for what happens to the remaining bytes in 
the register for instructions that generate less than 8 bytes: Those that generate 1- 
or 2-byte quantities leave the remaining bytes unchanged. Those that generate 4- 
byte quantities set the upper 4 bytes of the register to zero. The latter convention 
was adopted as part of the expansion from IA32 to x86-64. 

As the annotations along the right-hand side of Figure 3.2 indicate, different 
registers serve different roles in typical programs. Most unique among them is the 
stack pointer, %rsp, used to indicate the end position in the run-time stack. Some 
instructions specifically read and write this register. The other 15 registers have 
more flexibility in their uses. A small number of instructions make specific use of 
certain registers. More importantly, a set of standard programming conventions 
governs how the registers are to be used for managing the stack, passing function 














180 Chapter 3 Machine-Level Representation of Programs 



J 




I ^ 
i 


! 


L 

1 


r 

1 

1 


I 




I 



Iris XiflOd 

^ ^ ^ • {■ IT 1 1 y .» 

~ * \» T~S r'¥ 'V Ad 
I Xfl 1 '*■>•'1^ 'V|s> ^ «* •* 

• --;■ „ '* 3cs„'« ^ 

L&r,12 > .»■ .Si 


"S' * *‘”■4^*'®* ” ^ *s»' ' 

y,ri^, * ^ • 

... « ■*:!#» ».«• »W« 

%ri3d* ’/.rlSH r~ ‘/.rl3b | 

-- -H •* 'ii; ^ * ij» w -"H 

Lf .dv't* *• 'I"« 

7,rl4d '/.rldw | */.rl4b | 


4rd^ V4f« ''■^P « 


1, V.rl5d 


Figure 3.2 Integer registers. The low-order portions of all 16 registers can be accessed 
as byte, word (16-bit), double word (32-bit), and quad word (64-bit) quantities. 


arguments, returning values from functions, and storing local and temporary data. 
We will cover these conventions in our presentation, especially in Section 3.7, 
where we describe the implementation of procedures. 

3.4.1 Operand Specifiers 

Most instructions have one or more operands specifying the source values to use 
in performing an operation and the destination location into which to place the 



















Section 3.4 Accessing Information 181 


Type 

Form 

Operand value 

Name 

Immediate 

%lmm 

Imm 

Immediate 

Register 



Register 

Memory 

Imm 

M[Imm] 

Absolute 

Memory 

(r„) 

M[R[rJ 

Indirect 

Memory 

ImmiTh) 

M[Imm R[r^]] 

Base + displacement 

Memory 


M[R[ri] + R[r,]] 

Indexed 

Memory 

Immixtj.Xi) 

M[Imm 4- R[r*] -f R[r,]] 

Indexed 

Memory 

(,ri,j) 

M[R[r,] ■ i] 

Scaled indexed 

Memory 

Imm(.,Tj ,s) 

M[/mm -h R[r,] ■ s] 

Scaled indexed 

Memory 

(r*,r;,j) 

+ R[r,] ■ •?] 

Scaled indexed 

Memory 

lmm(.Tj,,Ti,s) 

M[/mm -h R[r*] + R[r,] • j] 

Scaled indexed 


Figure 3.3 Operand forms. Operands can denote immediate (constant) values, register 
values, or values from memory. The scaling factor s must be either 1, 2, 4, or 8. 


result. x86-64 supports a number of operand forms (see Figure 3.3). Source values 
can be given as constants or read from registers or memory. Results can be stored 
in either registers or memory. Thus, the different operand possibilities can be 
classified into three types. The first type, immediate, is for constant values. In ATT- 
format assembly code, these are written with a followed by an integer using 
standard C notation—for example, $-577 or $0xlF. Different instructions allow 
different ranges of immediate values; the assembler will automatically select the 
most compact way of encoding a value. The second type, register, denotes the 
contents of a register, one of the sixteen 8-, 4-, 2-, or 1-byte low-order portions of 
the registers for operands having 64, 32,16, pr 8 bits, respectively. In Figure 3.3, 
we use the notation to denote an arbitrary register a and indicate its value with 
the reference R[rjj], viewing the set of registers as an array R indexed by register 
identifiers. 

The third type of operand is a memory reference, in which we access some 
memory location according to a computed address, often called the effective ad¬ 
dress. Since we view the memory as a large array of bytes, we use the notation 
MhlAddr] to denote a reference to the &-byte value stored in memory starting at 
address Addr. To simplify things, we will generally drop the subscript b. 

As Figure 3.3 shows, there are many different addressing modes allowing dif¬ 
ferent forms of memory references. The most general form is shown at the bottom 
of the table with syntax Imm (r^, r,-, x ). Such a reference has four components: an 
immediate offeet Imm, a base-register r^,, an index register r,-, and a scale factor 
s, where s must be 1, 2, 4, or 8. Both the base and index must be 64-bit registers. 
The effective address is computed as Imm -t- R[ri] -h R[r,] • s. This general form is 
often seen when referencing elements of arrays. The other forms are -simply spe¬ 
cial cases of this general form where some of the components are omitted. As we 















I ' j 


182 Chapters Machine-Level Representation of Programs 

will see, the more complex addressing modes are useful when referencing array 
and structure elements. 


Assume the following values are stored at the indicated memory addresses and 


registers: 

Address 

Value 

Register 

Value 

0x100 

OxFF 

‘/.rax 

0x100 

0x104 

OxAB 

"/.rex 

0x1 

0x108 

0x13 

"/.rdx 

0x3 

OxlOC 

0x11 




Fill in the following table showing the values for the indicated operands: 


Operand 

'/.rax 

0x104 

$0x108 

('/.rax) 

4 ('/.rax) 
9('/.rax,'/.rdx) 
260('/.rcx,'/.rdx) 
0xFC( ,’/,rcx,4) 
('/.rax, '/.rdx, 4) 


3.4.2 Data Movement Instructions 

Am ong the most heavily used instructions are those’that copy data from one lo¬ 
cation to another. The generality of the operand ^rotation allows a simple data 
movement instruction to express a range of possibilities that in many machines 
would require a number of different instructions. We present a number of differ¬ 
ent data movement instructions, differing in their source and destination types, 
what conversions they perform, and other side effects they may have. In our pre¬ 
sentation, we group the many different instructions into instruction classes, where 
the instructions in a class perform the same operation but with different operand 
siz6i$* 

Figure 3.4 lists the simplest form of data movement instructibns— mov class. 
These instructions copy data from 'a source location to a destination location, 
without any transformation. The class consists of four instructions: movb, mow, 
movl, and movq..All four of these'instructions have simUar effects; they differ 
primarily, in that they operate on data of different sizes: 1, 2, 4, and 8 bytes, 
respectively. 
































Section 3.4 Accessing Information 183 


Instruction 


Effect 

Description 

MOV 

S,D 

D S 

Move 

movb 



Move byte 

mow 



Move word 

movl 



Move double word 

movq 



Move quad word 

movabsq 

I,R 

R ^ I 

Move absolute quad word 


Figure 3.4 Simple data movement instructions. 


The source operand desi^ates a value that is immediate, stored in a register, 
or stored in memory. The destination operand designates a location that is either a 
register or a memory address. x86-64 imposes the restriction that a move instruc¬ 
tion cannot have both operands refer to memory locations. Copying a value from 
one memory location to another requires two instructions—the first to load the 
source value into a register, and the second to write this register value to the des¬ 
tination. Referring to Figure 3.2, register operands for these instructions can be 
the labeled portions of any of the 16 registers, where the size of the register must 
match the size designated by the last character of the instruction (‘b’, ‘w’, ‘1’, or 
‘q’)- For most cases, the mov instructions will only update the specific register bytes 
or memory locations indicated by the destination operand. The only exception is 
that when movl has a register as the destination, it will also set the high-order 4 
bytes of the register to OvThis exception arises from the convention, adopted in 
x86-64, that any instruction that generates a 32-bit value for a register also sets the 
high-order portion of the regisijer to 0. 

The following mov instruction examples show the five possible combinations 
of source and destination types. Recall that the source operand comes first and 
the destination second. 


1 

movl $0x4050,%eax 

Immediate— Register, 

4 

bytes 

2 

movw Xbp,%sp 

Register — Register, 

2 

bytes 

3 

movb (Xrdi,Xrcx),Xal 

Memory—Regl st er, 

1 

byte 

4 

movb $-17,(Xesp) 

Immedi ate — Memory, 

1 

byte 

5 

movq %rax,-12(Xrbp) 

Regi ster — Memory, 

8 

bytes 


A final instruction documented in Figure 3.4 is for dealing with 64-bit imme¬ 
diate data. The regular movq instruction can only have immediate source operands 
that can be represented as 32-bit two’s-complement numbers. TTiis value is then 
sign extended to produce the 64-bit value for the destination. The movabsq in¬ 
struction can have an arbitrary 64-bit immediate value as its source operand and 
can only have a register as a destination. 

Figures 3.5 and 3.6 document two classes of data movement instructions for 
use when copying a smaller source value to a larger destination. All of these 
instructions copy data from a source, which can be either a register or stored 











184 Chapter 3 Machine-Level Representation of Programs 


- If > V * 

Aside Understanding how data rnoyement chaiiges a ae'stination register 

As described, there are two different conventions regarding wh^th^r and how data movement instruc¬ 
tions modify the upp^r bytes,of a d^tination register. This distinction is illustrated by the following 
code sequence: 


f 

raovabsq $0x0011223344556677, Xrair 

OQl 1223344556677 

2 

movb 

$- 1 . '/.al 

%isjc = Q0112233445Se6FF 

3 

movw 

$-1 ,.xy,ax 

%rax 001122'334456FFFF‘ 

4 

movl 

$-1, %eax 

Xrax = OOOOOOOoPffFFFFF 

5 

movq 

$-1, Xrax 

%r^ {"fFFFFFFFRFFTFFFF* 


In the followirig'discussion, we use hexadecimal notation. In the example, the instruction on lihe 1 
initializes register %rax to the pattern 001*1253344556671. The reinaihing instmctiops have iipmediate. ^ 
value —1 as their source values. Rec^ll.that the.hexad^cimaifepresej}{qtion of —1 is»of the form FF- • ,-F,“ i 
^here the numbef ofF’s istsyicq number of bytes in'the representatioji. The 510 vb instruction‘{lin.e,2) I 

therefore sets the low-order byte of.Xrax to FFf while the,movu ihsfruction (line 3).sets*the low-prdqf i 
2 bj'tes to FFFF, with the remaining bj'tes unchanged, Tlje mbyl* instruction (line 4) sets the Jow-qrder J 
4 bytes to FFFFFFFI\ but it also sets the high-order'4 bytes to OOOOOOO 6 . i^nsilly, Jhe mojrq instruction | 
(line 5) sets the complete register to FFFFFFFFFFF^^’pFF^ , n .it • , | 


SI*., W. «>*.•« WlM-' 




•tMtk.'ft;'* WWwMWMiV.' ^ 


Instruction Effect Description 


Movz S,R R ■<- ZeroExtend(5) 
movzbw 
movzbl 
movzwl 
movzbq 
raovzwq 


Move with zero extension 
Move zero-extended byte to word 
Move zero-extended byte to double word 
Move zero-extended word to double word 
Move zero-extended byte to quad word 
Move zero-extended word to quad word 


Figure 3.5 Zero-extending data movement instructions. These instructions have a 
register or memory location as the source and a register as the destination. 


in memory, to a register destination. Instructions in the movz class fill out the 
remaining bytes of the destination with zeros, while those in the movs class fill 
them out by sign extension, replicating copies of the most significant bit of the 
source operand. Observe that each instruction name has size designators as fts 
final two characters—the first specifying the source size, and the second specifying 
the destination size. As can be seen, there are three instructions in each of these 
classes, covering all cases of 1- and 2-byte source sizes and 2- and 4-byte destination 
sizes, considering only cases where the destination is larger than the source, of 
course. 













Section 3.4 Accessing Information 185 


Instruction 

Effect 

Description 

MOVs 5, R 

R SignExtend(5) 

Move with sign extension 

movsbw 


Move sign-extended byte to word 

movsbl 


Move sign-extended byte to double word 

movswl 


Move sign-extended word to double word 

movsbq 


Move sign-extended byte to quad word 

raovswq 


Move sign-extended word to quad word 

movslq 


Move sign-extended double word to quad word 

cltq 

%rax ■<- SignExtend(7.eax) 

Sign-extend Xeax to 7.rax 


Figure 3.6 Sign-extending dat^ movement instructions. The movs instructions have 
a register or memory location as the source and a register as the destination. The cltq 
instruction is specific to registers %eax and %rax. 


Note the absence of an explicit instruction to zero-extend a 4-byte source 
value to an 8 -byte destination in Figure 3.5. Such an instruction would logically 
be named movzlq, but this instruction does not exist. Instead, this type of data 
movement can be implemented using a movl instruction having a register as the 
destination. This technique takes advantage of the property that an instruction 
generating a 4-byte value with a register as the destination wilTfill the upper 4 
bytes with zeros. Otherwise, for 64-bit destinations, moving'with sign extension is 
supported for all three source types, and moving with zero extension is supported 
for the two smaller source types. 

Figure 3.6 also documents the cltq instruction. This instruction has no 
operands—it always uses register 7 ,eax as its source and V.rax as the destination for 
the sign-extended result. It therefore has the exact same effect as the instruction 
movslq %eax, %rax, but it has a more compact encoding. 


For each of the following lines of assembly language, determine the appropriate 
instruction suffix based on the operands. (For example, mov can be rewritten as 
movb, mow, movl, or movq.) 


mov_ 

7.eax, (7.rsp) 

mov_ 

(7.rax), 7.<ix 

mov_ 

$0xFF, 7.bl 

mov_ 

(7.rsp, 7.rdx, 4), 7.dl 

mov_ 

(7.rdx), 7trax 

mov_ 

7.dx, (%rax) 










186 Chapter 3 Machine-Level Representation of Programs 


Aside Comparing byte movement instructions' 

'i f 

The following example illustrates how different data movement instructions eitheT do or do not change 
the high-order bytes of the destination. Observe that the three byte-movement instructions movb, . 
njovsbq, and movzbq differ from’each other in subtle ways. Here is an example; 


1 

2 

3 

4 

5 


movabsq $0x0011223344556677, 

movb $0xAA, y.dl 

movb 

movsbq •/,dl,7,rax 
movzbq ”/,dl,Xrax 


Xrax Xz-ax = 001i223344S56677 
Xdl = AA 

Xraxr^ 00112233445&66AA 
%rax = FFFFFFFFFFFFFFAA 
Jrax = ooooogeoooooooAA 


* 

* 


In the following discussion, we use hexadecimal notation-for all pf the values. The first two lines » 
of the code initialize registers Xrax and 'Ml to 0011223344556677 and AA, respectively. The remaining 
instructions all copy the low-order byte of */,rdx to the low-order byte of Max. 'The ijovb instruction 
(line 3) does not change the other bytes. The movsbq instruction Oine 4) sets the other 7 bytes to ^ 
either all ones or all zeros depending on the high-order bit of the source byte. Since hexadecimal A 
represents binary value 1010, sign extension causes the higher-order byte^ to e^ch be set to FF. The ^ 
movzbq instruction (line 5) always sets the other 71)ytes to zero. 


Each of the following lines of code generates an error message when we invoke 
the assembler. Explain what is wrong with each line. 


movb $0xF, C7,ebx) 
movl 7.rax, (7.rsp) 
mow (7.rax) , 4 C7.rsp) 
movb Xal,Xsl 
movq Xrax,$0x123 
movl Xeax.Xrdx 
movb Xsi, 8(Xrbp) 


3.4.3 Data Movement Example 

As an example of code that uses data movement instructions, consider the data 
exchange routine shown in Figure 3.7, both as C code and as assembly code 
generated by gcc. 

As Figure 3.7(b) shows, function exchange is implemented with just three 
instructions: two data movements (movq) plus an instruction to return back to 
the point from which the function was called (ret). We will cover the details of 
function call and return in Section 3.7. Until then, it suffices to say that arguments 
are passed to functions in registers. Our annotated assembly code documents 
these. A function returns a value by storing it in register %rax, or in one of the 
low-order portions of this register. 



















Section 3.4 Accessing Information 

(a) C code 

long exchange(long ♦xp, long y) 

long X = *xp; 

♦xp = y; 
return x; 

} 

(b) Assembly code 

2ong exchange (long *xp, long y) 
xp in Xrdi, y in %rsi 

1 exchange: _ 

2 movq (Xrdi) , Xrax Get x at xp. Set as return value. 

3 movq %rsi, (%rdi) store y at xp. 

Return. 

Figure 3.7 C and assembly code for exchange routine. Registers %rdi and /(rsi 
hold parameters xp and y, respectively. 


When the procedure begins execution, procedure parameters xp and y are 
stored m registers %rdi and %rsi, respectively. Instruction 2 then reads x from 
memory and stores the value in register %rax, a direct implementation of the 
operation x = +xp in the C program. Later, register %rax will be used to return 
a value from the function, and so the return value wall be x. Instruction 3 writes y 
to the memory location designated by xp in register "/.rdfi, a'direct implementation 
of the operation *xp = y. This example illustrates how the mov instructions can be 
used to read from memory to a register (line 2), and to write from a register to 
memory (line 3). 

Two feapres about this assembly code are worth noting. First, we see that what 
we call pointers” in C are simply addresses. Dereferencing a pointer involves 
copying’ that pointer into a register, and thdn using this register in a memory 
reference. Second, local variafcles such as x are often kept in registers rather than 
stored m memory locations. Register access is much faster than memory access. 






wm 


Assume variables sp and dp are declared with types 


src_t *sp; 
dest_t *dp; 


where src_t and dest_t are data types declared with typedef. We wish to use 
the appropriate pair-of data movement instructions to implement the operation 


187 


»dp = (dest_t) *sp; 










188 Chapters Machine-Level Representation of Programs 


New to C? Some examples of pointers 

Function exchange (Figure 3.7(a)) provides a good illustration of the use of pointers in C. Argument ^ 
xp is a pointer to a long integer, vsfhile y is a long integer itself. The statement 


long X = *xp; 

indicates that we sRould read the value stored in the location designated by xp and store it as a local 
variable named x. This'read operation is known as pointer dereferencing. The C operator performs ^ 
pointer dereferencing. 

The statement 

i 

♦xp = y: * 

does the reverse-^t writes the value of parameter y at the location designated by xp. Ihisjs also a form 
of pointer dereferencing (and hence the operator *), but it indicates a write operation since it is on the. 
left-hand side of the assignment. 

The following is an example of exchange in action: 

. long a = 4; » 

long = exchange (&a, 3); 

printfC'a = */.ld, b = 7.1d\verb®\@n'', a, b); 

This code willsprint ' ^ *> ju 

a = 3, b = 4 

43J i' hO» * • V , ? . . u- * *1, 1 

The C operator % (called-the “^dck.ess of’ qperatpr) creates^ pointer, lathis case to the location 
holding local variable a. Functipmexchangq oyerwr(tes the, value*stored,in a^with 3 bjit return^ the 
previous value, 4, a§ (Re function Value. Qbserve hqw by,pas?iijg^ pointer to e;cchange, ihcould mpdify , 
data held at some remote location. ^ , « 


Assume that the values of sp and dp are stored iruregisters 7,rdi and /orsi, 
respectively. For each entry in the table, show the two instructions that implement 
the specified data movement. The first instructiqnjn the sequence should read 
from memory, do the appropriate conversion, and set the appropriate portion of 
register %rax. The second instruction should then write the appropriate portion 
of %rax to memory. In both cases, the portions may be %rax, 7,eax, '/,ax, or 7.al, 
and they may differ from one another. 

Recall that when performing a cast that involves both a size change and a 
change of “signedness” in C, the operation should change the size first (Section 
2 . 2 . 6 ). 

src_t dest_t_ Instruction _ 

long long movq f^rdi) , ^rax 

movq%rax, (7.rsi) 

char int -- 





















Section 3.4 Accessing Information 


.189 


char 

unsigned char 
int 

unsigned 

char 


unsigned 

long 

char 

unsigned char 
short 










You are given the following information. A function with prototype 


void decodeKlong *xp, long ♦yp, long * 2 p); 
is compiled into assembly code, yielding the following: 


void decoder (long *xp, long •yp, long *zp) 
xp la Zrdi, yp ia Xrsi, zp la %rdx 


decodel: 


movq 

(•/.rdi), %r8 

movq 

(%rsi), %rcx 

movq 

(%rdx), %rax 

movq 

%r8, (%rsi) 

movq 

Xtcx , (%rdx) 

movq 

%rax, (%rdi) 

ret 



Parameters xp, yp, and zp are stored in registers %rdi, %r6i, and %rdx, respec¬ 
tively. 

Write C code for decodel that will have an effect equivalent to thfe assembly 
code shown. 


3.4.4 Pushing and Popping Stack Data 

^ i: t 

The final two data movement operations are used to push data onto and pop data 
from the program stack,-as documented in Figure 3.8.. As we will see, the stack 
plays a vital role in the handling of procedure calls. By way of backgroun.d,;a stack 
is d data structure where values can be added or deleted j bub only according to 
a !“last-in, first-out” discipline. We add data to a stack via a push operation and 
remove it via a pop operation, with the property that the value popped will always 
be the value that was most recently pushed-and is still on.the stack. A stacbcan be 
implemented as an array, where we'always insert and remove elements from one 










190 Chapter 3 Machine-Level Representation of Programs 


Instruction 

Effect 

Description 

pushq S 

R[%rsp] +- R[“/.rsp]-8; 
M[R['/.rsp]] •«- S 

Push quad word 

popq D 

D M[R[’/.rsp]]; 

R[7.rsp] 4- R[7,rsp] -t- 8 

Pop quad word 

Figure 3.8 

Push and pop Instructions. 



Initially 


7.rax 

0x123 

'/.rdx 

0 

'/.rsp 

0x108 


Stack "bottom" 



pushq Xrax 


'/.rax 

0x123 

•/.rdx 

0 

'/.rsp 

0x100 



Stack ‘lop’’ 


popq '/.rdx 


'/.rax 

0x123 

'/.rdx 

0x123 

'/.rsp 

0x108 


Stack “bottom” 



Figure 3.9 Illustration of stack operation. By convention, we draw stacks upside down, 
so that the "top" of the stack is shown at the bottom. With x86-64, stacks grow toward 
lower addresses, so pushing involves decrementing the stack pointer (register ’/.rsp) and 
storing to memory, while popping involves reading fron^memory and incrementing the 
stack pointer. 


end of the array. This end is called the top of the stack. With x86-64, the program 
stack is stored in some region of memory. /s.s illustrated in Figure 3.9, the stack 
grows downward such that the top element of the stack has the lowest address of 
all stack elements.. (By convention, we draw stacks upside down, with the, stack 
“top” shown at the bottom of the-figure.) The stack pointer %rsp holds the address 

of the top stack element. ' 

The pushq instruction provides the ability to push data onto the stack, while 
the popq instruction pops it. Each of these instructions takes a single operand—the 
data source for pushing and the data destination for popping. 

PusTiing a quad word value onto tht st^ck involves first decrementing the 
stack pointer'by 8 and i then writing the value at the new top-of-stack address. 


























Section 3.5 Arithmetic and Logical Operations 191 

Therefore, the behavior of the instruction pushq 7,rbp is equivalent to that of the 
pair of instructions 

SUbq $8,/(rsp Decrement stack pointer 

movq /.rbp, (,4rsp) Store %rbp on stack 

except that the pushq instruction is encoded in the machine code as a single byte, 
whereas the pair of instructions shown above requires a total of 8 bytes The first 
^o columns in Figure 3.9 illustrate the effect of executing the instruction pushq 
/,rax when /,rsp is 0x108 and %rax is 0x123. First 7.rsp is decremented by 8 giving 
0x100, and then 0x123 is stored at memory address 0x100, 

Popping a quad word involves reading from the top-of-stack location and 
then incrementing the stack pointer by 8. Therefore, the instruction popq %rax 
is equivalent to the following pair of instructions: 

movq (/.rsp),^rax Read Xrax from stack 

addq $8,^rsp Increment stack pointer 

The third column ofFigure 3.9 illustrates the effect of executing the instruction 
popq /.edx immediately after executing the pushq. Value 0x123 is read from 
memory and written to register %rdx. Register %rsp is incremented back to 0x108 
As shown m the figure, the value 0x123 remains at memory location 0x104 until it 
is overwritten (e.g., by another push operation). However, the stack top is always 
considered to be the address indicated by %rsp. 

Since the stack is contained in the same memory as the program code and 
other forms of program data, programs can access arbitrary positions within the 
stack using the standard memory addressing methods. For example, assuming the 
topmost element of the stack is a quad word, the instruction movq 8 (%rsp) , %rdx 
will copy the second quad word from the stack to register %rdx. 


Arithmetic and Logical Operations 

Figure 3.10 lists some of the x86-64 integer and logic operations. Most of the 
operations are given as instruction classes, as they can have different variants with 
ditterent operand sizes. (Only leaq has no other size variants.) For example the 
instruction class add consists of four addition instructions: addb, addw, addl, and 
addq, adding bytes, words, double words, and quad words, respectively. Indeed 
each of the instruction classes shown has instructions for operating on these four’ 
different sizes of data. The operations are divided into four groups: load effective 
address, unary, binary, and shifts. Binary operations have two operands, while 
unary operations have one operand. These operands are specified using the same 
notation as described in Section 3.4. 

3.5.1 Load Effective Address 

The had effective address instruction leaq is actually a variant of the movq in¬ 
struction. It has the form of an instruction that reads from memory to a register, 









Chapter 3 Machine-Level Representation of Programs 


Instruction 

Effect 


Description 

leaq 

S, D 

D 

4— 

&s 

Load effective address 

INC 

D 

D 


D+l 

Increment 

DEC 

D 

D 

I- 

D-1 

Decrement 

NEO 

D 

D 

4— 

-D 

Negate 

NOT 

D 

D 


-D 

•Complement 

ADD 

S,D 

D 

4— 

D + S 

Add 

SUB 

S,D 

D 

4— 

D-S 

Subtract 

IMUL 

S,D 

D 

4— 

D*S 

Multiply 

XOR 

S,D 

D 

4-^ 

D~ S 

Exclusive-or 

OR 

S,D 

D 

■s- 

D 1 S 

Or 

AND 

S,D 

D 


D&S 

And 

SAL 

k,D 

D 

<- 

D << k 

Left shift 

SHL 

k,D 

D 


D«k 

Left shift (same as sal) 

SAR 

k,D 

D 

4— 

D »\k 

Arithmetic right shift 

SHR 

k, D 

D 


D >>L k 

Logical right shift 


Figure 3.10 Integer arithmetic operations. The load effective address (leaq) 
instruction is commonly used to perform simple arithmetic. The remaining ones ate 
more standard unary or binary operations. We use the notation >>a >>l to denote 

arithmetic and logical right shift, respectively. Note the nonintuitive ordering of the 
operands with ATT-format assembly code. 


but it does not reference memory at all. Its first operand appears to be a mem¬ 
ory reference, but instead of reading from the designated location, the instruction 
copies the effective address to the destination. We indicate this computation in 
Figure 3.10 using the C address operator &S. This instruction can be used to gener¬ 
ate pointers for later memory references. In addition, it can be used to compactly 
describe common arithmetic operations. For example, if register 7.rdx contains 
value X, then the instruction leaq 7(y.rdx,7.rdx,4) , 7.rax will set register 7.rax 
to 5x + 7. Compilers often find clever uses of leaq that have nothing to do with 
effective address computations. The destination operand must be a register. 




Suppose register 7.rax holds value x and 7,rcx holds value y. Fill in the table below 
with formulas indicating the value that wiU be stored in register 7,rdx for each of 
the given assembly-code instructions: 


Instruction 


leaq 6 (7.rax) , %rdx 
leaq (7.rax, 7,rcx) , 7.rdx 
leaq (7.rax,7.rcx,4) , 7.rdx 




7.rdx 


Result 



















leaq OxA (, %rcx, 4) , %rdx 
leaq9(%rax,%rcx,2),. %rdx 


Section 3.5 Arithmetic and Logical Operations 193 


C compiled code, consider the following 


long scaledqng x, long y, long z) { 
long t = x' + 4 * y + 12 + z;'"* 
re^:urn t; 


i ux ^ 


When compiled, the arithmetic operations of the function u.. 

of three leaq functions, as is documented by the comments on the 
rignt-hand side; 


long scaleQong x, long y, long z) 

X in %rdi^ y in Zxsi, z in Xrdx 

scale: 

leaq (%rdi,%rsi,4), Xrax x + 4*y 

l®^q (/{rdx,Xrdx,2) , %rdx z + 2*z = 3*z 

leaq (Xrax;%rdx,4). %rax (x+4*y) + 4*(3*z) - x + 4*y + I2*z 

ret 


The ability of the leaq instruction'to perform addition and limited forms of 

multiplication proves useful when compiling simple arithmetic expressions such 
as this example. 


Smputed-^^^ following code, in which we-have omitted the expression being 


long scale2(long x, long y, long z) { 

long t = __ . 

return t; 

} 


Compiling the actual function with gcc yields the following assembly code: 

long scal62(long x, long y, long z) 

X in Zrdi, y in Xrsi, z in Xrdx 
scals2: 

leaq (%rdl,%rdi.4), 7.rax 

leaq (7.rax,7.rsi.2). 7.rax 

leaq (Xrax,Xrdx,8). Xrax 

ret 

Fill in the missing expression in the C code. 







194 Chapter 3 Machine-Level Representation bf Programs 


3.5.2 Unary and Binary Operations 


Operations in the second group are unary operations, with the single operand 
serving as both source and destination. This operand can be either a register or 
a memory location. For example, the instruction incq (%rsp) causes the 8-byte 
element on the top of tlie stack to be incremented. This syntax is refnmiscent of 
the C increment (++) and decrement (---) operators. 

The third group consists of binary operations, where the second operand 
is used as both a source and a destination. This syqtax is reminiscent of the C 
assignment operators, such as x -= y. Observe, however, that the source operand 
is given first and the destination second. This looks pecuhar for noncommutative 
operations. For example, the instruction subq %rax,"/,rdx decrements register 
%rd.x by the value in Xrax. (It helps to read the instruction as Subtract ^rax from 
%rdx.”) The first operand can be either an immediate value, a register, or a memory 
location. The second can be either a register or a memory location. As:with-the 
MOV instructions, the two operands cannot both be memory locations. Note that 
when the second operand is a memory location, the processor must read the value 
from memory, perform the operation, and then write the result back to memory. 


registers; 

Address 

Value 

Register 

Value 

1 

0x100 

OxFF 

j 7.rax 

0x100 

0x108 

OxAB 

7.rcx 

0x1 

0x110 

0x13 

7.rdx 

0x3 

0x118 

0x11 




Fill in the following table showing the effects of the following instructions, 
in terms of both the register or memory location that will be updated and the 
resulting value: 

Instruction Destination Value 

addq %rcx, (‘/.rax) -- 

subq 7,rdx,8(7«rax) -- 

imulq $16, (7.ra,x, 7.rdx, 8)-- 

incq 16 (7.rax) -- 

decq 7»i’cx -- 

subq 7ordx,7.rax —- - 



3.5.3 Shift Operations 

j 

The final group consists of shift operations, where the shift amount is given first 
and the value to shift is given second. Both arithmetic and logical right shifts are 




























Section'3'.5 Arithmetic and-Logical Operations V195 


possible. The different shift instructions can specify the shift amount either as 
an unmediate value or with the single-byte register %cl. (These instructions are 
unusual in only allowing this specific register as the operand.) In principle, having 
a 1-byte shift amount would make it possible to encode shift amounts ranging up 
to 2 - 1 - 255. With X86-64, a shift instruction operating on data values that are 
w bits long determines the shift amount from the Jow-order m bits of register 
/.cl, where 2'” - u;. The higher-order bits are ignored. So, for example, when 
register /.cl has hexadecimal value OxFF, then instruction salb would shift by 

7, while salw would shift by 15, sail would shift by 31, and salq would shift 
by 63, 

As Figure 3.10 indicates, there are two names for the left shift instruction: sal 
and SHL. Both have the same effect, filling from the right with zeros. The right 
shift mstructions differ in that sae performs an arithmetic shift (fill with copies of 
the sign bit), whereas she performs a logical shift (fill with zeros). The destination 
operand of a shift operation can be either a register or a memory location. We 
denote the two different right shift operations in Figure 3.10 as >> a (arithmetic) 
and >>L (logical). ' 


Suppose we want to generate assembly code for the following C function: 

long shift_left;4_rightn(long x, long n) 
f 

X «= 4; 

X »= n; 

return x; 


T^e code that follows is a portion of the assembly code that performs the 
actual shifts and leaves the final value in register %rax. Two key instructions 
have been omitted. Parameters x and n are stored in registers %rdi and %rsi 
respectively. 

long shift_left4_ri^tn(long x. long n) 

X in %rdi, n in Zrsi 

shift_left4_rightn; 

movq %rdi, %rax Get x 

-- X «= 4 

movl %esi, %ecx Get n (4 bytes) 


in the missing instructions, following the annotations on the right. The 
right shift should be performed arithmetically. 











196 Chapters Machine-Level Representation of Programs 


(a) C code ' 

long arithClong x, long y, long z) 
{ 

long tl = X ~ y: 

long t2 = z ♦ 48; 

long t3 = tl & OxOFdFOFOF; 

long t4 = t2 - t3; 

return t'4; 

> 

(b) Assembly code 


T 

ib'iig aritbClong x, long y, lon^ z) 



X 'in Zrdi, 

‘ y in Zrsi, z in Xrdx 


1 

aritli: 



2 

xorq 

•/.rsi, y.rdi 

tl = X “ 7 

3 

leaq 

(%rdx,%rdx,i2^ , %rai 

3*z 

4 

salq 

$4, %rax 

t2 = 16 * (3*z) = 48*z 

5 

andl 

$252645135, °/.edi 

t3 = tl & OxOFOFOFOF 

6 

subq 

’/.rdi, 7,rax 

Return t2 - t3 

7 

ret 




Figure 3.11 C and assembly code'for arithmetic function. 


3.5.4 Discussion 

We see that most of the instructions shown in Figure 3.10 can be used for either 
unsigned or two’s-complement arithmetic. Only right shifting requires instructions 
that differentiate between signed versus unsigned data. This is one of the features 
that makes two’s-complement arithmetic the preferred way to implement signed 
integer arithmetic. 

Figure 3.11 shows an example of a’fuhction that performs arithmetic opera- 
tions^and its translation into assembly code. Arguments x, y, arid z a'rH'initially 
stored in registers y.rdi, %rsi, and "/.rdxfre'spectively.'The assembly-code instruc¬ 
tions correspond closely with the lines of C source code. Line 2 computes the value 
of x“y. Lines 3 and 4 compute the expression z*48 by a combination of leaq and 
shift instructions. Line 5 computes the and of tl and OxOFOFOFOF. The final sub¬ 
traction is computed by line 6. Since the destination of the subtraction is register 
%rax, this will be the value returned by the function. 

In the assembly code of Figure 3.11, the sequence of values in register /irax 
corresponds to program values 3*z, z*48, and t4 (as the return value). In general, 
compilers generate code that uses individual registers for multiple program values 
and moves program values among the re^sters. 



In the following variant oLthe function of Figure 3.11(a), the expressions have 
been replaced by blanks; 




















Section 3.5 Arithmetic and Ldgital Operations 197 


long aritli2(long x, long y, long z) ^ 

{ 

long tl =_; 

longl t2 “_: 

long t3 = --; ^ 

long t4 =_; 

return t4; 

> 

The portion of the generated assembly code implementing these expressions 
is as follows: 

long arith2(long x, long y, long z) 

X in Xrdi, y in‘Xrsi, z in %tdx 


aritM; 


orq 

Xrsi, °4rdi 

sarq 

$3, Xrdi 

notq 

Xfdi 

movq 

y,rdx, Xrax 

subq 

ret 

Xrdi, Xrax' 


Based on this^asserably code, fill in the missing portions of the C code. 



It is common to fin'd assembly-code lines of the form 


xorq, y,rdx,'4Edx i 

in code that was generated from C where no exclusive-or operations were 
present. 

A. Explain the effect of this particular exc!lusive-or instruction and what useful 
operation it iifiplements. 

B. What would be the more straightforward way to express this operation in 
assembly code? 

C Compare the number of bytes to encode these two different implementa¬ 
tions of the same operation. 


3.5.5 Special Arithmetic Operations 

As we saw in Section 2.3, multiplying two 64-bit signed or unsigned integers can 
yield .a product “that requires 128 bits to represent. The x86-64,instruction set 
provides limited support for operations involving 128-bit (16-byte) numbers. Con¬ 
tinuing with the naming convention of word (2 bytes), double“word (4 bytes), and 
quad word (8 bytes), Intel refers to a 16-byte quantity as an oct word. Figure 3.12 











198 Chapter 3 Machine-Level Representation of Programs 


Instruction 

Effect 

Description 

imulq 

S 

R[7.rdx]:R[7,rax] S x R[%rax] 

Signed fuU multiply 

mulq 

S 

R[7.rdx]:R[%rax] ^ 5 x R[y.rax] 

Unsigned full multiply 

cqto 


R[7,rdx]:R[7.rax] SignExtend(R[%rax]) 

Convert to oct word 

idivq 

s 

R["/.rdx] •«- R[%rdx];R[*/,rax]mod 5; 

R[%raxi -e- R[7.rdx]:R[7,rax]5 

Signed divide 

divq 

s 

R[7.rdx] R[y.rdx]:R[7,rax] mod 5; 

R[7.rax] R[y.rdx]:R[7.rax]-H S 

Unsigned divide 


Figure 3.12 Special arithmetic operations. These operations provide full 128-bit 
multiplication and division, for both signed and unsigned numbers. The pair of registers 
7,rdx and %rax are viewed as forming a single 128-bit oct word. 


describes instructions that support generating the full 128-bit product of two^6^-bit 
numbers, as well as integer division. 

The imulq instruction has two different forms One form, shown in Figure 3.10, 
is as a member of the imul instruction class. In this form, it serves as a “two- 
operand” multiply instruction, generating a 64-bit product from two 64-bit oper¬ 
ands. It implemehts the operations arid described in Sections 2.3.4 and2.3.5. 
(Recall that when truncating the product to 64 bits, both unsigned multiply and 
two’s-complement multiply have the same bit-level behavior.) 

Additionally, the x86-64 instruction set includes two different “one-operand” 
multiply instructions to compute th? full 128-bit product of two 64-bit values 
one for unsigned (mulq) and one for two’s-complement (imulq) multiplication. 
For both of these instructions, one argument must be in register 7orax, and the 
other is given as the instruction source operand. The product is then stored in 
registers Xrdx*^(high-order 64 bits) and %rax (low-order 64 bits). Although the 
name imulq is used for two distinct multiplication operations, the assembler can 
tell which one is intended by counting the number of operands. 

As an example, the following C code demonstrates the generation of a 128-bit 
product of two unsigned 64-bit numbers x and y; 

#include <inttypes,h> 

typedef unsigned_in'tl28 ; 

void s'tore_uprod(uin't 128_'t ♦dest, uin't64_t: x, uint64_'t y) { 

♦dest = X * (uintl28_t) y; 

} 

I 

In this program, we explicitly declare x and y to be 64-bit numbers, using defi¬ 
nitions declared in the file inttypes ,h, as part of an extension of the C standard. 
Unfortunately, this standard does notmake provisions for 128-bit values. Instead, 



















Section 3.5 Arithmetic and Logical Operations 199 


we rely on support provided by gcg for’128-bit integers, declared using the name 

_intl28. Our code uses a typedef declaration to define data type uintl28„t, 

following the naming pattern for other data types found in inttypes. h. The code 
specifies that the resulting product should be stored at the 16 bytes designated by 
pointer dost. 

The assembly code generated by gcc for this function is as follows: 

Void store_uprod(uintl28_t *dest, ulnt64_t x, uint64_t y) 
dost in %rdi, x in Xrsi, y in %rdx 
1 store_uprod; 


2 

movq 

'/rsl. 

%rax 

Copy X to multiplicand 

3 

mulq 

‘/.rdx 


Multiply by y 

4 

movq 

‘/rax. 

(’/.rdi) 

Store lower 8 bytes at dest 

5 

movq 

Xrdx, 

8(%rdi)'’^ 

Store upper 8 bytes at dest+8 

6 

ret 





Observe that storing the product requires two movq instructions; one for the 
low-order 8 bytes (line 4), and one for the high-order S'ljytes (line 5). Since the 
code is generated for a little-endian machine, the high-order bytes are stored at 
higher addresses, as indicated by the address specification 8 (lirdi'). 

Our earlier table of arithmetic operations (Figure 3.10) does not list any 
division or modulus operations. These operations are provided by the single¬ 
operand divide instructions similar to the single-operand multiply instructions. 
The signed division instruction idivl takes as its dividend the 128-bit quantity 
in registers Xrdx (high-order 64 bits) and %rax (low-order 64 bits). The divisor is 
given as the instruction operand. The instruction stores the quotient in register 
’/.rax and the remainder in register ’/,rdx. 

For most applications of 64-bit addition, the dividend is given as a 64-bit value. 
This value should be stored in register %rax. The bits of /irdx should then be set to 
either all zeros (unsigned arithmetic) or the sign bit of Xrax (signed arithmetic). 
The latter operation can be performed using the instruction cqto.^ This instruction 
takes no operands—it implicitly reads the sign bit from ’/.rax and copies it across 
allof°/,rdx. 

As an illustration of the implementation of division with x86-64, the following 
C function computes the quotient and remainder of two 64-bit, signed numbers: 

void remdivdong x, long y, 

long ♦qp, long ♦rp) f 
long q = x/y; 

’long r = x*/.y; 

♦qp = q; 

♦rp = r; 

> 


This instruction is called cqo in the Intel documentation, one of the few cases where the ATT-format 
lame for an instruction does not match the Intel name. 








200 Chapter 3. Machine-Level Representation of Programs 


This compiles to the following assembly code: 

void remdivdong x, long y, long *qp, long *rp) 
X in Xrdi, y in Zrsi, qp in %rdx. -rp in %rcx 


1 

remdiv: 




2 

movq 

’/.rdx. 

%r8 

Cof,^ qp , 

3 

movq 

7.rdi, 

%rax 

Wove X to lover 8 bytea of dividend 

4 

cqto 



Sign-extend to upper 8 bytea of dividend 

5 

idivq 

Xrsi 


Divide by y 

6 

movq 

Xrax, 

(•/.rS) 

Store quotient at qp 

7 

movq 

Xrdx, 

(•/.rex) 

Store remainder at rp 

8 

ret 





In this code, argument rp must first be^aved ip a different register (line 2), 
since argument register %rdx is required for the division operation. Lines 3—4 then 
prepare the dividend by copying and sign-extending x. Following the division, the 
quotient ipregijiter %pax gets stored at qp (line 6), while the remainder in register 
•/.rdx gets stored at rp (line 7). 

Unsigned clivision makes^use of the divq instruction. Typically, register /irdx 
is set to zero beforehand. j t 



Consider the following function for computing the quotient and remainder of twd 
unsigned 64-bit numbers: 


void uremdiv(unsigned long x, unsigned long y, 

unsigned long ♦qp, unsigned long *rp)‘t 
unsigned long q = x/y; 
unsigned long r = x7,y; 

♦qp =. q; '' ‘‘ 

♦rp = r; i 

} - 

Modify the assembly code shown for siped division to implement this function. 


3.6 Control > 

So far, we have only considered the behavior of straight-line code, where instruc- ; 
tions follow one another in sequence. Some constructs in C, such as conditionals, j 
loops, and switches, require conditional execution, where the sequence of oper- ; 
ations that get performed depends on the outcomes of tests applied to the data, j 
Machine code provides two basic low-level mechanisms for implementing condi- : 
tional behavior: it tests data values and then alters either the control flow or the j 
data flow based on the results of these tests. | 

Data-dependent control flow is the more general and more common approach j 
for implementing conditional behavior, and so we will examine this first. Normally, 

\ 











Section 3.6 Control 201 


I 

I both statements in C and instructions in machine code are executed sequentially, 
I in the order they appear in the program. The execution order of a set of machine- 
I code instructions can be altered with a jump instruction, indicating that control 
I should pass to some other part of the program, possibly contingent on the result 
' of some test. The compiler must generate instruction sequences that build upon 
I this low-level mechanism to implement the control constructs of C. 

I In our presentation, we first cover the two ways of implementing conditional 
I operations. We then describe methods for presenting loops and switch state- 
I ments. 

I 

' 3.6.1 Condition Codes 

I 

' In additioiL to the integer registers, the CPU maintains a set of single-bit co ndition 
I code registers describing attributes of the most recent arithmetic or logical oper- 
I ation. These registers can then be tested to perform conditional branches, These 
I condition codes are the most useful: 

I 

I CF; Carry flag. The most recent operation generated a carry out of the most 
I significant bit. XJsed to detect overflow for unsigned operations. 

I ZF; Zero flag. The most recent operation yielded zero. 

' SF: Sign flag. The most recent operation yielded a negative value. 

I OF: Overflow flag. -The most recent operation caused a two’s-complement 

I overflow—either negative or positive. 

I 

I For example, suppose we used one of the add instructions to perform the 
' equivalent of the C assignment t = a+b, where variables a, b, and t are integers. 

' Then the condition codes would be set according to the following C expressions; 


CF 

(unsigned) t < (unsigned) a 

Unsigned overflow 

ZF 

(t == 0) 

Zero 

SF 

(t '< 0) 

Negative 

of' 

(a < 6 == b < 0) M (t < 0 = a < 0) 

Signed overflow 


I 

I Hie leaq instruction does not alter any condition codes, since it is intended 
' to be used in address computations. Otherwise, all of the instructions listed in 
Figure’ 3.10 cause the condition codes to be set. For th6 logical operations, such 
I as xoR, the carry and overflow flags are set to zero. For the shift operations, the 
I carry flag is set to the last bit shifted out, while the overflow flag is set to zero. For 
I reasons that we will not delve into, the iNC and dec instructions set the overflow 
land zero flags, but they leave the carry flag unchanged. 

' In addition to the setting of condition.codes by the instructions of Figure 3.10, 
'there are two instruction classes (having 8-, 16-, 32-, and 64-bit forms) that set 
condition codes without altering any other registers; these are listed in Figure 3.13. 
iThe CMP instructions set the condition codes according to the differences of their 
two operands. They behave in the.sdme way as the sub instructions, except that 
they set the condition codes without updating their destinations. With ATT format, 

I 

I 

I 

I 





202 Chapter 3 Machine-Level Representation of Programs 


Instruction Based on Description 

Compare 
Compare byte 
Compare word 
Compare double word 
Compare quad word 

Test 

Test byte 
Test word 
Test double word 
Test quad word 

Figure 3.13 Comparison and test instructions. These instructions set the condition 
codes without updating any other registers. 

the operands are listed in reverse order, making the code difficult to read. These 
instructions set the zero flag if the two operands are equal. The other flags can 
be used to determine ordering relations between the two operands. The test 
instructions behave in the same manner as the and instructions, except that they 
set the condition codes without-altering their destinations. Typically, the same 
operand is repeated (e.g., testq %rax, "/.raix to see whether '/.rax is negative, zero, 
or positive), or one of the operands is a mask indicating which bits should be tested. 

3.6.2 Accessing the Condition Codes 

Rather than reading the condition codes directly, .there are three common ways 
of using the condition codes: (1) we can set a single byte to 0 or 1 depending 
on some combination of the condition codes, (2) we can conditionally jump to 
some other part of the program, or (3) we can conditionally transfer data. For the 
first case, the instructions described in Figure 3.14 set a single byte to 0 or to 1 
depending on some combination of the condition codes. We refer to this entire 
class of instructions as the set instructions; they differ from one another based on 
which combinations of conditiofi codes they consider, as indicated by the different 
suffixes for the instruction names. It is important to recognize that the suffixes for 
these instructions denote different conditions and not different operand sizes. For 
example, instructions setl and setb, denote “set less” and “set below, not set 
long word” or “set byte.” 

A SET instruction has either ctne of the low-order single-byte register elements 
(Figure 3.2) or a single-byte memory location as its destination, setting this byte to 
either 0 or 1. To generate a 32-bit or 64-bit result, we must also clear the highiorder 
bits. A typical instruction sequence to compute the C expression a < b, where a 
and b are both of type long, proceeds as follows: 


CMP Sj, S 2 *^2 “ -^1 

ciapb 
cmpw 
cmpl 
cmpq 

TEST 5 i, S2 Si & S2 

testb 

testw 

testl 

testq 
























Section 3.6 Control 203 


Iristructioi) 

Synonym 

E&ct' 

Set condition 

seta 

P 

setz 

p 


Equal / zero 

setne 

D 

,setnz 

D 

-ZF 

Not equal / not zero 

sets 

D 


D 

SF 

Negative 

setns 

D 


D 

-SF 

Nonnegative 

setg 

D 

setnle 

D 

t- “(SF-OFia-ZF 

Greater (signed >) 

setge 

D 

‘setnl 

D 

~ (SF - OF) 
